{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title","getFn":null},{"path":["body"],"id":"body","weight":1,"src":"body","getFn":null}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Root","n":1},"1":{"v":"\nMy personal knowledge cache using [Dendron](https://www.dendron.so)\n\n## History\nIt was during my first job, at the start-up company called PixelShift.AI, that I acquired most prominent skill as a software engineer-the ability to read source code. It opened up a new way of learning and my knowledge expanded rapidly ever since. Learning new things is thriving but noting them down cogently and organized is hard. I often found myself forget the context few month later, making the notes completely intelligible. Additionally, some notes need frequent update as my previous understanding could be parochial, antiquated or even erroneous.\n\nIn desperate attempt to facilitate the noting/updating experience, I came across [Dendron](https://www.dendron.so) and found it surprisingly congenial to my personal need. With it, I have gradually grown the habit of caching, linking back and extending my knowledge base.\n\n## Disclaimer\nThe notes are collections of information I found along the journey. I use backquote to denote sources, but I am likely to miss some references, so apology in advance. Also, they are my personal understandings, usually added when I first encounter the topic and will be updated only when I come across it again, so some of them could be incomplete or wrong.","n":0.072}}},{"i":2,"$":{"0":{"v":"Code Read","n":0.707}}},{"i":3,"$":{"0":{"v":"Igl","n":1}}},{"i":4,"$":{"0":{"v":"Subdivision","n":1},"1":{"v":"\nMore info: [[geometry.subdivision]]\n\n## Concepts\n### Barycenter\nCenter of mass\n\n### Edge-manifold\nEvery edge are either boundary or incidents two faces (oppositely oriented)\n\n## Implementations\n\n### barycenter\n\nAverage of all [[Simplex|geometry.winding-number#simplex]] vertex coords\n\n### edge_topology\n\n1. Verify [[is_edge_manifold|code-read.igl.subdivision#is_edge_manifold]]\n2. Collect [smaller_vert_idx, larger_vert_idx, face_idx, edge_idx_in_face] for all edges, then sort it [[lexicographically|coding.cpp.container#vector-comparison]]\n3. After sorting, each two consecutive edge are equal (boundary) or not (incident two faces), keep track of edge_vert (NE x 2), edge_face (NE x 2), face_edge (NF x 3) relations\n4. For edge_face relations, make sure first face on the left of edge, by checking if this oriented edge appears on the first face\n\n\n### is_edge_manifold\n\n1. Compute [[unique_edge_map|code-read.igl.neighbourhood-connectivity#unique_edge_map]]\n2. Count occurence of each unique unoriented edges, check if all are less or euqal to 2\n","n":0.094}}},{"i":5,"$":{"0":{"v":"Normal Vector","n":0.707},"1":{"v":"\n\n## Concepts\n### Normal Vector\nNormal Vector at a point is the cross product of two tangent vectors at that point.\n### Tangent Vector\nA vector tangents to a surface at a given point\n### Degenerated Normal Vector\ntwo tangent vectors are linear depend\n\n## Implementations\n\n### per_face_normals\n\n```\nV: Vertices (NV x 3)\nF: Faces (NF x 3)\n\nf = F[i] # for face i\nv1 = V[f[1]] - V[f[0]]\nv2 = V[f[2]] - V[f[0]]\nfn = cross(v1, v2)\nif fn is zero vector:\n    return degenerated normal\nelse:\n    return normalized fn\n```\n\n### per_vertex_normals\n\n> Weighted sum of adjacent faces' normal, interpolated by vertex shader\n\n```\nWeight W: NF x 3\ncase uniform: \n    all 1\ncase area: \n    compute face areas `doublearea` (NF x 1) replicated to (NF x 3)\ncase internal_angle: \n    `internal_angle` angle of each face cornors (angle between edges that share the same vertex)\n\n# theoritically loop through each vertex, compute its normal vn by\nvn = sum([w[i] * fn for normal of faces thats shares v])\n# implementation-wise it's more efficient to loop over faces\n\nreturn normalized vn\n\n```\n\n> Interpolated vertex normals -> [[Smooth shading in Blender|coding.blender.python-api#mesh-ops]]\n\n### per_corner_normals\n>Weighted (by double area) sum of selected (by angle between two normal) incident face normal, interpolated by vertex shader\n>\n>See: [[vertex_triangle_adjacency Unrolled|code-read.igl.neighbourhood-connectivity#unrolled]]\n\n\n```\nFA: double face area\nFN: face norm\nVF[NI[F(i, j)] + k]: (kth incident face) face index that incident to the jth vertex of face i\nFN.row(VF[NI[F(i, j)] + k]): normal of (kth incident face) face that incident to the jth vertex of face i\n```\n\n### Vertex Shader\n> ViewerData\n\nVertex\n```\nmeshgl.V_normals_vbo = data.V_normals.cast<float>();\n```\nFace or corner\n```\nmeshgl.V_normals_vbo.resize(data.F.rows()*3,3);\nfor (unsigned i=0; i<data.F.rows();++i)\n    for (unsigned j=0;j<3;++j)\n        meshgl.V_normals_vbo.row(i*3+j) =\n            per_corner_normals ?\n            data.F_normals.row(i*3+j).cast<float>() :\n            data.F_normals.row(i).cast<float>();\n```\n\n\n> MeshGL\n\n```\nbind_vertex_attrib_array(shader_mesh,\"normal\", vbo_V_normals, V_normals_vbo, dirty & MeshGL::DIRTY_NORMAL);\n```\n\n> MeshGL:: mesh_vertex_shader\n\n```\nin vec3 normal;\n```\n\n## TODO\n- [ ] Laplace-Beltrami operator\n- [ ] Cotangent stiffness matrix (Polygon Laplacian Made Simple)\n\n\n","n":0.061}}},{"i":6,"$":{"0":{"v":"Neighbourhood Connectivity","n":0.707},"1":{"v":"\n## Concepts\n### Manifold mesh\n> Reference: https://pages.mtu.edu/~shene/COURSES/cs3621/SLIDES/Mesh.pdf\n\n- Each edge is incident to only one or two faces\n- Faces incident to a vertex form a closed or an open fan\n\n## vertex_triangle adjacency\n\n### vertex_triangle_adjacency\n\n#### Vector of vector\n\nLoop through all faces with their vertex indices, push back occurrence\n\n|Vert index|Incident Face indices|\n|:-------- |:------------------- |\n|0         | 0, 1                |\n|1         | 9, 13, 1            |\n\n#### Unrolled\n\n```\nvfd(i): number of incident faces at vertex i\nNI: cumsum of vfd, prepend 0\nNI(i): number of cummulated incident faces before vertex i\nVF(NI(i) + k): kth face incident on vertex i (vertex index -> face index)\n```\n\nWe unroll and store it into array and index it by cumsum\n\nArray:\n```\n[0, 1, 9, 13, 1]\n```\n\nIndex:\n```\n[0, 2, 5]\n```\n\n## vertex_vertex adjacency\n### adjacency_list\n\n1. Loop through all faces with their vertex indices, for the vector of each vertex index, push back all other vertex indices\n2. sort and remove duplicates\n\n\n> My numpy implementation: [[Implementation|polygon.connect-polygons-with-their-offset-ones#implementation]]\n\n## edge_edge adjacency\n\n### facet_components\n\nCompute number of connected facets and assign each face an id based on its connected components\n\n1. Compute face adjacency matrix using [[facet_adjacency_matrix|code-read.igl.neighbourhood-connectivity#facet_adjacency_matrix]]\n2. Perform **breadth first search** on adjacency matrix row-wise, increase id when each row finished\n\n### facet_adjacency_matrix\n\n1. Call [[unique_edge_map|code-read.igl.neighbourhood-connectivity#unique_edge_map]], we get a list of oriented edges indices that share the same unique unoriented occurence\n2. From [[oriented_facets|code-read.igl.neighbourhood-connectivity#oriented_facets]], by modding oriented facets index over NF, we can get its face index\n3. Combine 1 and 2, we know which faces share the same edge, thus a NF x NF sparse adjacency matrix can be constructed\n\n### unique_edge_map\n\n1. Find all oriented edges using [[oriented_facets|code-read.igl.neighbourhood-connectivity#oriented_facets]] then filter using [[unique_simplices|code-read.igl.neighbourhood-connectivity#unique_simplices]]. (faster than directly finding unoriented edges) (`EMAP` is `IC` in [[unique_rows|code-read.igl.neighbourhood-connectivity#unique_rows]], used to construct all oriented edges from unique unoriented edges, 3NF)\n\n2. Same idea as [[Unrolled|code-read.igl.neighbourhood-connectivity#unrolled]], unroll a list of oriented edges indice (3NF) grouped by unique unoriented occurance, as well as a array of index, with each element be the number of oriented edges before the first of unique unoriented occurrence, NUE(number of unique unoriented edges) + 1\n\n|Unique edge index|edge indice in edge list|\n|:-------- |:------------------- |\n|0         | 0, 10               |\n|1         | 9, 13, 1            |\n\n```\nVectorXI uEK;\nigl::accumarray(EMAP,1,uEK);\n```\ncomputes occurance for each unique edges\n\n```\nuEC: cumsum of uEK as index\nuEO: array used for counting\ne: the e'th edge in oriented edge lists\nue = EMAP(e): index of unique unoriented edge corresponding to e'th edge in oriented edge lists\nuEE(uEC(ue)+ uEO(ue)) = e: record oriented edge index\n```\n\n### oriented_facets\n\nFor manifold triangle mesh\n\nGiven $F \\in \\mathbb{Z}^{n \\times 3}$, so that\n\n$$\nE = \n\\begin{bmatrix}\nF[:, 1] & F[:, 2] \\\\\nF[:, 2] & F[:, 0] \\\\\nF[:, 0] & F[:, 1] \\\\\n\\end{bmatrix}\n\\in \n\\mathbb{Z}^{3n \\times 2}\n$$\n\n### unique_simplices\n\nGiven N x D simplices matrix (D is the dimension of simplex)\n\n1. Sort row-wise\n2. Keep unique ones by calling [[unique_rows|code-read.igl.neighbourhood-connectivity#unique_rows]]\n\n### unique_rows\n\n```\nA: input\nC: unique rows\nIA: index to construct C from A\nIC: index to construct A from C\n```\n\nSimilary to `np.unique`, see [[snippets.remove_duplicate_vertices]]\n- `IA` -> `unique_indices`\n- `IC` -> `unique_indices`\n\n\n## TODO\n- [x] Manifold mesh","n":0.047}}},{"i":7,"$":{"0":{"v":"Mesh Parameterization","n":0.707},"1":{"v":"\n### boundary_loop\n1. Compute [[vertex_triangle_adjacency|code-read.igl.neighbourhood-connectivity#vertex_triangle_adjacency]]\n2. Compute [[triangle_triangle_adjacency|code-read.igl.mesh-parameterization#triangle_triangle_adjacency]]\n3. Compute [[is_border_vertex|code-read.igl.mesh-parameterization#is_border_vertex]], create a set for all border vertices\n4. Perform breadth-first-search to traverse border vertice till the edge loop is found. \n\n```\nenqueue one border vertex\nwhile not finished:\n    dequeue latest border vertex\n    for all its incident triangles:\n        if the triangle contains boundary edge:\n            enqueue the other vertex of the border edge\n``` \n\n### triangle_triangle_adjacency\n1. Compute [[vertex_triangle_adjacency|code-read.igl.neighbourhood-connectivity#vertex_triangle_adjacency]]\n2. Compute adjacency matrix `TT` (face index -> index of adjacent face)\n```\nfor each face for each corner vertex in face:\n    vi: current vertex\n    vin: next vertex in current face\n    for all incident faces of vi:\n        if incident face index does not equal to current face index:\n            if that incident face contains vin:\n                that face is adjacent to current face w.r.t. edge [vi vin] (share same edge)             \n```\n3. Compute adjacency matrix `TTi` (face index -> incident edge index in incident triangle)\n```\nfor each face for each corner vertex in face:\n    vi: current vertex\n    vj: next vertex in current face\n    fn: face adjacent to current face w.r.t. edge [vi vj]\n    if fn exists:\n        for each corner vertex of fn:\n            vin: current vertex in fn\n            vjn: next vertex in fn\n            if edge [vjn, vin] is [vi vj]:\n                record corner vertex index\n```\n\n### is_border_vertex\n1. Compute [[triangle_triangle_adjacency|code-read.igl.mesh-parameterization#triangle_triangle_adjacency]]\n2. Find edge of triangle that has not incident triangle, the two vertices of that edge are border vertices\n\n### map_vertices_to_circle\n1. Compute cumulated length for each boundary edges\n2. Treat total length as circumference of a unit circle\n3. Boundary uv coordinates are then $(cos, sin)$ of their portion w.r.t. circumference\n\n### local_basis\n\n```\nx = (v_1 - v_0).normalized()\nz = x.cross(v_2 - v_0).normalized()\ny = -x.cross(z).normalized()\n```\n\n### grad\n1. Compute edge vector `v32`, `v13`, `v21`\n2. Compute normal with cross product and double area with cross product magnitutude. See: [[Properties|linear-algebra.cross-product#properties]]\n3. Normalize normal by double area, or by building a equilateral triangle with double area and recompute normal\n> Not sure why use equilateral triangle\n4. Rotate edge vectors `v21` and `v13` 90 degree by performing cross product with normal (So the direction is pointing from vertex to it opposite edge and is also perpendicular to it). Then scale it by double area.\n5. Build sparse gradient operator matrix `3NF x NV`\n\nVector for each vertex in face\n```\nv1: -v13 - v21\nv2: v13\nv3: v21\n```\nStored in data structure like\n\n|  | NV |\n|--|:--:|\n|NF| x  |\n|NF| y  |\n|NF| y  |\n\n## TODO\n- [ ] Abstract equilateral triangle for normal computation","n":0.051}}},{"i":8,"$":{"0":{"v":"Matcap","n":1},"1":{"v":"\nMaterial Capture\n\n> Reference: https://github.com/nidorx/matcaps\n\nComplete material from image without actual compute reflection and lighting\n\n## Implementation\n\n### Normal matrix\n\nView matrix: W -> Camera\n\nSee: [[rendering.camera-mvp-matrix]] and [[linear-algebra.affine-transformation.multiplication.transform-normal-vector]]\n\n```\nnorm = view.inverse().transpose();\n```\n\n### Vertex shader\n\n```\nnormal_eye = vec3 (normal_matrix * vec4 (normal, 0.0));\nnormal_eye = normalize(normal_eye);\n```\n\n### Fragment shader\n\n```\nvec2 uv = normalize(normal_eye).xy * 0.5 + 0.5;\noutColor = texture(tex, uv);\n```\n","n":0.144}}},{"i":9,"$":{"0":{"v":"Marching Cubes","n":0.707},"1":{"v":"Construct vertices and faces from grid sample points. For each cube:\n1. Determine cube type by comparing each cube's 8 corner values with iso-surface value\n2. Query its vertex and face config from predetermined $2^8$-entry table\n\n## Implementation\n\n### Data structure\n- `EdgeKey`: 2 indice\n- `EdgeHash`: Hash function for `EdgeKey`\n- `edge2vertex`: std::unordered_map<EdgeKey, unsigned int>\n- `offset`: flattned index of following coordinates\n$$\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n1 & 0 & 0 \\\\\n1 & 1 & 0 \\\\\n0 & 1 & 0 \\\\\n0 & 0 & 1 \\\\\n1 & 0 & 1 \\\\\n1 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n\\end{bmatrix}\n$$\n\n### Constructor\n1. For each cube, obtain 8 corner flattened index using offset\n\n2. For each corner, query its value using flattened index, compare it with iso-surface value, determine its `cubetype` (char, 1 byte) by leftshift `1` n bit and apply logical OR\n\n3. Reject cubes with all positve/negative corner values\n\n4. Query `cubetype` in `edgeTable` to check which edge needs adding new vertex. Apply [[add_vertex|code-read.igl.marching-cubes#add_vertex]] if type matches\n\n5. Query `cubetype` in `triTable` to connect and add new faces\n\n### add_vertex\nCompute new vertex on given cube edge by linear interpolating cube edge corner values \n\n\n## TODO\n- [ ] edgeTable\n- [ ] triTable\n\n","n":0.072}}},{"i":10,"$":{"0":{"v":"Laplacian","n":1},"1":{"v":"\n## Concepts\n\n### Circumcenter\n> Reference: https://mathworld.wolfram.com/Circumcenter.html\n\nFor all edges, the line segement from circumcenter to edge midpoint is perpendicular to that edge. \n\nIt is the center of the triangle's circumcircle, with its radius equal to distance between circumcenter to all vertices.\n\nThe coordinate of circumcenter w.r.t. vertex angles (Trilinear Coordinates) can be written as:\n$$\nR \\cos A : R \\cos B: R \\cos C\n$$\n\n### Law of cosines\n$$\n\\cos A = \\frac{b^2 + c^2 - a^2}{2bc}\n$$\n\n###  Law of sines\n$$\n\\frac{\\sin A}{a} = \\frac{\\sin B}{b}\n$$\n\n### Cotangent\n\n$$\n\\cot \\theta = \\frac{1}{\\tan \\theta} = \\frac{\\cos \\theta}{\\sin \\theta}\n$$\n\n#### Formula\nFor a triangle of vertices $\\{A, B, C\\}$ with opposite edges $\\{a, b, c\\}$\n\nSuppose we know the aera of the triangle $Area$, the height $h$ of base $a$ is \n\n$$\nh = \\frac{Aera}{a} \\\\\n$$\nWe can express $\\sin B$ with $h$ as\n$$\n\\sin B = \\frac{h}{c} = \\frac{Aera}{ac}\n$$\n\nThen with [[Law of sines|code-read.igl.laplacian#law-of-sines]]\n\n$$\n\\sin A = \\frac{a \\sin B}{b} = \\frac{Aera}{bc}\n$$\n\nThus, combined with [[Law of cosines|code-read.igl.laplacian#law-of-cosines]]:\n\n$$\n\\cot A = \\frac{\\cos A}{\\sin A} = \\frac{b^2 + c^2 - a^2}{0.5 * Aera}\n$$\n\n## Implementation\n### cotmatrix\n1. For each face, for edges {(1, 2), (2, 0), (0, 1)} in face, compute [[cotmatrix_entries|code-read.igl.laplacian#cotmatrix_entries]] for triplets\n2. Form SparseMatrix by `setFromTriplets`. Note both orientations of same edge are appended, which will be [summed up](https://eigen.tuxfamily.org/dox/classEigen_1_1SparseMatrix.html#a8f09e3597f37aa8861599260af6a53e0) during sparse construction.\n\n**IMPORTANT!**\n\n$$\nL_{ij} = \n\\begin{cases}\n\\cot \\alpha_{ij} + \\cot \\beta_{ij} & j\\in N_1(i) \\\\\n0 & j\\notin N_1(i) \\\\\n-\\sum_{k \\neq i} L_ik & i=j\n\\end{cases}\n$$\n\nSuppose row $i$ of $L \\in \\mathbb{R}^{NV \\times NV}$ is\n$$\n\\begin{bmatrix}\nL_a & 0 & \\dots & 0 & L_b & 0 & \\dots & 0 &  -(L_a + L_b + L_c + L_d) & 0 & \\dots & 0 &  L_c & 0 & \\dots & 0 &  L_d\n\\end{bmatrix}\n$$\n$V \\in \\mathbb{R}^{NV \\times 3}$ is the matrix of vertices\n\nThen the row $i$ of $-L \\cdot V$ is\n\n$$\n\\begin{aligned}\n&\\begin{bmatrix}\n(L_a + L_b + L_c + L_d) \\cdot x_i - (L_a x_a + L_b x_b + L_c x_c + L_d x_d) \\\\\n(L_a + L_b + L_c + L_d) \\cdot y_i - (L_a y_a + L_b y_b + L_c y_c + L_d y_d) \\\\\n(L_a + L_b + L_c + L_d) \\cdot z_i - (L_a z_a + L_b z_b + L_c z_c + L_d z_d)\n\\end{bmatrix}^T \\\\\n=\n&\\begin{bmatrix}\nL_a(x_i - x_a) + L_b(x_i - x_b) + L_c(x_i - x_c) + L_d(x_i - x_d) \\\\\nL_a(y_i - y_a) + L_b(y_i - y_b) + L_c(y_i - y_c) + L_d(y_i - y_d) \\\\\nL_a(z_i - z_a) + L_b(z_i - z_b) + L_c(z_i - z_c) + L_d(z_i - z_d)\n\\end{bmatrix}^T \\\\\n= &\\sum_{j \\in \\{a, b, c, d\\}} L_{ij} (\\bm{x}_i - \\bm{x}_j)\n\\end{aligned}\n$$\n\nwhich is equivalent to the definition in [[Discrete Mean curvature Normal|geometry.discrete-laplace-operator#discrete-mean-curvature-normal]]\n\n### cotmatrix_entries\n1. Compute [[squared_edge_lengths|code-read.igl.laplacian#squared_edge_lengths]]\n2. Compute [[doublearea|code-read.igl.laplacian#doublearea]] with edge_length\n3. For `squared_edge_lengths` of each face {a^2, b^2, c^2}, compute cotangent with [[Formula|code-read.igl.laplacian#formula]] as `(a^2 + b^2 - c^2) / doublearea / 4.0`\n\n### squared_edge_lengths\nFor each triangle face, compute `squaredNorm` for `edge` in {(1, 2), (2, 0), (0, 1)}, shape of NF x 3\n\n### doublearea\nInput: NF x 3 edge length\n\nCompute double triangle area using **Kahan's Heron's formula** to avoid numerical issues\n1. Sort edge length `a`, `b`, `c`\n2. Compute `arg = (a + (b + c)) * (c - (a - b)) * (c + (a - b)) + (a + (b - c))`\n3. `doublearea = 2.0 * 0.25 * sqrt(arg)`\n4. Perform [[coding.cpp.nan-check]] and replace `NaN` with pre-specified replacement value\n\n\n### massmatrix\n1. Compute `edge_length`, NF x 3\n2. Call [[massmatrix_intrinsic|code-read.igl.laplacian#massmatrix_intrinsic]]\n\n### massmatrix_intrinsic\nCompute [[doublearea|code-read.igl.laplacian#doublearea]] $dblA \\in \\mathbb{R}^{NF \\times 3}$\n#### MASSMATRIX_TYPE_BARYCENTRIC\nFor $F = \n\\begin{bmatrix}\nc_0 & c_1 & c_2\n\\end{bmatrix} \\in \\mathbb{R}^{NF \\times 3}$, build SparseMatrix by $(x, y)$ indices of \n$$\n\\begin{bmatrix}\nc_0 \\\\\nc_1 \\\\\nc_2\n\\end{bmatrix} \\in \\mathbb{R}^{3NF}\n$$\nand value of (using [[repmat|code-read.igl.laplacian#repmat]])\n$$\n\\begin{bmatrix}\ndblA/6 \\\\\ndblA/6 \\\\\ndblA/6\n\\end{bmatrix} \\in \\mathbb{R}^{3NF \\times 3}\n$$\nA.k.a. each oriented edge (opposite vertex) has mass $dblA/6$.\n\nNote that the resulting SparseMatrix is a diagonal matrix with shape NV x NV\n\n#### MASSMATRIX_TYPE_VORONOI\nSame indices as above, but with voronoi triangle area computed as:\n1. Compute cosines using [[Law of cosines|code-read.igl.laplacian#law-of-cosines]]\n2. Divide triangle area for vertex by its cosine times opposite edge_length (length of projection of circumcenter to each edge is porportional to the cosine of opposite vertex angle)\n3. Handle [[Obtuse triangle|geometry.discrete-laplace-operator#obtuse-triangle]] as described in [[Discrete Mean curvature Normal|geometry.discrete-laplace-operator#discrete-mean-curvature-normal]]\n\n### repmat\nLike `np.repeat`, supporting SparseMatrix\n\n### gaussian_curvature\n\nSee [[Discrete Gaussian curvature|geometry.discrete-laplace-operator#discrete-gaussian-curvature]]\n\n### principal_curvature\n\n1. For each vertex, find k-ring/sphere neighbourhood within `k * average_edge_distance` using breadth first search\n2. Verify neighbourhood vertex normal direction (dot product with target vertex normal is positive)\n3. Compute normal via averaging neighbourhood vertices normal or using [[getProjPlane|code-read.igl.laplacian#getprojplane]]\n4. If has too many neighbourhood vertices, apply Monte Carlo to uniformly sample a subset, each with $\\frac{target_count}{current_vertex_count}$ chance to survive\n5. Build new basis, computed normal as z axis, projected direction from target vertex to one of its incident vertex as x axis, y is then cross product of x and z\n6. Center neighbourhood vertices w.r.t. target vertex, change to new basis and fit Quadric (height function fit, with polynoimal basis {xx, xy, yy, x, y}, z as target, solve least square with `Eigen::jacobiSvd`)\n> What's the relation between quadric fit and priciple tensor?\n\n7. Compute [[curvature tensor|geometry.discrete-laplace-operator#pre-vertex-principle-curvatures]] from quadric fit and tranform to normal coordinate system\n\n#### getProjPlane\n1. Compute sum of target vertex incident faces normal, or sum of target vertex neighbourhood vertices normals, as the original normal\n2. Right shift coordinate (a, b, c) n times so abs(c) is the biggest\n3. Normalize coordinate to $(\\frac{a}{\\sqrt{a^2 + b^2 + c^2}}, \\frac{b}{\\sqrt{a^2 + b^2 + c^2}}, \\frac{\\sqrt{1 - a^2 - b^2}}{\\sqrt{a^2 + b^2 + c^2}})$, left shift it back as the tmp normal\n4. Loop through all $\\pm$ combination of tmp normal, return one that is closest to original normal (maximum dot product)\n\n## TODO\n- [ ] [Kahan's Heron's formula](https://people.eecs.berkeley.edu/~wkahan/Triangle.pdf)\n- [ ] Triangle edge length inequality\n- [x] Cotangent formula\n- [ ] Idea behind [[getProjPlane|code-read.igl.laplacian#getprojplane]]\n- [ ] Relation between quadric fit and curvature tensor","n":0.033}}},{"i":11,"$":{"0":{"v":"Colormap","n":1},"1":{"v":"\nMaps value given color palette\n\n## Implementation\n### colormap\n\n```\npalette p: 256 x 3 (RGB8) array\nfactor f: [0, 1]\n\nmapped_f = f * (256 - 1)\na = floor(f)\nb = ceil(f)\n\n# apply linear interpolation for rgb\nc = mix(p[a], p[b], mapped_f - a)\n```\n\n### Fragment shader\n\nSubstitude material diffuse [[Phong lighting model|rendering.rendering-terms#phong-lighting-model]]\n\n> Bind buffer\n\n```\nbind_vertex_attrib_array(shader_mesh,\"Kd\", vbo_V_diffuse, V_diffuse_vbo, dirty & MeshGL::DIRTY_DIFFUSE);\n```\n> Vertex shader (for interpolation)\n\n```\nKdi = Kd;\n```\n\n> Fragment shader\n\n```\nvec4 color = vec4(lighting_factor * (Is + Id) + Ia + (1.0-lighting_factor) * vec3(Kdi),(Kai.a+Ksi.a+Kdi.a)/3);\n```","n":0.118}}},{"i":12,"$":{"0":{"v":"Filament","n":1}}},{"i":13,"$":{"0":{"v":"Renderer","n":1},"1":{"v":"## Prepare\n```mermaid\ngraph TD;\n    View::prepare-->|View::mCullingCamera|cullFrustum;\n    View::prepare-->|invIblRotation & invCameraTranslation|worldOriginScene;\n    worldOriginScene-->Scene::prepare\n    Scene::prepare-->|renderableInstance & transformInstance|Scene::sceneData;\n    Scene::prepare-->|lightInstance|Scene::lightData;\n    cullFrustum-->|cullSpotLight|View::lightCulling;\n    Scene::sceneData-->View::renderableCulling;\n    cullFrustum-->|cullRenderable|View::renderableCulling;\n    View::lightCulling-->ShadowMap;\n    Scene::lightData-->ShadowMap;\n    ShadowMap-->|ShadowCamera|ShadowCullFrustum;\n    ShadowCullFrustum-->|cullShadowCaster|View::ShadowCasterCulling;\n    Scene::sceneData-->View::ShadowCasterCulling;\n    View::renderableCulling-->View::groupRenderableBasedOnVisibility;\n    View::ShadowCasterCulling-->View::groupRenderableBasedOnVisibility;\n    View::groupRenderableBasedOnVisibility-->renderable;\n    View::groupRenderableBasedOnVisibility-->renderable+directionalLightShadowCaster;\n    View::groupRenderableBasedOnVisibility-->directionalLightShadowCaster;\n    View::groupRenderableBasedOnVisibility-->punctualLightShadowCaster;\n    View::groupRenderableBasedOnVisibility-->invisibleRenderable;\n    renderable-->visibleRenderable;\n    renderable+directionalLightShadowCaster-->visibleRenderable;\n    directionalLightShadowCaster-->visibleRenderable;\n    punctualLightShadowCaster-->visibleRenderable;\n    visibleRenderable-->updateRenderableUBOs;\n    updateRenderableUBOs-->endPrepare\n    Scene::lightData-->updateLightUBOs\n    updateLightUBOs-->endPrepare\n```\n","n":0.169}}},{"i":14,"$":{"0":{"v":"Audio","n":1}}},{"i":15,"$":{"0":{"v":"Formant Estimation","n":0.707}}},{"i":16,"$":{"0":{"v":"Formant Estimation with LPC Coefficients","n":0.447},"1":{"v":"\nThe core idea: formant frequency equals to the roots (a.k.a. coefficients) of linear predictive coding (LPC) prediction polynomial. [Matlab Reference](https://www.mathworks.com/help/signal/ug/formant-estimation-with-lpc-coefficients.html?s_tid=srchtitle).\n\n## Spectrogram\nUse spectrogram to identify segment for analysis. Spectrogram is obtained by using short-time fourier transform (STFT)\n- x axis: time\n- y axis: frequency \n\n## Fourier analysis and sampling\n\nFourier analysis: time domain $\\rightarrow$ frequency domain.\n- Continuous Time Fourier transform (CTFT): Find spectrum of continuous time signal\n- Discrete Time Fourier Transform (DTFT): Find the spectrum of sampled version of continuous time signal\n- Discrete Fourier Transform (DFT): Find the frequency spectrum of a discrte-time signal (finite sample). \n  - Fast Fourier Transform computes DFT faster ($O(N \\log N)$ vs. $O(N^2)$).\n\n  - Nyquist–Shannon sampling theorem: Sufficient sampling frequency is anything larger than 2 times highest frequency of function to be sampled.\n\n  - Nyquist frequency: half of the sampling frequency\n\n## Windowing \n\nFFT assume circular topologies in both time domain and frequency domain (Two endpoints are interpreted to be connected). This assumption doesn't hold for non-integer period measurement (discontinuous endpoints), introducing high-frequency alias (much higher than Nyquist frequency). It produces smeared spectrum, with the phenomenon known as spectral leakage,\n\nWindowing reduces amplititude of discontinuities at the boundary, by multiplying a finite-length window in **time domain** with an amplitude that varies smoothly and gradually toward zero at the edges.\n\n[More details](https://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf)\n\n## Pre-emphasis\n\nIncease the magnitude of some (usually high) frequencies with respect to manitude of other (usually low) frequencies to improve overall signal-to-noise ratio, such as apply high pass IIR filter\n\n- Infinite impulse response filter(IIR filter): impulse response doesn't reach zero indefinitely \n- Finite impulse response filter(FIR filter): impulse response reaches zero in finite time\n\n## IPC\n\n[Linear prediction speech model](http://scribblethink.org/Work/lipsync91/lipsync91.pdf) models speech signal $s_t$ as a exicitation signal $\\alpha x_t$ plus a weighted sum of the input and past outputs. General rule: order ${p}$ equals two times number of expected vowels plus two.\n\n$$\n{s_t = \\alpha x_t + \\sum_{k=1}^P} a_k s_{t-k}\n$$\n\nThe exicitation signal $x_t$ results in vowel and coefficients $a_k$ are assumed to be constant during a short interval.\n\n\n","n":0.055}}},{"i":17,"$":{"0":{"v":"Unreal5","n":1}}},{"i":18,"$":{"0":{"v":"Nanite","n":1},"1":{"v":"\n[Keynotes](https://advances.realtimerendering.com/s2021/Karis_Nanite_SIGGRAPH_Advances_2021_final.pdf)\n\n## Possible mesh alternatives\n- voxelization is a form of uniform resampling and uniform resampling means loss\n- subdivision by definition is amplification only\n- displacement can’t increase the genus of a surface\n- projecting to normal or displacement maps is a form of uniform resampling\n- points require hole filling, but it's impossible to know the difference between a small gap that should be there and a hole that should be filled for certain without extra connectivity data\n\n## Nanite mesh virtualization\n- group triangles into clusters and build a bounding box for each cluster then cull the clusters based on their bounds\n- \"What was visible last frame is very likely to be what is visible this frame.\" 1. Draw what was visible in the previous frame 2. Build hierarchical Z-buffer (HZB) from that 3. Test the HZB to determine what is visible now but wasn’t in the last frame and draw anything that’s new\n- decouple visibility from materials, use pixel level visibility buffer (deferred renderer)\n- In general the cost of rendering geometry should scale with screen resolution, not scene complexity. That means constant time in terms of scene complexity and constant time means \nlevel of detail\n- do level of detail with clusters, find a cut of the tree that matches the desired LOD, in a view dependent way based on the screen space projected error of the cluster\n- group clusters and force them to make the same LOD decision for a level, request data based on demand\n- lock the shared boundary edges between clusters during simplification, alternate group boundaries from level to level by grouping different clusters\n- edge collapsing decimation, optimize for minimal error in new positions and attributes, projected on screen to a number of pixels of error to determine which LOD to select so is foundational to both quality and efficiency\n","n":0.058}}},{"i":19,"$":{"0":{"v":"Survey","n":1}}},{"i":20,"$":{"0":{"v":"Structure from Motion","n":0.577},"1":{"v":"\n[Leveraging Deep Visual Descriptors for Hierarchical Efficient Localization](https://arxiv.org/pdf/1809.01019.pdf)\n\n1. keyframes -> distilled VLAD -> global descriptors (prior frames)\n2. group prior frames by covisibility, as cluster of places\n3. for each places, for query frame, match 3d-2d correspondency\n\nFurther reading\n- [ ] VLAD, image retrieval\n- [ ] distillation\n\n[From Coarse to Fine](https://arxiv.org/pdf/1812.03506.pdf)\n\n1. global, coarse search followed by a fine pose estimation\n2. hierarchical features(HF) net, compute local and global feature in one shot\n\nFurther reading\n- [ ] [P3p](https://rpg.ifi.uzh.ch/docs/CVPR11_kneip.pdf)\n- [ ] [SuperPoint](https://arxiv.org/pdf/1712.07629.pdf) more sparser, faster than transposed convolution\n- [ ] [LF-Net](https://arxiv.org/pdf/1805.09662.pdf)\n- [ ] multitask [distillation](https://arxiv.org/pdf/1503.02531.pdf)\n\n[SuperGlue](https://arxiv.org/pdf/1911.11763.pdf)\n\nGraph neural network for feature matching\n\nFurther reading\n- [ ] GNN\n- [ ] Attention\n- [ ] dustbin matching\n- [ ] [Sinkhorn Distances](https://proceedings.neurips.cc/paper/2013/file/af21d0c97db2e27e13572cbf59eb343d-Paper.pdf)\n\n[Pixel-Perfect Structure-from-Motion with Featuremetric Refinement](https://arxiv.org/pdf/2108.08291.pdf)\n","n":0.095}}},{"i":21,"$":{"0":{"v":"Snippets","n":1}}},{"i":22,"$":{"0":{"v":"Tweak Blink Detection","n":0.577},"1":{"v":"\n```\n  +--------(eye open)--------+\n  |                          |\n  v                          |\nOPEN ---(eye close)---> CLOSE_WAIT\n  ^                          |\n  |                 (eye still close)\n(eye still open)             |\n  |                          v\nOPEN_WAIT <--(eye open)-- CLOSE\n  |                          ^\n  |                          |\n  +-------(eye close)--------+\n```\n\n- Apply [[snippets.adaptive-threshold]]\n- Track velocity to filter out untrustworthy data\n- Add `WAIT` states for FSM\n- Sync left/right eye by close/open both eye when all/one of them are in CLOSE_WAIT/OPEN_WAIT state (depends if ignore single blink)\n\n","n":0.125}}},{"i":23,"$":{"0":{"v":"Rotation from Vector to Vector","n":0.447},"1":{"v":"\n```\n// Reference: https://math.stackexchange.com/a/897677\nmat3f rotation(float3 fromVec, float3 toVec){\n    float3 a = fromVec;\n    float3 b = toVec;\n    float dotProd = dot(a, b);\n    float3 crossProd = cross(a, b);\n    float cos = dotProd;\n    float sin = norm(crossProd);\n    // pure rotation matrix\n    auto G = mat3f(float3(cos, sin, 0), float3(-sin, cos, 0), float3(0, 0, 1));\n    // three orthogonal basis the rotation applied upon\n    // 1. normalized vector projection of b on a\n    // 2. normalized vector rejection of b on a\n    // 3. cross product of b and a\n    // the basis change matrix is actually F^-1\n    auto F = mat3f(a, normalize(b - cos * a), normalize(crossProd));\n    // orthonormal basis form orthogonal matrix\n    mat3f R = F * G * transpose(F);\n    return R;\n}\n```\n","n":0.092}}},{"i":24,"$":{"0":{"v":"Rotate Landmarks in Normalized Coordinate","n":0.447},"1":{"v":"\n```\nImage coordinate                  Normalized coordinate\n0------------+------ img_w        0------------------+-----------1\n|            |           |        |                  |           |\n|            |           |        |                  |           |\n|            |           |        |                  |           |\n|            |           |        |                  |           |\n|           y|           |        |               y_n|           |\n|       +-------------+  |        |         +-----------------+  |\n|       |     ^       |  | ======>|         |        ^        |  |\n|   x   |    h|       |  |        |   x_n   |     h_n|        |  |\n+------>|     v       |  |        +------>  |        v        |  |\n|       +-------------+  |        |         +-----------------+  |\n|       <------------->  |        |         <----------------->  |\n|            w           |        |                 w_n          |\nimg_w -------------------+        1------------------------------+\n```\n\n- Move coordinate from top left to center\n- Scale coordinate such that x, y axis units are equally scaled `x = x / img_w * img_h`\n- Apply 2d rotation matrix $\\begin{bmatrix}\\cos(\\theta) & \\sin(\\theta)\\\\-\\sin(\\theta) & \\cos(\\theta)\\end{bmatrix}$\n- Scale coordinate back `x = x / img_h * img_w`\n- Move coordinate back from center to top left\n","n":0.087}}},{"i":25,"$":{"0":{"v":"Remove_duplicate_vertices","n":1},"1":{"v":"\n```\n_, unique_indices, unique_inverse = np.unique(points, return_inverse=True, return_index=True, axis=0)\nfaceVertexIndices = Vt.IntArray((unique_indices[unique_inverse])[faceVertexIndices].tolist())\n```\n\n- `unique_indices` The indices of the first occurrences of the - unique values in the original array.\n- `unique_inverse` The indices to reconstruct the original array from the unique array.\n- `unique_indices[unique_inverse]`: same shape as original array, but with each element be the index of the first occurrences of the unique value\n","n":0.13}}},{"i":26,"$":{"0":{"v":"Float Transitionor","n":0.707},"1":{"v":"Transition a number for a period of time over a single loop call\n\n## Implementation\n```\nclass FloatTransitioner: NSObject {\n    enum TransitionState {\n        case idle, waiting, transitioning\n    }\n\n    private var state: TransitionState = .idle\n    private var targetVal: Float = 0\n    private var currentVal: Float = 0\n    private var waitingDuration: Float = 0\n    private var waitingDurationElapsed: Float = 0\n    private var transitionIncrement: Float = 0\n    private var finishCallback: (() -> Void)?\n\n    func schedule(targetVal: Float,\n                  waitingDuration: Float?,\n                  transitionDuration: Float,\n                  finishCallback: (() -> Void)?) {\n        if self.currentVal == targetVal {\n            state = .idle\n            finishCallback?()\n            return\n        }\n\n        if transitionDuration <= 0 { return }\n\n        self.targetVal = targetVal\n        self.transitionIncrement = (targetVal - self.currentVal) / transitionDuration\n        self.finishCallback = finishCallback\n        self.state = .transitioning\n\n        if let waitingInterval = waitingDuration {\n            if waitingInterval > 0 {\n                self.waitingDuration = waitingInterval\n                self.waitingDurationElapsed = 0\n                state = .waiting\n            }\n        }\n    }\n\n    func update(_ deltaTime: Float) -> Float? {\n        switch state {\n        case .idle:\n            return nil\n        case .waiting:\n            waitingDurationElapsed += deltaTime\n            if waitingDurationElapsed >= waitingDuration {\n                state = .transitioning\n            }\n            return currentVal\n        case .transitioning:\n            currentVal += deltaTime * transitionIncrement\n            if sign(transitionIncrement) * (currentVal - targetVal) > 1e-5 {\n                currentVal = targetVal\n                state = .idle\n                self.finishCallback?()\n            }\n            return currentVal\n        }\n    }\n\n    func release() {\n        self.state = .idle\n        self.finishCallback = nil\n    }\n}\n\n```","n":0.07}}},{"i":27,"$":{"0":{"v":"Distance to Line Segment","n":0.5},"1":{"v":"> Reference: https://stackoverflow.com/questions/849211/shortest-distance-between-a-point-and-a-line-segment\n\nCompute distance from point p to line segment defined by w, w.\n```\n// compute projected distance by dot product\nvar t = p.sub(v).dot(w.sub(v).normalize());\n// the crucial step, as we are dealing with segment here\nt = Math.min(Math.max(t, 0), 1.0));\n// compute the projected point\nconst proj = v.add(w.sub(v).normalize().mul(t);\n// compute distance to projected point\nconst d = p.sub(proj).norm();\n```","n":0.139}}},{"i":28,"$":{"0":{"v":"Classification Override","n":0.707},"1":{"v":"\nAllow infrequent to override frequent class for more diverse results\n\n```\n// Row: query_key\n// Cols: current_key\n// visemeOverride[query_key][current_key] is query_key allowed to override current key\nconst visemeOverride: Boolean[][] = [[true, true, true, true, false], [false, true, true, true, false], [false, false, true, true, false], [false, false, false, true, false], [true, true, true, true, true]];\n\nconst visemeDuration: Viseme = { A: 4, E: 3, I: 2, O: 1, U: 5 }; // number of ticks each key is expected to last\n","n":0.115}}},{"i":29,"$":{"0":{"v":"Adaptive Threshold","n":0.707},"1":{"v":"> Similar idea: https://stackoverflow.com/questions/22583391/peak-signal-detection-in-realtime-timeseries-data\n\n- Apply [AlphaFiltering](https://en.wikipedia.org/wiki/Alpha_beta_filter) to raw data\n- Apply [Welford's online algorithm](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance) to compute online mean and variance from filtered data (ignore first few observations until statistics stabilized)\n- Detect using `mean +/- 3 * std` as threshold\n","n":0.162}}},{"i":30,"$":{"0":{"v":"Rendering","n":1}}},{"i":31,"$":{"0":{"v":"Raycast","n":1}}},{"i":32,"$":{"0":{"v":"Oriented Bounding Capsule (OBC)","n":0.5},"1":{"v":"\n> Reference: https://iquilezles.org/www/articles/intersectors/intersectors.htm\n\n```\nfun hitTestOrientedBoundingCapsule(capsule: Capsule): Boolean {\n        val ao = capsule.pa - origin\n        val bo = capsule.pb - origin\n\n        val a = dot(ao, direction)\n        val b = dot(bo, direction)\n\n        return sqrt(dot(ao, ao) - a * a) <= capsule.ra || sqrt(dot(bo, bo) - b * b) <= capsule.ra\n    }\n\n    fun intersectOrientedBoundingCapsule(capsule: Capsule): Float? {\n        val pa = capsule.pa\n        val pb = capsule.pb\n        val ra = capsule.ra\n        val ro = origin\n        val rd = direction\n\n        val ba = pb - pa\n        val oa = ro - pa\n        val baba = dot(ba, ba)\n        val bard = dot(ba, rd)\n        val baoa = dot(ba, oa)\n        val rdoa = dot(rd, oa)\n        val oaoa = dot(oa, oa)\n        val a = baba - bard * bard\n        var b = baba * rdoa - baoa * bard\n        var c = baba * oaoa - baoa * baoa - ra * ra * baba\n        var h = b * b - a * c\n        if (h >= 0f) {\n            val t = (-b - sqrt(h)) / a\n            val y = baoa + t * bard\n            // body\n            if (y> 0f && y <baba) return t\n            // caps\n            val oc = if (y <= 0f) oa else ro - pb\n            b = dot(rd, oc)\n            c = dot(oc, oc) - ra * ra\n            h = b * b - c\n            if (h> 0f) {\n                return -b - sqrt(h)\n            }\n        }\n        return null\n    }\n\n```\n","n":0.066}}},{"i":33,"$":{"0":{"v":"Oriented Bounding Box (OBB)","n":0.5},"1":{"v":"\n> Reference: http://www.opengl-tutorial.org/miscellaneous/clicking-on-objects/picking-with-custom-ray-obb-function/\n\n```\nfun intersectOBBox(scaledHalfExtend: Float3, modelMatrix: Mat4): Float? {\n        // farthest near intersection distance\n        var dMin = 0.0f\n        // nearest far intersection distance\n        var dMax = 100000.0f\n\n        // vector from origin To OBB Center\n        val delta = modelMatrix.position - origin\n\n        // intersection check for two planes perpendicular to x/y/z axis separately\n        listOf(modelMatrix.x.xyz, modelMatrix.y.xyz, modelMatrix.z.xyz).map { normalize(it) }.zip(listOf(scaledHalfExtend.x, scaledHalfExtend.y, scaledHalfExtend.z)).forEach { (axis, extend) ->\n            val deltaProj = dot(axis, delta)\n            val cosTheta = dot(direction, axis)\n\n            if (abs(cosTheta) > 0.001f) {\n                // distance to near intersection\n                var dNear = (deltaProj - extend) / cosTheta\n                // distance to far intersection\n                var dFar = (deltaProj + extend) / cosTheta\n\n                // swap if needed\n                if (dNear > dFar) {\n                    dNear = dFar.also { dFar = dNear }\n                }\n\n                dMin = dMin.coerceAtLeast(dNear)\n                dMax = dMax.coerceAtMost(dFar)\n\n                if (dMin > dMax) {\n                    return null\n                }\n            } else {\n                // ray perpendicular to plane normal a.k.a. parallel to plane\n                // (ruichen): I don't understand logic here\n                if (-deltaProj - extend > 0.0f || -deltaProj + extend < 0.0f) {\n                    return null\n                }\n            }\n        }\n        return dMin\n    }\n\n```\n","n":0.075}}},{"i":34,"$":{"0":{"v":"Gltf","n":1}}},{"i":35,"$":{"0":{"v":"Skinning","n":1},"1":{"v":"\n## Skinned mesh data structure\n```\nNode {\n    skin: Skin\n    mesh: Mesh\n    // transformation defined by matrix or rotation/scale/translation\n    matrix: mat4\n    rotation: vec4\n    scale: vec3\n    translation: vec3\n}\n\nSkin {\n    // len(inverseBindMatrices) == len(joints)\n    inverseBindMatrices: [mat4f]\n    joints: [int]\n    ...\n}\n\nMesh {\n    primitives: [Primitive]\n    ...\n}\n\nPrimitive {\n    attributes {\n        POSITION: [vec3]\n        NORMAL: [vec3]\n        // len(JOINTS_n) == len(WEIGHTS_n), usually 4\n        JOINTS_n: [int]\n        WEIGHTS_n: [float]\n        ...\n    }\n    indices: [int]\n    ...\n}\n\n```\n\n## Skinning matrix\n\n### Joint matrix $T_{Joint}$\n- $T_{WorldMesh}$: Mesh global transformation matrix\n- $T_{WorldJoint}$: Joint global transformation matrix\n- $T_{JointMesh}$: Inverse bind matrix\n\n$$\nT_{Joint} = {T_{WorldMesh}}^{-1} T_{WorldJoint} T_{JointMesh}\n$$\n\nGlobal transformation can be computed via chain multiplication. $T_{Joint}$ computation involves the transformation of coordinate system.\n$T_{mesh}$ -> $T_{joint}$ -> $T_{mesh}$. See: [[Transformation of coordinate system|linear-algebra.affine-transformation.multiplication.chain-multiplication#transformation-of-coordinate-system]]\n\n### Skin Matrix $T_{Skin}$\n- $T_{Joint\\ i}$: Joint matrix for joint i\n- $W_{Joint\\ i}$: Weight for Joint i\n\n$$\nT_{Skin} = W_{Joint\\ 0} T_{Joint\\ 0} + W_{Joint\\ 1} T_{Joint\\ 1} + \\ldots + W_{Joint\\ n} T_{Joint\\ n}\n$$\n\n$T_{Skin}$ is in mesh/vert coordinate, per primitive.\n","n":0.082}}},{"i":36,"$":{"0":{"v":"Filament","n":1}}},{"i":37,"$":{"0":{"v":"Image Based Lighting","n":0.577},"1":{"v":"> Reference: https://google.github.io/filament/Filament.html#lighting/imagebasedlights/processinglightprobes\n\n## Primitives\n\n**Environment mapping(reflection mapping)**: method for producing environment based reflections on surfaces of reflective objects\n\n**Environment map**: image representation of the environment surrounding a point in a scene, can be further processed to shade object based on surrounding lighting conditions in image based lighting\n\n**Cube (environment) map**: a type of environment map, a texture that contains 6 individual 2D textures that each form one side of a cube\n\n**Image based lighting (IBL)**: a technique for environment based lighting that efficiently computes lighting while shading objects with the utilization of pre-processed environment map\n\n**Incident light**: light falls on object's surface\n\n**Irradiance**: the contribution of the whole environment light to a given point on the object's surface \n\n**Radiance**: the resulting light bouncing off of the object\n\n## Image based lighting for physical based rendering\n\n**Rendering equation**:\n\nOutgoing radiance = incoming radiance + integration of the surface response for incident light from a single direction over entire hemisphere\n\n**Bidirectional Reflectance Distribution Function (BRDF)** describe surface response of a standard function over two terms:\n- diffuse component \n- specular component \n\n**Diffuse computation (invariant to view angle)**: irradiance map + spherical harmonics\n\nIrradiance integral is precomputed using Lambertian BRDF and stored into a **cubemap**. It can be approximated using spherical harmonics decomposition for cheap runtime calculation\n\n**Specular computation**: pre-filtered importance sampling + split-sum approximation\n\nRadiance integral = [DFG1(n.dot(v), a) + DFG2(n.dot(v), a)] x LD(n, a)\n\nwhere, n is surface normal vector, v is viewing vector, a relates to perceptual roughness\n\nDFG1 and DFG2 can be pre-calculated in a 2D texture or analytically approximated at runtime (on android); LD can be stored in **mipmaped cubemaps** (more rough, more blur)\n\n## Filament IBL Requirements\n\nIndirect light reflection requires (log2(width) + 1) levels mip-mapped **cubemap** generated by cmgen, or prefiltered with [generatePrefilterMipmap (cpu)](https://github.com/google/filament/blob/main/filament/include/filament/Texture.h#L468) or [libibl (gpu)](https://github.com/google/filament/tree/main/libs/iblprefilter), with each cubemap encodes the irradiance for a roughness level\n","n":0.058}}},{"i":38,"$":{"0":{"v":"Android","n":1}}},{"i":39,"$":{"0":{"v":"Graphics Architecture","n":0.707},"1":{"v":"\n- OpenGL: Graphics API (standard/specification)\n- OpenGL loading library: Loads pointers to OpenGL functions from OS specific drivers\n- OpenGL ES: OpenGL for embedded system\n- EGL: Interface between OpenGL ES and native platform window system (provide OpenGL context and an application window to draw in)\n\nReference: https://source.android.com/devices/graphics/architecture\n\n![](/assets/images/2021-12-21-10-34-00.png)","n":0.151}}},{"i":40,"$":{"0":{"v":"GLES Draw Bitmap","n":0.577},"1":{"v":"\n```\n// Vertex Shader\nval vertexShaderCode =\n        \"attribute vec4 vPosition;\" +\n            \"attribute vec4 vInputTextureCoordinate;\" +\n            \"varying vec2 vTextureCoordinate;\" +\n            \"void main() {\" +\n            \"    gl_Position = vPosition;\" +\n            \"    vTextureCoordinate = vInputTextureCoordinate.xy;\" +\n            \"}\"\n\n// Fragment Shader\nval fragmentShaderCode =\n        \"varying highp vec2 vTextureCoordinate;\" +\n            \"uniform sampler2D logoFrame;\" +\n            \"void main() {\" +\n            \"    gl_FragColor = texture2D(logoFrame, vTextureCoordinate);\" +\n            \"}\"\n\nfun loadShader(type: Int, shaderCode: String): Int {\n        return GLES20.glCreateShader(type).also { shader ->\n            GLES20.glShaderSource(shader, shaderCode)\n            GLES20.glCompileShader(shader)\n        }\n    }\n\n// const\nprivate const val COORDS_PER_VERTEX = 3\nprivate const val COORDS_PER_TEXTURE = 2\nprivate const val DRAW_COUNT = 6\nprivate const val BYTES_PER_FLOAT = 4\nprivate const val BYTES_PER_INT = 4\nprivate const val BYTES_PER_COORDS = 2\n\n// bitmap to draw\nval Bitmap: Bitmap\n\n// texture for bitmap\nval textureHandle: IntBuffer.allocate(1)\n\n// OpenGL texture coordinate\n//\n// (0, 1) ---- (1, 1)\n//    |          |\n//    |          |\n//    |          |\n// (0, 0) ---- (1, 0)\n\nval squareCoords = floatArrayOf(\n        buttonLeftX, topRightY, 0.0f, // top left\n        buttonLeftX, buttonLeftY, 0.0f, // bottom left\n        topRightX, buttonLeftY, 0.0f, // bottom right\n        topRightX, topRightY, 0.0f // top right\n    )\n\nval quadVertexHandle = ByteBuffer.allocateDirect(squareCoords.size * BYTES_PER_FLOAT).run {\n        order(ByteOrder.nativeOrder())\n        asFloatBuffer().apply {\n            put(squareCoords)\n            position(0)\n        }\n    }\n\nval drawOrderHandle: ShortBuffer\n    by lazy {\n        val drawOrder = shortArrayOf(0, 1, 2, 0, 2, 3)\n        ByteBuffer.allocateDirect(drawOrder.size * BYTES_PER_COORDS).run {\n            order(ByteOrder.nativeOrder())\n            asShortBuffer().apply {\n                put(drawOrder)\n                position(0)\n            }\n        }\n    }\n\n\n// create gl program\nvar program = GLES20.glCreateProgram().also {\n        GLES20.glAttachShader(it, loadShader(GLES20.GL_VERTEX_SHADER, vertexShaderCode))\n        GLES20.glAttachShader(it, loadShader(GLES20.GL_FRAGMENT_SHADER, fragmentShaderCode))\n        GLES20.glLinkProgram(it)\n    }\n\n// load bitmap into texture\nGLES20.glGenTextures(1, textureHandle)\nGLES20.glBindTexture(GLES20.GL_TEXTURE_2D, textureHandle[0])\nGLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MIN_FILTER, GLES20.GL_LINEAR)\nGLES20.glTexParameteri(GLES20.GL_TEXTURE_2D, GLES20.GL_TEXTURE_MAG_FILTER, GLES20.GL_LINEAR)\nGLUtils.texImage2D(GLES20.GL_TEXTURE_2D, 0, GLES20.GL_RGBA, bitmap, 0)\n\n// draw\nGLES20.glUseProgram(program)\nval vertexPosHandle = GLES20.glGetAttribLocation(program, \"vPosition\")\nGLES20.glEnableVertexAttribArray(vertexPosHandle)\nGLES20.glVertexAttribPointer(vertexPosHandle, COORDS_PER_VERTEX, GLES20.GL_FLOAT, false, COORDS_PER_VERTEX * 4, quadVertexHandle)\n\nval texturePosHandle = GLES20.glGetAttribLocation(program, \"vInputTextureCoordinate\")\nGLES20.glEnableVertexAttribArray(texturePosHandle)\nGLES20.glVertexAttribPointer(texturePosHandle, COORDS_PER_TEXTURE, GLES20.GL_FLOAT, false, COORDS_PER_TEXTURE * 4, textureHandle)\n\nGLES20.glEnable(GLES20.GL_BLEND)\nGLES20.glBlendFunc(GLES20.GL_ONE, GLES20.GL_ONE_MINUS_SRC_ALPHA)\n\nval logoHandle = GLES20.glGetUniformLocation(program, \"logoFrame\")\nGLES20.glActiveTexture(GLES20.GL_TEXTURE0)\nGLES20.glBindTexture(GLES20.GL_TEXTURE_2D, textureHandle[0])\nGLES20.glUniform1i(logoHandle, 0)\n\nGLES20.glDrawElements(GLES20.GL_TRIANGLES, DRAW_COUNT, GLES20.GL_UNSIGNED_SHORT, drawOrderHandle)\n\nGLES20.glDisableVertexAttribArray(vertexPosHandle)\nGLES20.glDisableVertexAttribArray(texturePosHandle)\nGLES20.glDisablbe(GLES20.GL_BLEND)\n\n// release\nGLES20.glDeleteTextures(1, textureHandle)\nGLES20.glDeleteProgram(program)\n```","n":0.062}}},{"i":41,"$":{"0":{"v":"Shading","n":1},"1":{"v":"\n## Vertex & Fragment shader\n```\nVertex Shader -> every vertex\n |\n | Coordinate interpolation\n v\nFragment Shader -> every pixel\n```\n\n## Data types\n- `const`: compile time constant\n- `uniform` (per-primitive): constant during entire draw call\n\n### Before 4.2\n- `attribute` (per-vertex): position, normal, uv, etc.\n- `varying` (per-fragment): Interpolated between vertex and fragment shader\n\n### 4.2+\n\n> Both `attribute` and `varying` are deprecated\n\n- `in`: linkage in from previous stage\n- `out`: linkage out to next stage\n\n> Reference: https://gamedev.stackexchange.com/questions/29672/in-out-keywords-in-glsl\n\n\n## Pass data from Vertex to Fragment shader\n\n### Before 4.2\nDeclare `varying` variable in both shaders\n\n\n### 4.2+\nDeclare `out` for vertex shader, `in` for fragement shader\n\n## Render order\n- Opaque: front to back\n- Transparent: back to front\n\n> Portal effect: Render occlusion object first (write depth but no color), then render the rest. Fragment further than occlusion depth will be discarded.","n":0.09}}},{"i":42,"$":{"0":{"v":"Tips","n":1},"1":{"v":"\n## Optimization\n\nIt's worth it to reduce texture sampling count at the cost of increasing logic complexity (more `if`s)\n","n":0.236}}},{"i":43,"$":{"0":{"v":"Rendering Terms","n":0.707},"1":{"v":"\n## Incident light\nLight falls on object surface\n\n## Specular reflection\nMirror like reflection, determined by the angle between reflection vector(reflection of light vector on normal) and view vector (direction from view position to fragement position)\n\n## Diffuse reflection\nThe reflection of light from a surface such that a ray incident on the surface is scattered as many angles (contrast to specular reflection). The brightness is the same regardless of viewing angle, but will change if surface is tilted away from light direction, as its incident light change. Usually calculated by dot product of light direction and normal.\n\n## Albedo\n1. Physics: The proportion of incident light that is reflected away from a surface\n2. PBR: base color without shadows (Diffuse in non-PBR)\n\n## Forward shading\n![](/assets/images/2021-12-21-10-34-51.png)\n\n## Deferred shading (G-Buffer):\n![](/assets/images/2021-12-21-10-35-02.png)\n\n## Phong lighting model\n> Reference: https://learnopengl.com/Lighting/Basic-Lighting\n\n`(ambient + diffuse + specular) * lightColor`\n\n- Ambient: constant\n```\n    float ambientStrength = 0.1;\n    vec3 ambient = ambientStrength * lightColor;\n```\n\n- Diffuse: simulate [[diffuse reflection|rendering.rendering-terms#diffuse-reflection]]\n\n```\n    vec3 norm = normalize(Normal);\n    vec3 lightDir = normalize(lightPos - FragPos);\n    float diff = max(dot(norm, lightDir), 0.0);\n    vec3 diffuse = diff * lightColor;\n```\n\n- Specular: simulate [[specular reflection|rendering.rendering-terms#specular-reflection]]\n\n```\n    float specularStrength = 0.5;\n    vec3 viewDir = normalize(viewPos - FragPos);\n    vec3 reflectDir = reflect(-lightDir, norm);\n    float spec = pow(max(dot(viewDir, reflectDir), 0.0), 32);\n    vec3 specular = specularStrength * spec * lightColor;\n```","n":0.07}}},{"i":44,"$":{"0":{"v":"Ray Isosurface Intersection","n":0.577},"1":{"v":"> Reference: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.118.8397&rep=rep1&type=pdf\n\n- Traverse cells in volume checking bounds for isovalue, can be solved in close form as root of cubic polynomial\n- Speed up traversal using spatial index (like R-tree)\n\n## Fast Voxel Traversal Algorithm\n> https://www.flipcode.com/archives/A%20faster%20voxel%20traversal%20algorithm%20for%20ray%20tracing.pdf\n\n### Intersection culling\n- Hierarchical bounding volumes\n- Space partitioning (octree/gird)\n\n### Algorithm\nBreak ray down into intervals of size $t$ spanning one voxel\n\n$$\n\\bm{u} + t \\bm{v}\n$$\n\n#### Initialization: \n- Find voxel the ray originated, or entry voxel if origin is outside voxel\n- Compute minimum of voxel boundary intersect, as maximum distance of traversal that still inside the grid\n- Compute delta $t$ along all axis that equals to unit voxel size\n\n#### Incremental traversal\n- Find axis with minimum boundary intersect\n- Increment by the delta along that axis\n- Move to next grid and repeat\n\n## TODO\n- [ ] DDA (Digital Differential Analyzer) algorithm","n":0.088}}},{"i":45,"$":{"0":{"v":"Panorama to Cubemap","n":0.577},"1":{"v":"\n> Reference: https://stackoverflow.com/questions/29678510/convert-21-equirectangular-panorama-to-cube-map\n\n## Core ideas:\n- View panorama as the equirectangular projection of sphere, such that spherical coordinate can be directly used to query its pixel value\n- Map from target to source (inverse transformation), by finding the spherical representation of cartesian coordinate in target image\n- Perform sampling and interpolation\n\n## Details \n### Spherical coordinates to Cartesian coordinate convertion\n$$\n{\\begin{cases} x= r\\sin\\theta\\cos\\phi \\\\ y = r\\sin\\theta\\sin\\phi \\\\ z=r\\cos\\theta \\end{cases}}\n$$\nWhere\n$$\n\\begin{cases} r=1 \\\\ 0<\\theta<\\pi \\\\ -\\frac{\\pi}{4} < \\phi < \\frac{7\\pi}{4} \\end{cases}\n$$\n### Project to sides\nLet $x = 1$ such that $a = \\frac{1}{\\sin\\theta\\cos\\phi}$\nSo\n$$\ny = \\tan\\phi, z = \\frac{1}{\\tan\\theta\\cos\\phi}\n$$\nThen\n$$\n\\phi = atan2(y), \\theta = atan2(\\frac{1}{z\\cos\\phi})\n$$\n\n### Project to top\nLet $z = 1$ such that $a = \\frac{1}{\\cos\\theta}$\nSo\n$$\nx = \\tan\\theta\\cos\\phi, y = \\tan\\theta\\sin\\phi\n$$\nThen\n$$\n\\phi = atan2(\\frac{y}{x}), \\theta = atan2(\\sqrt{x^{2} + y^{2}})\n$$\n\n## Python sample code\n```\n# left, front, right, back, top, bottom\noffset = [0, 0.25, 0.5, 0.75, 0, 0]\n\nface_id = 0\n\nfor w in range(edge):\n    for h in range(edge):\n        \n        # map to range (-1, 1)\n        u = (w/edge - 0.5)*2\n        v = (h/edge - 0.5)*2\n        \n        if face_id == 4:\n            phi = pi - atan2(v, u)\n            theta = atan2(sqrt(v*v+u*u), 1)\n        elif face_id == 5:\n            phi = -pi + atan2(v, u)\n            theta = -atan2(sqrt(v*v+u*u), 1)\n        else:\n            phi = atan2(u, 1)\n            theta = atan2(-1, v*cos(phi))\n        \n        x = phi/(2*pi)\n        y = theta/pi\n        \n        # TODO: use bilinear interpolation\n        x_idx = int(width*(x + offset[face_id]))\n        y_idx = int(height*y)\n        \n        face_pixel[w, h] = panorama_pixel[x_idx, y_idx]\n```\n","n":0.067}}},{"i":46,"$":{"0":{"v":"Frustum Culling","n":0.707},"1":{"v":"> Reference: https://gist.github.com/nothings/913056601b56e5719cc987684a16544e\n\nFilter out renderable outside viewing frustum, reduce vertex shading cost.","n":0.289}}},{"i":47,"$":{"0":{"v":"Camera MVP Matrix","n":0.577},"1":{"v":"\n## Model Matrix\nObject w.r.t. World (O -> W)\n\n$$\nT_W = T_{WO} T_{O}\n$$\n\n**IMPORTANT** ([[linear-algebra.matrix-matrix-dot-product]])\n\n- A w.r.t. B means A's basis in B's coordinate system\n- $T_O$ transforms coordinate in $O$ to standard basis $S$. Columns of $T_O$ are basis of $O$ seen in $S$. It can also be written as $T_{SO}$, $O$ w.r.t. $S$, or just a linear transformation in $S$\n\n## View Matrix\n> **IMPORTANT** Inverse Camera View Matrix is its Model Matrix\n \nWorld w.r.t. Camera (W -> C)\n\n$$\nT_C = T_{CW} T_{W}\n$$\n\n## Projection Matrix\n> **IMPORTANT** OpenGL(rendering) camera is different from epipolar geometry camera in computer vision, More see: https://amytabb.com/tips/tutorials/2019/06/28/OpenCV-to-OpenGL-tutorial-essentials/\n\nCamera w.r.t. Normalized Device coordinate (C -> NDC)\n\n$$\nT_{NDC} = T_{NDC,C} T_{C}\n$$\n","n":0.098}}},{"i":48,"$":{"0":{"v":"Anti Aliasing","n":0.707},"1":{"v":"> Reference: [Aliasing Wiki](https://en.wikipedia.org/wiki/Aliasing), [MSAA Wiki](https://en.wikipedia.org/wiki/Multisample_anti-aliasing)\n\n## Aliasing\nCaused by sampling at insufficient rate. In computer graphics, it happens when representing a high-resolution image at a lower resolution.\n\n## Anti-Aliasing\nMethods to smooth out jagged edges.\n\n## Full-Scene Anti-Aliasing (FSAA)\nSample multiple locations within pixel (supersampling), render and combine the results.\n\n## Multisample Anti-Aliasing (MSAA)\nMultisampling refers to a specific optimization of supersampling (a.k.a. not supersampling every pixel), usually only at edges of primitives.\n\n> More read: https://www.reddit.com/r/Games/comments/1rb964/antialiasing_modes_explained/\n","n":0.12}}},{"i":49,"$":{"0":{"v":"Ambient Occlusion","n":0.707},"1":{"v":"Exposure w.r.t. ambient light\n\n## Screen Space Ambient Occlusion (SSAO)\nApproximation of Ambient Occlusion\n\n> [From Wiki](https://en.wikipedia.org/wiki/Screen_space_ambient_occlusion):\nFor every pixel on the screen, the pixel shader samples the depth values around the current pixel and tries to compute the amount of occlusion from each of the sampled points. In its simplest implementation, the occlusion factor depends only on the depth difference between sampled point and current point.\n\n\n[SSAO artifacts](https://mtnphil.wordpress.com/2013/06/26/know-your-ssao-artifacts/)\n","n":0.125}}},{"i":50,"$":{"0":{"v":"Probability","n":1}}},{"i":51,"$":{"0":{"v":"Parameter Estimation","n":0.707},"1":{"v":"\nReference: https://mml-book.github.io\n\n## Problem formation\nLet random variable $\\pmb{x} \\in \\mathbb{R}^{D}$, parameter $\\pmb{\\theta} \\in \\mathbb{R}^{D}$, function value $f \\in \\mathbb{R}$ observed target value $y \\in \\mathbb{R}$\n\n$$\ny = f(\\pmb{x}) + \\epsilon = \\pmb{x}^{T} \\pmb{\\theta} + \\epsilon\n$$\nwhere $\\epsilon \\sim \\mathcal{N}(0,\\sigma^2)$ is independent identically distributed (i.i.d.) measurement noise.\n$$\ny \\sim \\mathcal{N}(y|\\pmb{x}^{T} \\pmb{\\theta},\\sigma^2)\n$$\n\n\n![[Bayes' theorem|probability.basis#bayes-theorem]]\n\n## Goal\nGiven training set of N $\\{\\pmb{x}_i, \\pmb{y} \\}$, find the optimal model parameter $\\pmb{\\theta}$ that predict $y$ from unseen $\\pmb{x}$. \n\nVector form: $\\pmb{X} = \\begin{bmatrix} \\pmb{x}_1 & \\pmb{X}_2 \\dots \\pmb{X}_N \\end{bmatrix}^T \\in \\mathbb{R}^{N \\times D}$, $\\pmb{y} = \\begin{bmatrix} \\pmb{y}_1 & \\pmb{y}_2 \\dots \\pmb{y}_N \\end{bmatrix} \\in \\mathbb{R}^{N}$, ","n":0.104}}},{"i":52,"$":{"0":{"v":"Maximum Likelihood Estimation (MLE)","n":0.5},"1":{"v":"> prone to overfitting\n\n## Basic\n\nFind the optimal parameter $\\pmb{\\theta}$ by maximizing the likelihood function $p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta})$.\n> Likelihood is a distribution of $\\pmb{y}$, a function of $\\pmb{\\theta}$\n\n$$\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\X}{\\pmb{X}}\n\\newcommand{\\x}{\\pmb{x}}\n\n\\begin{aligned}\n\\pmb{\\theta}^* &= \\underset{\\pmb{\\theta}}{\\argmax} \\ p(\\y|\\X, \\pmb{\\theta}) \\\\ &= \\underset{\\pmb{\\theta}}{\\argmin} (- \\log p(\\y|\\X, \\pmb{\\theta})) \\\\ &= \\underset{\\pmb{\\theta}}{\\argmin} - (\\log \\prod_{i=0}^{N} p(y_i|{\\x}_i, \\pmb{\\theta})) \\\\ &= \\underset{\\pmb{\\theta}}{\\argmin} \\sum_{i=1}^{N} - \\log p(y_i|{\\x}_i, \\pmb{\\theta})\n\\end{aligned}\n$$\n\nWe know $y \\sim \\mathcal{N}(y|\\pmb{x}^{T} \\pmb{\\theta},\\sigma^2)$ such that:\n\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\n- \\log p(y_i|{\\x}_i, \\pmb{\\theta}) = \\frac{1}{2\\sigma^2}(y_i - \\x^{T} \\pmb{\\theta})^2 + const\n$$\n\nThe loss function (negative log likelihood) can be defined in Vector form:\n\n$$\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\X}{\\pmb{X}}\n\nL(\\pmb{\\theta}) = \\frac{1}{2\\sigma^2} (\\y - \\X\\pmb{\\theta})^T(\\y - \\X\\pmb{\\theta})\n$$\n\nAs a quadratic term of $\\pmb{\\theta}$, the global minimum can be computed by setting gradient to 0:\n\n$$\n\\begin{aligned}\n\\frac{\\partial L}{ \\partial \\pmb{\\theta}} &= -\\frac{1}{\\sigma^2} (\\pmb{y} - \\pmb{X}\\pmb{\\theta})^T\\pmb{X} = \\pmb{0}\n\\\\\n\\pmb{\\theta}^* &= (\\pmb{X}^T\\pmb{X})^{-1}\\pmb{X}^T \\pmb{y}\n\\end{aligned}\n$$\n\n## With Features\nLet nonlinear transformation $\\phi: \\mathbb{R}^D \\rightarrow \\mathbb{R}^K$, $\\pmb{\\theta} \\in \\mathbb{R}^K$\n$$\ny = \\phi^T(\\pmb{x})\\pmb{\\theta} + \\epsilon\n$$\n\nIn vector form:\n$$\n\\pmb{\\Phi} = \\pmb{\\phi}(\\pmb{x}) = \\begin{bmatrix} \\phi_1(\\pmb{x})& \\phi_2(\\pmb{x}) \\dots \\phi_k(\\pmb{x}) \\end{bmatrix}^T \\in \\mathbb{R}^K\n$$\n\nWith close form solution:\n\n$$\n\\pmb{\\theta}^* = (\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T \\pmb{y}\n$$\n\n### Polynomial basis\nFor $x \\in \\mathbb{R}$\n\n$$\n\\phi(\\bm{x}) = \\begin{bmatrix} 1 \\\\ x \\\\ x^2 \\\\ \\vdots \\\\ x^k \\end{bmatrix} \\in \\mathbb{R}^{k+1}\n$$\n\nFor $\\bm{x} \\in \\mathbb{R}^d$, multivariate polynomial\n\n> Reference: https://www.dbs.ifi.lmu.de/Lehre/MaschLernen/SS2017/Skript/03-BasisFunctions2017.pdf\n\n$$\n\\bm{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix} \\in \\mathbb{R}^3\n$$\nIts multivariate polynomial basis of degree 2 (with all permutation of elements in $\\bm{x}$)\n\n$$\n\\phi(\\bm{x}) = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ x_1 x_2 \\\\ x_2 x_3 \\\\ x_1 x_3 \\\\ x_1^2 \\\\ x_2^2 \\\\ x_3^2 \\end{bmatrix} \\in \\mathbb{R}^{10}\n$$\n\n## Regularization\n\n$$\nL(\\pmb{\\theta}) =  -\\log p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta}) + \\lambda \\|\\pmb{\\theta}\\|_2^2\n$$","n":0.065}}},{"i":53,"$":{"0":{"v":"Maximum a Posterior Estimate (MAP)","n":0.447},"1":{"v":"> Regularize with prior $p(\\pmb{\\theta})$\n\nFind the optimal parameter $\\pmb{\\theta}$ by maximizing the posterior function $p(\\pmb{\\theta}|\\pmb{X}, \\pmb{y})$\n\n$$\np(\\pmb{\\theta}|\\pmb{X}, \\pmb{y}) = \\frac{p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta})p(\\pmb{\\theta})}{p(\\pmb{y}|\\pmb{X})}\n$$\n\nApply negative log transform:\n> Marginal likelihood is not a function of $\\pmb{\\theta}$\n\n$$\n-\\log p(\\pmb{\\theta}|\\pmb{X}, \\pmb{y}) = - \\log p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta}) - \\log p(\\pmb{\\theta}) + const\n$$\n\nAssume gaussian prior $p(\\pmb{\\theta}) = \\mathcal{N}(\\pmb{0}, b^2 \\pmb{I})$, $b^2 = \\frac{1}{2 \\lambda}$\n\n$$\n- \\log p(\\pmb{\\theta}) = \\frac{1}{2b^2}\\pmb{\\theta}^T \\pmb{\\theta} = \\lambda \\|\\pmb{\\theta}\\|_2^2\n$$\n\nIt is equivalent to [[MLE with Regularization|probability.parameter-estimation.maximum-likelihood-estimation-mle#regularization]].\n\nThe negative log posterior loss function:\n\n$$\nL(\\pmb{\\theta}) = \\frac{1}{2\\sigma^2} (\\pmb{y} - \\pmb{X}\\pmb{\\theta})^T(\\pmb{y} - \\pmb{X}\\pmb{\\theta}) + \\frac{1}{2b^2}\\pmb{\\theta}^T \\pmb{\\theta} + const\n$$\n\nSet gradient to 0:\n\n$$\n\\frac{\\partial L}{\\partial \\pmb{\\theta}} = - \\frac{1}{\\sigma^2}(\\pmb{y} - \\pmb{X}\\pmb{\\theta})^T\\pmb{X} - \\frac{1}{b^2}\\pmb{\\theta}^T = 0\n\n\\\\\n\\pmb{\\theta} = (\\pmb{X}^T \\pmb{X} - \\frac{\\sigma^2}{b^2} \\pmb{I})\\pmb{X}^T \\pmb{y}\n\n$$","n":0.097}}},{"i":54,"$":{"0":{"v":"Bayesian Linear Regression","n":0.577},"1":{"v":"Compute full posterior, make predictions by taking account of all possible model parameters.\n\n## Train\n\nWith bayes' theorem\n$$\np(\\pmb{\\theta}|\\pmb{X}, \\pmb{y}) = \\frac{p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta})p(\\pmb{\\theta})}{p(\\pmb{y}|\\pmb{X})}\n$$\n\nWe have\n\n$$\np(\\pmb{\\theta}|\\pmb{X}, \\pmb{y}) \\propto p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta})p(\\pmb{\\theta})\n$$\n\nGiven likelihood $p(\\pmb{y}|\\pmb{X}, \\pmb{\\theta}) = \\mathcal{N}(\\pmb{y}|\\pmb{\\Phi}\\pmb{\\theta}, \\sigma^2 \\pmb{I})$, assuming Gaussian prior $p(\\pmb{\\theta}) = \\mathcal{N}(\\pmb{\\theta}|\\pmb{m}_0, \\pmb{S}_0)$, $p(\\pmb{\\theta}|\\pmb{X}, \\pmb{y})$ is also Gaussian distributed. \n\nAssume $p(\\pmb{\\theta}|\\pmb{X}, \\pmb{y}) = \\mathcal{N}(\\pmb{\\theta}|\\pmb{m}_N, \\pmb{S}_N)$, we have\n\n$$\n\\mathcal{N}(\\pmb{\\theta}|\\pmb{m}_N, \\pmb{S}_N) \\propto \\mathcal{N}(\\pmb{y}|\\pmb{\\Phi \\theta}, \\sigma^2 \\pmb{I}) \\mathcal{N}(\\pmb{\\theta}|\\pmb{m}_0, \\pmb{S}_0)\n$$\n\nApply linear transform $\\pmb{\\theta} = (\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T\\pmb{y}$ to transform Gaussian variable $\\pmb{y}$ to $\\pmb{\\theta}$\n$$\n\\begin{aligned}\n\\mathcal{N}(\\pmb{y}|\\pmb{\\Phi \\theta}, \\sigma^2 \\pmb{I}) &=\n\\mathcal{N}((\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T\\pmb{y}|(\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T\\pmb{\\Phi}(\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T\\pmb{y}, \\sigma^2 (\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T((\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T)^T) \\\\ &=\n\\mathcal{N}(\\pmb{\\theta}|(\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1}\\pmb{\\Phi}^T\\pmb{y}, \\sigma^2 (\\pmb{\\Phi}^T\\pmb{\\Phi})^{-1})\n\\end{aligned}\n$$\n\nUse [[Gaussian Product rule|probability.gaussian-distribution#product]]\n\n$$\n\\pmb{S}_N = (\\pmb{m}_0^{-1} + \\sigma^{-2} (\\pmb{\\Phi}^T\\pmb{\\Phi}))^{-1}\n\\\\\n\\pmb{m}_N = \\pmb{S}_N(\\pmb{S}_0^{-1} \\pmb{m}_0 + \\sigma^{-2}\\pmb{\\Phi}^T\\pmb{y})\n$$\n\n## Inference\nFor new data, the predictive distribution\n$$\ny_* = \\phi(\\pmb{x_*})^T \\pmb{\\theta} + \\epsilon\n\\\\\np(y_*|\\pmb{X}, \\pmb{y}, \\phi(\\pmb{x_*})) = \\int p(y_*|\\phi(\\pmb{x_*}), \\pmb{\\theta}) p(\\pmb{\\theta} | \\pmb{X}, \\pmb{y}) d \\pmb{\\theta} = \\mathcal{N}(\\phi(\\pmb{x_*})^T\\pmb{m}_N, \\phi(\\pmb{x_*})^T \\pmb{S}_N\\phi(\\pmb{x_*}) + \\sigma^2)\n$$\n\n> Predictive mean coincides with MAP\n\nIntegrate out $\\pmb{\\theta}$ induces the distribution of function\n\n$$\nf(.) \\sim \\mathcal{N}(\\phi(.)^T\\pmb{m}_N, \\phi(.)^T \\pmb{S}_N\\phi(.))\n$$\n> $\\phi(.)^T\\pmb{m}_N$ is the mean function\n\nBy sampling $\\pmb{\\theta}_i \\sim p(\\pmb{\\theta} | \\pmb{X}, \\pmb{y})$, we obtain a function realization $f_i(.) = \\phi(.)^T\\pmb{\\theta}_i$","n":0.079}}},{"i":55,"$":{"0":{"v":"Gaussian Distribution","n":0.707},"1":{"v":"\n>FIXME: 1. Global Marco 2. Nested Alignment\n\nFor $x \\in \\mathbb{R} \\sim \\mathcal{N}(\\mu, \\sigma)$\n\n$$\np(x|\\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp{(-\\frac{(x - \\mu)^2}{2\\sigma^2})}\n$$\n\nFor $\\pmb{x} \\in \\mathbb{R}^{D} \\sim \\mathcal{N}(\\pmb{\\mu}, \\pmb{\\Sigma})$\n\n$$\np(\\pmb{x} | \\pmb{\\mu}, \\pmb{\\Sigma}) = (2\\pi)^{-\\frac{D}{2}} (|\\pmb{\\Sigma}|)^{-\\frac{1}{2}}\\exp{(-\\frac{1}{2}(\\pmb{x} - \\pmb{\\mu})^{T}\\pmb{\\Sigma}^{-1}(\\pmb{x} - \\pmb{\\mu}))} = \\frac{1}{\\sqrt{(2\\pi)^D \\pmb{\\Sigma}}}\\exp{(-\\frac{1}{2}(\\pmb{x} - \\pmb{\\mu})^{T}\\pmb{\\Sigma}^{-1}(\\pmb{x} - \\pmb{\\mu}))}\n$$\n\nConsider joint distribution\n$$\np(\\pmb{x}, \\pmb{y}) = \\mathcal{N}(\\begin{bmatrix} \\pmb{\\mu}_{\\pmb{x}} \\\\ \\pmb{\\mu}_{\\pmb{y}} \\\\ \\end{bmatrix}, \\begin{bmatrix} \\pmb{\\Sigma}_{\\pmb{xx}} &\\pmb{\\Sigma}_{\\pmb{xy}} \\\\ \\pmb{\\Sigma}_{\\pmb{yx}} &\\pmb{\\Sigma}_{\\pmb{yy}} \\\\ \\end{bmatrix})\n$$\n\n## Conditional\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\np(\\x|\\y) = \\mathcal{N}(\\pmb{\\mu}_{\\x|\\y}, \\pmb{\\Sigma}_{\\x|\\y}) \\\\\n\\pmb{\\mu}_{\\x|\\y} = \\pmb{\\mu}_{\\x} + \\pmb{\\Sigma}_{\\x\\y}\\pmb{\\Sigma}_{\\y\\y}^{-1}(\\y - \\pmb{\\mu}_{\\y}) \\\\\n \\pmb{\\Sigma}_{\\x|\\y} = \\pmb{\\Sigma}_{\\x\\x} - \\pmb{\\Sigma}_{\\x\\y}\\pmb{\\Sigma}_{\\y\\y}^{-1}\\pmb{\\Sigma}_{\\y\\x}\n$$\n\nProof:\n> Theorem: All conditional distributions of a multivariate normal distribution are normal\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\z}{\\pmb{z}}\n\\newcommand{\\A}{\\pmb{A}}\n\\newcommand{\\mux}{\\pmb{\\mu}_{\\x}}\n\\newcommand{\\muy}{\\pmb{\\mu}_{\\y}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Sigmaxx}{\\pmb{\\Sigma}_{\\x\\x}}\n\\newcommand{\\Sigmaxy}{\\pmb{\\Sigma}_{\\x\\y}}\n\\newcommand{\\Sigmayx}{\\pmb{\\Sigma}_{\\y\\x}}\n\\newcommand{\\Sigmayy}{\\pmb{\\Sigma}_{\\y\\y}}\n\n\\text{define} \\ \\z = \\x + \\A\\y, \\text{where} \\ \\A = - \\Sigmaxy \\Sigmayy^{-1}\n\n\\\\\n\n\\begin{aligned}\n\\Cov[\\z, \\y] &= \\Cov[\\x + \\A\\y, \\y] \\\\ &= \\Cov[\\x, \\y] + \\Cov[\\A\\y, \\y] \\\\ &= \\Sigmaxy + \\A \\Sigmayy \\\\ &= \\Sigmaxy - \\Sigmaxy \\Sigmayy^{-1} \\Sigmayy \\\\ &= \\pmb{0}\n\\end{aligned}\n\n\\\\\n\\begin{aligned}\n\\E[\\x|\\y] &= \\E[\\z - \\A\\y | \\y] \\\\ &= \\E[\\z|\\y] - \\E[\\A\\y | \\y] \\\\ &= \\E[\\z] - \\A \\y \\\\ &= \\mux - \\A(\\y - \\muy) \\\\ &= \\mux + \\Sigmaxy \\Sigmayy^{-1}(\\y - \\muy)\n\\end{aligned}\n\n\\\\\n\\begin{aligned}\n\\Var[\\x|\\y] &= \\Var[\\z - \\A\\y | \\y] \\\\ &= \\Var[\\z | \\y] + \\Var[-\\A\\y | \\y] + \\Cov[\\z|\\y, - \\A\\y | \\y] + \\Cov[- \\A\\y | \\y, \\z|\\y] \\\\ &= \\Var[\\z] \\\\ &= \\Var[\\x + \\A\\y] \\\\ &= \\Var[\\x] + \\Var[\\A\\y] + \\Cov[\\x, \\A\\y] + \\Cov[\\A\\y, \\x] \\\\ &= \\Sigmaxx + \\A \\Sigmayy \\A^T + \\A \\Sigmaxy + \\Sigmayx \\A^T \\\\ &= \\Sigmaxx + (- \\Sigmaxy \\Sigmayy^{-1}) \\Sigmayy (- \\Sigmaxy \\Sigmayy^{-1})^T + (- \\Sigmaxy \\Sigmayy^{-1}) \\Sigmaxy + \\Sigmayx (- \\Sigmaxy \\Sigmayy^{-1})^T \\\\ &= \\Sigmaxx + \\Sigmaxy \\Sigmayy^{-1} \\Sigmayx - 2\\Sigmaxy \\Sigmayy^{-1}\\Sigmayx \\\\ &= \\Sigmaxx - \\Sigmaxy \\Sigmayy^{-1}\\Sigmayx\n\\end{aligned}\n$$\n\n## Marginal\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\mux}{\\pmb{\\mu}_{\\x}}\n\\newcommand{\\Sigmaxx}{\\pmb{\\Sigma}_{\\x\\x}}\n\np(\\x) = \\int p(\\x, \\y) d\\y = \\mathcal{N}(\\x | \\mux, \\Sigmaxx)\n$$\n\n## Product\nThe product of two Gaussians $\\mathcal{N}(\\pmb{x} | \\pmb{a}, \\pmb{A})$$\\mathcal{N}(\\pmb{x} | \\pmb{b}, \\pmb{B})$ is an unnormalized Gaussian $\\mathcal{N}(\\pmb{x} | \\pmb{c}, \\pmb{C})$ with\n$$\n\\pmb{C} = (\\pmb{A}^{-1} + \\pmb{B}^{-1})^{-1}\n\\\\\n\\pmb{c} = \\pmb{C}(\\pmb{A}^{-1} \\pmb{a} + \\pmb{B}^{-1} \\pmb{b})\n$$\n\nProof:\n$$\n\\newcommand{\\a}{\\pmb{a}}\n\\newcommand{\\b}{\\pmb{b}}\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\A}{\\pmb{A}}\n\\newcommand{\\B}{\\pmb{B}}\n\\newcommand{\\C}{\\pmb{C}}\n\n\\begin{aligned}\n\\text{Apply log transformation} \\ &\\log \\mathcal{N}(\\x|\\a, \\A) \\mathcal{N}(\\x|\\b, \\B) \\\\ = & -\\frac{1}{2}((\\x - \\a)^T \\A^{-1}(\\x - \\a) + (\\x - \\b)^T \\B^{-1}(\\x - \\b)) + const \\\\\n= & -\\frac{1}{2}(\\x^T \\A^{-1} \\x - 2 \\a^T \\A^{-1} \\x + \\a^T \\A^{-1} \\a + \\x^T \\B^{-1} \\x - 2 \\b^T \\B^{-1} \\x + \\b^T \\B^{-1} \\b) + const \\\\ = & -\\frac{1}{2}(\\x^T (\\A^{-1} + \\B^{-1}) \\x - 2(\\A^{-1} \\a + \\B^{-1} \\b)^T\\x) + const\n\\end{aligned}\n\n\\\\\n\n\\begin{aligned}\n\\text{We know that} \\ & \\exp(\\log \\mathcal{N}(\\x | \\pmb{c}, \\C)) = \\exp(-\\frac{1}{2}((\\x - \\pmb{c})^T \\C^{-1}(\\x - \\pmb{c}) + const) \\\\ \\propto & \\exp(-\\frac{1}{2}(\\x^T \\C^{-1} \\x - 2 (\\C^{-1} \\pmb{c})^T \\x)) \\\\ \\propto & \\exp(\\log \\mathcal{N}(\\x|\\a, \\A) \\mathcal{N}(\\x|\\b, \\B)) \\\\ \\propto & -\\frac{1}{2}(\\x^T (\\A^{-1} + \\B^{-1}) \\x - 2(\\A^{-1} \\a + \\B^{-1} \\b)^T\\x)\n\\end{aligned}\n\n\\\\\n\\begin{aligned}\n\\text{By completing the square, we have} \\ & \\C = (\\A^{-1} + \\B^{-1})^{-1}\n\\\\& \\pmb{c} = \\C(\\A^{-1} \\a + \\B^{-1} \\b)\n\\end{aligned}\n$$\n\nThe normalizing constant is\n$$\nc = (2\\pi)^{-\\frac{D}{2}}|\\pmb{A} + \\pmb{B}|^{\\frac{1}{2}}\\exp{(-\\frac{1}{2}(\\pmb{a}-\\pmb{b})^T(\\pmb{A} + \\pmb{B})^{-1}(\\pmb{a}-\\pmb{b}))}\n$$\n\n## Linear transformation\n> Linear transformation of Gaussian variable is Gaussian distributed\n\nLet $p(\\pmb{x}) = \\mathcal{N}(\\pmb{x} | \\pmb{\\mu}, \\pmb{\\Sigma})$, $\\pmb{y} = \\pmb{A} \\pmb{x}$, then\n$$\n\\pmb{y} = \\mathcal{N}(\\pmb{y} | \\pmb{A} \\pmb{\\mu}, \\pmb{A} \\pmb{\\Sigma} \\pmb{A}^T)\n$$\n\nLet $p(\\pmb{y}|\\pmb{x}) = \\mathcal{N}(\\pmb{y}|\\pmb{A}\\pmb{x}, \\pmb{\\Sigma})$, then\n$$\n\\pmb{x} = (\\pmb{A}^T \\pmb{A})^{-1}\\pmb{A}^T\\pmb{y}\n\n\\\\\n\np(\\pmb{x}|\\pmb{y}) = \\mathcal{N}((\\pmb{A}^T \\pmb{A})^{-1}\\pmb{A}^T\\pmb{y}, (\\pmb{A}^T \\pmb{A})^{-1}\\pmb{A}^T \\pmb{\\Sigma} \\pmb{A}(\\pmb{A}^T \\pmb{A})^{-1})\n$$\n\n## Sampling with reparameterization trick\nInstead of sampling from $\\pmb{y} \\sim \\mathcal{N}(\\pmb{\\mu}, \\pmb{\\Sigma})$, sampling from $\\pmb{x} \\sim \\mathcal{N}(\\pmb{0}, \\pmb{I})$, find $\\pmb{A}\\pmb{A}^T = \\pmb{\\Sigma}$, then $\\pmb{y} = \\pmb{A}\\pmb{x} + \\pmb{\\mu}$\n\n## Sum of independent\n> Sum of independent Gaussian variables is also Gaussian distributed\n\n$$\np(\\pmb{x} + \\pmb{y}) = \\mathcal{N}(\\pmb{\\mu}_{\\pmb{x}} + \\pmb{\\mu}_{\\pmb{y}}, \\pmb{\\Sigma}_{\\pmb{x}} + \\pmb{\\Sigma}_{\\pmb{y}})\n$$","n":0.043}}},{"i":56,"$":{"0":{"v":"Expected Value and Covariance","n":0.5},"1":{"v":">FIXME: 1. Global Marco 2. Nested Alignment\n\n## Expected value\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\E}{\\mathrm{E}}\n\n\\E_{\\x}[\\x] = \\int\\x p(\\x) d\\x \\in \\mathbb{R}^D \\\\\n\n\\E_{\\x}[g(\\x)] = \\int g(\\x) p(\\x) d\\x\n$$\n\n## Covariance\nLet $\\pmb{x} \\in \\mathbb{R}^D$, $\\pmb{y} \\in \\mathbb{R}^E$\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\n\\Cov[\\x, \\y] = \\pmb{\\Sigma}_{\\x \\y} = \\E_{\\x, \\y}[\\x \\y^T] - \\E_{\\x}[\\x]\\E_{\\y}[\\y]^T = {\\Cov[\\y, \\x]}^T \\in \\mathbb{R}^{D \\times E} \\\\\n\n\\Var[\\x] = \\pmb{\\Sigma}_{\\x \\x} = \\E_{\\x}[(\\x - \\pmb{\\mu})(\\x - \\pmb{\\mu})^{T}] = \\E_{\\x}[\\x\\x^T] - \\E_{\\x}[\\x]\\E_{\\x}[\\x]^T \\in \\mathbb{R}^{D \\times D}\n$$\n\n## For dataset\n> Monte-Carlo: random repeated sampling\n\nMean\n$$\n\\pmb{\\mu} = \\frac{1}{N}\\sum_{n=1}^{N}\\pmb{x}_n \\in \\mathbb{R}^{D}\n$$\nVariance\n$$\n\\pmb{\\Sigma} = \\frac{1}{N}\\sum_{n=1}^{N}(\\pmb{x}_n - \\pmb{\\mu})(\\pmb{x}_n - \\pmb{\\mu})^T \\in \\mathbb{R}^{D \\times D}\n$$\n\n## Arithmetics\nLet $\\pmb{x} \\in \\mathbb{R}^D$, $\\pmb{y} \\in \\mathbb{R}^D$\n\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\n\\begin{aligned}\n\\E[\\x+\\y] &= \\E[\\x] + \\E[\\y] \\\\\n\\E[\\x-\\y] &= \\E[\\x] - \\E[\\y] \\\\\n\\Var[\\x+\\y] &= \\Var[\\x] + \\Var[\\y] - \\Cov[\\x, \\y] + \\Cov[\\y, \\x] \\\\\n\\Var[\\x-\\y] &= \\Var[\\x] + \\Var[\\y] - \\Cov[\\x, \\y] - \\Cov[\\y, \\x]\n\\end{aligned}\n$$\n\nProof:\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\\begin{aligned}\n\\Var[\\x+\\y] &= \\E[(\\x + \\y)(\\x + \\y)^T] - \\E[\\x + \\y]\\E[\\x + \\y]^T \\\\\n&= \\E[\\x\\x^T] + \\E[\\x\\y^T] + E[\\y\\x^T] + \\E[\\y\\y^T] - (\\E[\\x]\\E[\\x]^T + \\E[\\x]\\E[\\y]^T + \\E[\\y]\\E[\\x]^T + \\E[\\y]\\E[\\y]^T) \\\\\n&= (\\E[\\x\\x^T] - \\E[\\x]\\E[\\x]^T) + (\\E[\\y\\y^T] - \\E[\\y]\\E[\\y]^T) + (\\E[\\x\\y^T] - \\E[\\x]\\E[\\y]^T) + (E[\\y\\x^T] - \\E[\\y]\\E[\\x]^T) \\\\\n&= \\Var[\\x] + \\Var[\\y] + \\Cov[\\x, \\y] + \\Cov[\\y, \\x]\n\\end{aligned}\n$$\n\n## Conditional\n> Expect the value of a random variable, given its known value, to be that value\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\n\\E[\\x|\\x] = \\x\n\\\\\n\\Var[\\x|\\x] = \\pmb{0}\n$$\n\n## Affine transformation\nLet $\\pmb{x}$ be a random variable with mean $\\pmb{\\mu}$ and variance $\\pmb{\\Sigma}$, random variable $\\pmb{y}$ as an affine transform of $\\pmb{x}$ such that $\\pmb{y} = \\pmb{A}\\pmb{x} + \\pmb{b}$\n\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\A}{\\pmb{A}}\n\\newcommand{\\b}{\\pmb{b}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\n\\begin{aligned}\n\\E_{\\x}[\\y] &= \\E_{\\x}[\\A\\x + \\b] = \\A \\E_{\\x}[\\x] + \\b = \\A\\pmb{\\mu} = \\b \\\\\n\\Var_{\\y}[\\y] &= \\Var_{\\x}[\\A\\x + \\b] = \\Var_{\\x}[\\A\\x] = \\A \\Var_{\\x} \\A^T = \\A \\pmb{\\Sigma} \\A^T \\\\\n\\Cov[\\x, \\y] &= \\pmb{\\Sigma} \\A^T\n\\end{aligned}\n$$\n\nProof:\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\y}{\\pmb{y}}\n\\newcommand{\\z}{\\pmb{z}}\n\\newcommand{\\A}{\\pmb{A}}\n\\newcommand{\\b}{\\pmb{b}}\n\\newcommand{\\E}{\\mathrm{E}}\n\\newcommand{\\Var}{\\mathrm{Var}}\n\\newcommand{\\Cov}{\\mathrm{Cov}}\n\n\\begin{aligned}\n\\Var_{\\x}[\\A\\x] &= \\E_{\\x}[\\A\\x(\\A\\x)^T] - \\E[\\A\\x]\\E[\\A\\x]^T \\\\\n&= \\E_{\\x}[\\A\\x\\x^T\\A] - \\A\\E[\\x](\\A\\E[\\x])^T \\\\\n&= \\A \\E[\\x\\x^T]\\A^T - \\A\\E_{\\x}[\\x]\\E_{\\x}[\\x]^T\\A^T \\\\\n&= \\A (\\E_{\\x}[\\x\\x^T] - \\E_{\\x}[\\x]\\E_{\\x}[\\x]^T) \\A^T \\\\\n&= \\A \\Var_{\\x} \\A^T\n\\end{aligned} \n\n\\\\\n\n\\begin{aligned}\n\\Cov[\\x + \\y, \\z] &= \\E[(\\x+\\y)\\z^T] + \\E[\\x + \\y]\\E[\\z]^T \\\\ \n&= \\E[\\x\\z^T] + \\E[\\y\\z^T] + E[\\x]\\E[\\z]^T + \\E[\\y]\\E[\\z]^T \\\\\n&= (\\E[\\x\\z^T] + E[\\x]\\E[\\z]^T) + (\\E[\\y\\z^T] + \\E[\\y]\\E[\\z]^T) \\\\\n&= \\Cov[\\x, \\z] + \\Cov[\\y, \\z]\n\\end{aligned}\n\n\\\\\n\n\\begin{aligned}\n\\Cov[\\x, \\y] &= \\Cov[\\x, \\A\\x + \\b] \\\\ \n&= \\Cov[\\x, \\A\\x] + \\Cov[\\x, \\b] \\\\ \n&= \\Cov[\\x, \\A\\x] \\\\\n&= \\Cov[\\x, \\x] \\A^T \\\\\n&= \\pmb{\\Sigma} \\A^T\n\\end{aligned}\n$$\n","n":0.054}}},{"i":57,"$":{"0":{"v":"Basis","n":1},"1":{"v":"## Probability density function (pdf)\n$$\n\\int_{\\mathbb{R}^{D}}p(\\pmb{x})d\\pmb{x}=1\n$$\n\n## Sum rule / Marginalization \n$$\np(\\pmb{x}) = \\int p(\\pmb{x}, \\pmb{y})d\\pmb{y}\n$$\n\n## Product rule\n$$\np(\\pmb{x}, \\pmb{y}) = p(\\pmb{y}|\\pmb{x})p(\\pmb{x})\n$$\n\nCombine sum rule and product rule\n$$\np(\\pmb{x}) = \\int p(\\pmb{x}|\\pmb{y})p(\\pmb{y})d\\pmb{y}\n$$\n\n## Bayes' theorem\n$$\np(\\pmb{\\theta}|\\pmb{x}, y) = \\frac{p(y|\\pmb{x}, \\pmb{\\theta})p(\\pmb{\\theta})}{p(y|\\pmb{x})}\n$$\n\n$$\nposterior = \\frac{likelihood \\times prior}{marginal\\ likelihood}\n$$\n\nProof with product rule\n$$\np(y|\\pmb{x}, \\pmb{\\theta})p(\\pmb{\\theta}) = p(\\pmb{\\theta}, y|\\pmb{x})\n$$\n\n$$\np(\\pmb{\\theta}|\\pmb{x}, y)p(y|\\pmb{x}) = p(\\pmb{\\theta}, y|\\pmb{x})\n$$\n\nAlso with sum rule\n> Marginal likelihood is the likelihood averaged over all possible $\\pmb{\\theta}$\n\n$$\np(y|\\pmb{x}) = \\int p(y|\\pmb{x}, \\pmb{\\theta})p(\\pmb{\\theta})d\\theta\n$$","n":0.123}}},{"i":58,"$":{"0":{"v":"Polygon","n":1}}},{"i":59,"$":{"0":{"v":"Smooth","n":1},"1":{"v":"\n[Bezier Curves Interpolation](https://web.archive.org/web/20131027060328/http://www.antigrain.com/research/bezier_interpolation/index.html#PAGE_BEZIER_INTERPOLATION)\n\nMy numpy implementation:\n## Bezier fit\n```\ndef bezier_fit(polygon, smooth_factor=1.0):\n    \"\"\"\n    Bezier_curve_interpolation.\n    \n    Parameters\n    ----------\n    polygon : n x 2 numpy array\n    smooth_factor : the larger the value, more close the curve is to the edge\n    resample_count : number of samples for each bezier segment\n\n    Returns\n    -------\n    control_points : bezier control points in the form of n x 3 x 2 numpy array (center, left_handle, right_handle)\n    \"\"\"\n\n    polygon_next = np.roll(polygon, -1, axis=0)\n    polygon_last = np.roll(polygon, 1, axis=0)\n    mid_point = (polygon + polygon_next) / 2\n\n    dist_to_last = np.linalg.norm(polygon - polygon_last, axis=1)\n    dist_to_next = np.linalg.norm(polygon - polygon_next, axis=1)\n    dist_sum = smooth_factor * (dist_to_next + dist_to_last)\n    dir = np.roll(mid_point, 1, axis=0) - mid_point\n\n    cpt_left = polygon + dir * (dist_to_last / dist_sum).reshape(-1, 1)\n    cpt_right = polygon - dir * (dist_to_next / dist_sum).reshape(-1, 1)\n\n    control_points = np.array(\n        [polygon, cpt_left, cpt_right]).transpose([1, 0, 2])\n    \n    return control_points\n```\n\n## Bezier fit and resample\n```\ndef bezier_curve_interpolation(polygon, smooth_factor = 1.0, resample_count = 8):\n    \"\"\"\n    Bezier_curve_interpolation.\n    \n    Parameters\n    ----------\n    polygon : n x 2 numpy array\n    smooth_factor : the larger the value, more close the curve is to the edge\n    resample_count : number of samples for each bezier segment\n    \"\"\"\n\n    def bezier_curve(cpts, t):\n        tile = lambda x: np.tile(np.expand_dims(x, -1), len(t))\n        pt0 = tile(cpts[0])\n        pt1 = tile(cpts[1])\n        pt2 = tile(cpts[2])\n        pt3 = tile(cpts[3])\n        return np.power(1 - t, 3) * pt0 + 3 * np.power(1 - t, 2) * t * pt1 + 3 * (1 - t) * np.power(t, 2) * pt2 + np.power(t, 3) * pt3\n    \n    polygon_next = np.roll(polygon, -1, axis=0)\n    polygon_last = np.roll(polygon, 1, axis=0)\n    mid_point = (polygon + polygon_next) / 2\n\n    dist_to_last = np.linalg.norm(polygon - polygon_last, axis=1)\n    dist_to_next = np.linalg.norm(polygon - polygon_next, axis=1)\n    dist_sum = smooth_factor * (dist_to_next + dist_to_last)\n    dir = np.roll(mid_point, 1, axis=0) - mid_point\n\n    cpt_left = polygon + dir * (dist_to_last / dist_sum).reshape(-1, 1)\n    cpt_right = polygon - dir * (dist_to_next / dist_sum).reshape(-1, 1)\n\n    control_points = np.array([polygon, cpt_right, np.roll(cpt_left, -1, axis=0), polygon_next])\n    return bezier_curve(control_points, np.linspace(0, 1, resample_count + 1)[:-1]).transpose((0, 2, 1)).reshape(-1, 2)\n```\n","n":0.055}}},{"i":60,"$":{"0":{"v":"Simplification","n":1},"1":{"v":"\n[Ramer–Douglas–Peucker & Visvalingam-Whyatt](https://github.com/urschrei/simplification)\n","n":0.577}}},{"i":61,"$":{"0":{"v":"Orientation","n":1},"1":{"v":"\n## Use pyclipper\n\n```\nfrom pyclipper import Orientation, scale_to_clipper\n\ndef is_CCW(polygon):\n    return Orientation(scale_to_clipper(polygon))\n\n```\n\n## Numpy\n\n> Reference: https://stackoverflow.com/questions/1165647/how-to-determine-if-a-list-of-polygon-points-are-in-clockwise-order\n\n> FIXME: it keeps giving wrong results, figure out why?\n\n\n```\nimport numpy as np\n\ndef is_CCW(coutour):\n    start = coutour[1:]\n    end = coutour[:-1]\n    return np.sum((start[:, 0] - end[:, 0]) / (start[:, 1] + end[:, 1])) < 0\n```","n":0.147}}},{"i":62,"$":{"0":{"v":"Offset","n":1},"1":{"v":"\n## Use library\n[Clipper library](http://www.angusj.com/delphi/clipper.php)\n([python binding](https://github.com/fonttools/pyclipper))\n\n> Clipper seems to return a densely sampled polygon\n\n## Using GLU\n> Algorithm reference: https://mcmains.me.berkeley.edu/pubs/DAC05OffsetPolygon.pdf\n\n> GLU reference chapter: https://people.eecs.ku.edu/~jrmiller/Courses/672/InClass/PolygonTessellation/PolygonTessellation.html\n\nMy implementation\n```\n# Only handles single polygon offset\n# Algorithm: https://mcmains.me.berkeley.edu/pubs/DAC05OffsetPolygon.pdf\ndef polygon_edge_segments_offset(polygon, offset):\n    vertex_start = polygon\n\n    normalized = lambda x: x / (np.linalg.norm(x, axis=1) + 1e-10).reshape(\n        -1, 1)\n\n    def compute_edge_specs(edge_vector):\n        edge_vector = np.insert(edge_vector, obj=2, values=0, axis=1)\n        edge_vector_next = np.roll(edge_vector, -1, axis=0)\n        # angle between two adjacent edges\n        angles = np.arccos(-np.sum(edge_vector * edge_vector_next, axis=1) /\n                           (np.linalg.norm(edge_vector, axis=1) *\n                            np.linalg.norm(edge_vector_next, axis=1)) + 1e-10)\n        # angle for arcs centered at shared vertices\n        arc_angle = np.pi - angles\n        # CCW, right concave, left convex\n        is_convex = np.sign(np.cross(edge_vector, edge_vector_next)[:, -1]) > 0\n        # CCW, toward right\n        exterior_normal = np.cross(\n            edge_vector,\n            np.tile(np.array([0, 0, 1]), reps=(len(edge_vector), 1)))[:, :2]\n        return arc_angle, is_convex, normalized(exterior_normal)\n\n    vertex_end = np.roll(vertex_start, -1, axis=0)\n    edge_segments = np.hstack((vertex_start, vertex_end)).reshape(-1, 2, 2)\n    edge_vector = vertex_end - vertex_start\n    arc_angle, is_convex, exterior_normal = compute_edge_specs(edge_vector)\n\n    # Subdivide with a simple policy\n    # Interval:    0   pi/8   3*pi/8\n    # Subdivision:   |  0  |  2  |  4\n    c1 = arc_angle > np.pi / 8\n    c2 = arc_angle < 3 * np.pi / 8\n\n    with_arc = is_convex if np.sign(offset) > 0 else np.invert(is_convex)\n    query_d1 = np.all([np.all([c1, c2], axis=0), with_arc], axis=0)\n    query_d2 = np.all([np.invert(c2), with_arc], axis=0)\n\n    # -1: use shared vertex\n    #  0: direct connect\n    #  1: divide once, then connect\n    #  2: divide twice, then connect\n    subdivide = np.zeros(len(query_d2))\n    subdivide[np.invert(with_arc)] = -1\n    subdivide[query_d1] = 1\n    subdivide[query_d2] = 2\n\n    offset_edge_segments = edge_segments + (offset * exterior_normal).reshape(\n        -1, 1, 2)\n    offset_vertex_start = offset_edge_segments[:, 0, :]\n    offset_vertex_end = offset_edge_segments[:, 1, :]\n    offset_vertex_start_next = np.roll(offset_vertex_start, -1, axis=0)\n\n    def subdivision(left, right, centroid, radius, level):\n        if level == 0:\n            return []\n        middle = radius * normalized(0.5 *\n                                     (left + right) - centroid) + centroid\n        return subdivision(left, middle, centroid, radius, level - 1) + [\n            middle\n        ] + subdivision(middle, right, centroid, radius, level - 1)\n\n    arc_vertices = np.array(\n        subdivision(offset_vertex_end, offset_vertex_start_next, vertex_end,\n                    abs(offset), 2)).transpose([1, 0, 2])\n\n    lift_once = lambda x: [y for y in x]\n    lift_twice = lambda x: [lift_once(y) for y in x]\n\n    def get_auxiliary_vertices(shared_vertices, auxiliary_vertices, level):\n        if level == -1:\n            return [shared_vertices]\n        elif level == 0:\n            return []\n        elif level == 1:\n            return [auxiliary_vertices[1]]\n        elif level == 2:\n            return lift_once(auxiliary_vertices)\n\n    # no idea how to vectorize, use for loop instead\n    auxiliary_vertices = [\n        get_auxiliary_vertices(vertex_end[i], arc_vertices[i], subdivide[i])\n        for i in range(len(subdivide))\n    ]\n\n    # edge vertices -> auxiliary vertices -> next edge vertices ...\n    new_vertex_list = [[]] * 2 * len(subdivide)\n    new_vertex_list[0::2] = lift_twice(offset_edge_segments)\n    new_vertex_list[1::2] = auxiliary_vertices\n\n    # Reference: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-a-list-of-lists\n    new_vertex_list_merged = list(\n        itertools.chain.from_iterable(new_vertex_list))\n\n    return new_vertex_list, new_vertex_list_merged\n\n\n# Reference: https://www.glprogramming.com/red/chapter11.html\ndef get_positive_winding_polygon_boundary(polygon):\n\n    polygon = np.array(polygon)\n    if polygon.shape[1] == 2:\n        polygon = np.insert(polygon, 2, 0, axis=1)\n\n    boundary_polygons = []\n\n    def glu_tess_begin_callback(type):\n        global boundary_polygon\n        boundary_polygon = []\n\n    def glu_tess_vertex_callback(vertex_data):\n        global boundary_polygon\n        boundary_polygon.append(vertex_data[:2])\n\n    def glu_tess_end_callback():\n        global boundary_polygon\n        boundary_polygons.append(boundary_polygon)\n\n    def glu_tess_error_callback(errno):\n        print(\"glu_tess_error_callback\", GLU.gluErrorString(errno))\n\n    def glu_tess_combine_callback(vertex,\n                                  neighbors,\n                                  neighborWeights,\n                                  out=None):\n        out = vertex[:3]\n        return out\n\n    tes = GLU.gluNewTess()\n    GLU.gluTessCallback(tes, GLU.GLU_TESS_BEGIN, glu_tess_begin_callback)\n    GLU.gluTessCallback(tes, GLU.GLU_TESS_VERTEX, glu_tess_vertex_callback)\n    GLU.gluTessCallback(tes, GLU.GLU_TESS_END, glu_tess_end_callback)\n    GLU.gluTessCallback(tes, GLU.GLU_TESS_ERROR, glu_tess_error_callback)\n    GLU.gluTessCallback(tes, GLU.GLU_TESS_COMBINE, glu_tess_combine_callback)\n    GLU.gluTessProperty(tes, GLU.GLU_TESS_BOUNDARY_ONLY, GL.GL_TRUE)\n    GLU.gluTessProperty(tes, GLU.GLU_TESS_WINDING_RULE,\n                        GLU.GLU_TESS_WINDING_POSITIVE)\n    GLU.gluTessNormal(tes, 0, 0, 1)\n\n    GLU.gluTessBeginPolygon(tes, None)\n    GLU.gluTessBeginContour(tes)\n    for vert in polygon:\n        GLU.gluTessVertex(tes, vert, vert)\n    GLU.gluTessEndContour(tes)\n    GLU.gluTessEndPolygon(tes)\n    GLU.gluDeleteTess(tes)\n\n    return boundary_polygons\n\n\ndef polygon_offset(polygon, offset):\n    _, edge_offset_polygon = polygon_edge_segments_offset(polygon, offset)\n    return get_positive_winding_polygon_boundary(edge_offset_polygon)\n```\n","n":0.044}}},{"i":63,"$":{"0":{"v":"Connect Polygons with Their Offset Ones","n":0.408},"1":{"v":"\n## Steps:\n1. Get list of edges for exterior and interior polygons\n2. Force exterior to be CCW, interior to be CW\n3. Apply delaunay triangulation\n4. For each delaunay triangle, find the ones that do not intersect with exterior/interior polygons, append its non-polygon edges\n5. Find all chordless cycles in undirected graph using wall-walking algorithm (ignore CW traversal of boundary edges)\n6. Treat all chordless cycles as faces. For non-triangle faces, tessellated them if necessary\n\n## Implementation\n```\nimport numpy as np\nfrom scipy.spatial import Delaunay\nfrom rtree.index import Index\nfrom pyclipper import PointsInPolygon, scale_to_clipper\n\n\ndef gen_loop_edges(polygon, offset):\n    vertex_indices = np.arange(len(polygon))\n    edges = np.vstack([vertex_indices, np.roll(vertex_indices, -1, axis=0)]).T\n    return edges + offset\n\n\ndef gen_poly_edge_indices(polygons):\n    offsets = np.insert(np.cumsum([len(poly) for poly in polygons])[:-1],\n                        obj=0,\n                        values=0)\n\n    return [\n        gen_loop_edges(poly, offset)\n        for (poly, offset) in zip(polygons, offsets)\n    ]\n\n\ndef gen_poly_edges(polygons):\n    poly_edge_indices = gen_poly_edge_indices(polygons)\n    return np.vstack(polygons)[np.vstack(poly_edge_indices)]\n\n\ndef gen_poly_rtree(polygons):\n    poly_edges = gen_poly_edges(polygons)\n    edge_bboxes = np.sort(poly_edges, axis=1).reshape(-1, 4)\n    ids = np.arange(len(edge_bboxes))\n\n    poly_rtree = Index()\n    for (id, bbox) in zip(ids, edge_bboxes):\n        poly_rtree.insert(id, tuple(bbox))\n\n    return poly_edges, poly_rtree\n\n\n# Reference: https://stackoverflow.com/questions/3838329/how-can-i-check-if-two-segments-intersect\ndef ccw(A, B, C):\n    return (C[:, 1] - A[:, 1]) * (B[:, 0] - A[:, 0]) > (B[:, 1] - A[:, 1]) * (\n        C[:, 0] - A[:, 0])\n\n\ndef intersect(A, B, C, D):\n    return np.all([ccw(A, C, D) != ccw(B, C, D),\n                   ccw(A, B, C) != ccw(A, B, D)],\n                  axis=0)\n\n\ndef edge_intersect(key_edge, query_edges):\n\n    key_edge_tiled = np.repeat(key_edge.reshape(1, 2, 2),\n                               len(query_edges),\n                               axis=0)\n\n    disconnected = np.sum(np.isin(query_edges, key_edge_tiled).reshape(-1, 4),\n                          axis=1) == 0\n\n    if np.sum(disconnected) == 0:\n        return False\n\n    intersect_check = intersect(key_edge_tiled[disconnected][:, 0, :],\n                                key_edge_tiled[disconnected][:, 1, :],\n                                query_edges[disconnected][:, 0, :],\n                                query_edges[disconnected][:, 1, :])\n\n    if np.sum(intersect_check) > 0:\n        return True\n    else:\n        return False\n\n\ndef adjaceny_list(edges, vertex):\n    idx, _ = np.where(edges == vertex)\n    query = edges[idx]\n    return query[query != vertex].tolist()\n\n\n# Reference: https://stackoverflow.com/questions/838076/small-cycle-finding-in-a-planar-graph\ndef find_all_chordless_cycles(all_verts, adjaceny_lists, excluded_edges):\n\n    all_cycles = []\n    edge_occurrence_set = set()\n\n    # inner edges shall be traverse twice, while edges belong to the boundary of polygons shall only be travered once\n    # if boundary edges were traversed twice, we end up getting boundary of polygons\n    for edge in excluded_edges:\n        edge_occurrence_set.add((edge[0], edge[1]))\n\n    def find_chordless_cycles(vertex, cycle):\n        adj_verts = adjaceny_lists[vertex]\n\n        # filter out edges that already traversed\n        adj_verts = list(\n            filter(\n                lambda adj_vertex:\n                (vertex, adj_vertex) not in edge_occurrence_set, adj_verts))\n\n        if len(cycle) == 0 and len(adj_verts) > 0:\n            find_chordless_cycles(adj_verts[0], [vertex])\n        elif len(cycle) > 0:\n            edge_occurrence_set.add((cycle[-1], vertex))\n            # don't want to link back\n            adj_verts = list(\n                filter(lambda adj_vertex: adj_vertex != cycle[-1], adj_verts))\n\n            cycle_found = False\n            if vertex in cycle:\n                cycle_found = True\n                all_cycles.append(cycle[cycle.index(vertex):])\n\n            if len(adj_verts) > 0:\n                dir_forward = all_verts[adj_verts] - all_verts[vertex]\n                dir_back = np.tile(all_verts[cycle[-1]] - all_verts[vertex],\n                                   [len(dir_forward), 1])\n                # angle between, clip for robustness\n                # FIXME: [use atan2 for robustness](https://stackoverflow.com/questions/10133957/signed-angle-between-two-vectors-without-a-reference-plane)\n                angle = np.arccos(\n                    np.clip(\n                        np.sum(dir_forward * dir_back, axis=1) /\n                        (np.linalg.norm(dir_forward, axis=1) *\n                         np.linalg.norm(dir_back, axis=1) + 1e-13),\n                        -1.0 + 1e-13, 1.0 - 1e-13))\n\n                # sign, positive if at left (CCW), negative if at right (CW)\n                opposite = np.sum(\n                    np.cross(np.tile([0, 0, 1], [len(dir_forward), 1]),\n                             np.insert(dir_back, 2, 0, axis=1))[:, :2] *\n                    dir_forward,\n                    axis=1) < 0\n                angle[opposite] = 2 * np.pi - angle[opposite]\n\n                find_chordless_cycles(adj_verts[np.argmin(angle)],\n                                      [vertex] if cycle_found else cycle +\n                                      [vertex])\n\n    for i in range(len(all_verts)):\n        find_chordless_cycles(i, [])\n\n    return all_cycles\n\n\ndef filter_intersecting_triangles(pt, tri, polygons):\n    verts = pt[tri]\n    verts_next = np.roll(verts, -1, axis=1)\n    edges = np.concatenate([verts, verts_next], axis=-1).reshape(-1, 3, 2, 2)\n    # we are only interested in edges that does not belong to a polygon\n    non_edge_indices = np.abs(np.diff(tri, append=tri[:, 0].reshape(-1,\n                                                                    1))) != 1\n\n    query_edges = edges[non_edge_indices]\n    query_bboxes = np.sort(query_edges, axis=1).reshape(-1, 4)\n    unique_query_bboxes, unique_index, unique_inverse_index = np.unique(\n        query_bboxes, return_index=True, return_inverse=True, axis=0)\n    unique_query_edges = query_edges[unique_index]\n\n    # not thread safe, cannot be reused\n    poly_edges, poly_rtree = gen_poly_rtree(polygons)\n\n    query_results = np.array([\n        edge_intersect(key_edge, poly_edges[query_indices])\n        for (key_edge, query_indices) in zip(unique_query_edges, [\n            list(poly_rtree.intersection(bbox)) for bbox in unique_query_bboxes\n        ])\n    ])\n\n    edge_intersection_results = np.full_like(tri, False, dtype=bool)\n    edge_intersection_results[non_edge_indices] = query_results[\n        unique_inverse_index]\n\n    return np.sum(edge_intersection_results, axis=1) > 0\n\n\ndef filter_auxiliary_edges(pt, tri, polygons):\n    poly_edges, poly_rtree = gen_poly_rtree(polygons)\n\n    verts = pt[tri]\n    verts_next = np.roll(verts, -1, axis=1)\n    # filltering needs to be performed at triangle level\n    edges = np.concatenate([verts, verts_next], axis=-1).reshape(-1, 3, 2, 2)\n\n    edge_indices = np.concatenate([tri, np.roll(tri, -1, axis=1)],\n                                  axis=-1).reshape(-1, 3, 2)\n\n    # we are only interested in edges that does not belong to a polygon\n    # first filter out consecutive edges\n    non_edge_indices = np.abs(np.diff(tri, append=tri[:, 0].reshape(-1,\n                                                                    1))) != 1\n\n    # then filter out polygon last edges\n    poly_edge_indices = gen_poly_edge_indices(polygons)\n    polygon_last_edges = [\n        edge_index[-1].tolist() for edge_index in poly_edge_indices\n    ]\n    edge_indices_list = edge_indices[non_edge_indices].tolist()\n    tmp = non_edge_indices[non_edge_indices]\n    tmp[[edge_index in polygon_last_edges\n         for edge_index in edge_indices_list]] = False\n    non_edge_indices[non_edge_indices] = tmp\n\n    # prepare for rtree query\n    query_edges = edges[non_edge_indices]\n    query_bboxes = np.sort(query_edges, axis=1).reshape(-1, 4)\n    unique_query_bboxes, unique_index, unique_inverse_index = np.unique(\n        query_bboxes, return_index=True, return_inverse=True, axis=0)\n    unique_query_edges = query_edges[unique_index]\n\n    query_results = np.array([\n        edge_intersect(key_edge, poly_edges[query_indices])\n        for (key_edge, query_indices) in zip(unique_query_edges, [\n            list(poly_rtree.intersection(bbox)) for bbox in unique_query_bboxes\n        ])\n    ])\n\n    auxiliary_edges = edge_indices[non_edge_indices][unique_index][np.invert(\n        query_results)]\n\n    return np.vstack(poly_edge_indices), auxiliary_edges\n\n\ndef delaunay_triangulation(exterior_polygons, interior_polygons):\n    tri = Delaunay(np.vstack(exterior_polygons + interior_polygons))\n\n    # triangle is convex\n    barycenters_clipper = np.sum(\n        np.array(scale_to_clipper(tri.points))[tri.simplices], axis=1) / 3\n\n    def barycenters_in_polygons(polygons):\n        return [\n            PointsInPolygon(barycenters_clipper, scale_to_clipper(polygon))\n            for polygon in polygons\n        ]\n\n    points_in_exterior = np.any(np.array(\n        barycenters_in_polygons(exterior_polygons)) == 1,\n                                axis=0)\n    point_not_in_interior = np.all(np.array(\n        barycenters_in_polygons(interior_polygons)) == 0,\n                                   axis=0)\n\n    if not exterior_polygons:\n        points_in_exterior = [True] * len(points_in_exterior)\n\n    if not interior_polygons:\n        point_not_in_interior = [True] * len(points_in_exterior)\n\n    valid_indices = np.all((points_in_exterior, point_not_in_interior), axis=0)\n\n    if not exterior_polygons or not interior_polygons:\n        tmp = valid_indices[valid_indices]\n        tmp[filter_intersecting_triangles(\n            tri.points, tri.simplices[valid_indices],\n            exterior_polygons + interior_polygons)] = False\n\n        valid_indices[valid_indices] = tmp\n\n        return tri.points, tri.simplices[valid_indices].tolist()\n\n    else:\n        poly_edges, auxiliary_edges = filter_auxiliary_edges(\n            tri.points, tri.simplices[valid_indices],\n            exterior_polygons + interior_polygons)\n\n        all_edges = np.vstack([poly_edges, auxiliary_edges])\n        adjaceny_lists = [\n            adjaceny_list(all_edges, i) for i in range(len(tri.points))\n        ]\n        return tri.points, find_all_chordless_cycles(tri.points,\n                                                     adjaceny_lists,\n                                                     poly_edges)\n```\n\n> Patch for pyclipper\n\n```\ndiff --git a/src/pyclipper/_pyclipper.pyx b/src/pyclipper/_pyclipper.pyx\nindex bb179a1..a952a01 100644\n--- a/src/pyclipper/_pyclipper.pyx\n+++ b/src/pyclipper/_pyclipper.pyx\n@@ -319,6 +319,33 @@ def PointInPolygon(point, poly):\n     return result\n \n \n+def PointsInPolygon(points, poly):\n+    \"\"\" Determine where does the point lie regarding the provided polygon.\n+    More info: http://www.angusj.com/delphi/clipper/documentation/Docs/Units/ClipperLib/Functions/PointInPolygon.htm\n+\n+    Keyword arguments:\n+    points -- a list of positions in question\n+    poly  -- closed polygon\n+\n+    Returns:\n+    List of array the same length as points, with each of its element\n+        0  -- point is not in polygon\n+        -1 -- point is on polygon\n+        1  -- point is in polygon\n+    \"\"\"\n+\n+    cdef Path c_path = _to_clipper_path(poly)\n+    cdef IntPoint *c_points = <IntPoint *> malloc(len(points) * sizeof(IntPoint))\n+    cdef int* results = <int *> malloc(len(points) * sizeof(int))\n+    for i in range(len(points)):\n+        c_points[i] = _to_clipper_point(points[i])\n+        with nogil:\n+            results[i] = <int>c_PointInPolygon(c_points[i], c_path)\n+    results_python = [results[i] for i in range(len(points))]\n+    free(c_points)\n+    return results_python\n+\n+\n def SimplifyPolygon(poly, PolyFillType fill_type=pftEvenOdd):\n     \"\"\" Removes self-intersections from the supplied polygon.\n     More info: http://www.angusj.com/delphi/clipper/documentation/Docs/Units/ClipperLib/Functions/SimplifyPolygon.htm\n```","n":0.032}}},{"i":64,"$":{"0":{"v":"Paper Read","n":0.707}}},{"i":65,"$":{"0":{"v":"Project Starline A high-fidelity telepresence system","n":0.408},"1":{"v":"\n> https://storage.googleapis.com/pub-tools-public-publication-data/pdf/3696afb4c1cccbe0876a9fedd1586f0f9c84f737.pdf\n\n## HCI\nVR, AR: hard to capture under headset, insufficient pixel density and viewport\n\nautostereoscopic display (65-inch 8K 33.1M pixel 60Hz, 1.25m eye-to-eye distance, 45 pixel per degree)\n\n- Face near display panel (reduce disparity)\n- Place middle wall to block bottom of display (reduce depth conflicts)\n- Matching remote to local transform for both sides (ensure mutual eye contact)\n\n### TODO\n- [ ] crosstalk\n- [ ] vergence-accommodation conflict\n- [ ] disparity\n- [ ] depth conflicts\n- [ ] mutual eye contact\n\n## Setup\n- 4 tracking cameras (2 above, 2 side): localized eyes, ears, mouth, 120Hz\n- 3 depth (stereo) camera: depth stream, 180Hz -> image-based depth fusion\n- 4 color camera: color stream, 60Hz -> Normal based texture blending\n\n### TODO\n- [ ] HRTF (head-related transfer function)\n- [ ] Normal based texture blending\n\n## Implementation\n\n### Lighting\n- IBL (Image-based rendering), no illumination or reflectance\n- Interpolates textures from 4 color cameras\n- Non-Lambertian reflectance rander incorrectly under non-diffuse lighting (mitigated using soft lighting)\n- illumination nonuniformity (stronger intensities for display unit near wall)\n\n### Calibration\n- Minimizing reprojection error over planar targets\n- Color calibration to Illuminant D65 (gain, color correction matrix, gamma)\n\n### Capture\n- Capture sensor placed at periphery of the display, large parallax\n- Near-infrared (NIR) global shutter image sensors\n- Customized depth sensor with near depth continuities\n- 3 types of NIR illumination (NIR bounce light, NIR spot light, illuminate back wall)\n\n### 3D Face Tracking\n- Eye location -> stereo rendering\n- Mouth & ear location -> spatial audio rendering + crosstalk cancellation\n- 34 facial landmarks, determine 5 2D features (eyes, mouth, ears) and triangulated into 3D\n- Latency causes crosstalk, mitigated by extrapolation + double exponential smoothing + hysteresis filter\n\n### Compression and transmission\n- Transmit color (8 bit) and depth (10 bit) images via video compression (NVENC/NVDEC unit, H.256 codec, YUV420 chroma subsampling)\n- Clamp depth to reduce quantization artifacts\n- Transmit via WebRTC\n\n### Rendering\n- Compute shadow map from color cameras by raycasting on input depth map\n- Compute user view output depth maps by raycasting on input depth map\n- Use shadow map to weighted color blend output depth map\n\n#### Traditional Surface fusion\n- Fuse depth view into [[epipolar-geometry.tsdf]] as volumetric grid, weighting depth pixel based on depth gradient\n- March along rays into precomputed voxel grid, find root (iso surface)\n\n#### Novel raycast approach\n- Rasterizing input depth view to low-res output view, find lower/upper bound of distance along the ray using 2D min/max filter, dilate it with small margin\n- For any point on user view ray, for each depth images, transform point to depth camera view coordinate, sample depth and fusion weight\n- Fusion weight is inverse proportional to depth standard deviation over 7x7 pixel neighbourhood (0 if point is outside viewport)\n\n#### Weighted color blending\n- Compute partial visibility using percentage close filter on shadow map\n- Modulate blend weight by squared cosine of angle between surface normal and camera vector (?)\n- Adaptively blur composite image along depth discontinuities (Edge blending)\n\n### Audio\n[skip for now]\n\n### TODO\n- [ ] Non-Lambertian reflectance\n- [ ] Non-diffuse lighting\n- [ ] Lighting ratio\n- [ ] Non-planar warp\n- [ ] ESPReSSo\n- [ ] NIR illumination\n- [ ] Hysteresis filter\n- [ ] NVENC & NVDEC\n- [ ] TSDF depth gradient weighting\n- [ ] [2D min/max filter](http://www.code-spot.co.za/2011/01/24/2d-minimum-and-maximum-filters-algorithms-and-implementation-issues/)\n- [ ] Bisection search\n- [ ] Relief texture mapping\n- [ ] [Percentage closer filter](https://developer.nvidia.com/gpugems/gpugems/part-ii-lighting-and-shadows/chapter-11-shadow-map-antialiasing)\n- [ ] Squared cosine of angle between surface normal and camera vector","n":0.043}}},{"i":66,"$":{"0":{"v":"Lightweight Multi Person Total Motion Capture","n":0.408},"1":{"v":"\n> https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Lightweight_Multi-Person_Total_Motion_Capture_Using_Sparse_Multi-View_Cameras_ICCV_2021_paper.pdf\n\n## Single image / video\n\n### Pose regression\n> SMPL-X & Adam\n\n- Direct regression from image to parameters\n- Real time but not suitable for occlusion or multi-person\n- No geometric correspondence with 2D joints\n\n### Key-point detection\n- Guarantee geometric alignment\n- Need heavy post-processing\n- Susceptible to self-occlusion\n\n## Sparse Multi-view\n\n### 4D Body Association\n\nSparse multi-view\n- 2D keypoints -> triangulate 3D keypoints\n- Tracked 3D point -> match 2D detection (Pnp?)\n\n### Hand and Face Bootstrapping\n\n- body skeleton -> 3D bounding sphere with constant radius (commensurate with physical size) -> project 2D bbox -> further refinement with hand detector -> refined 2D bbox\n- For each refined 2D bbox (associated with body wrist), apply NMS using hand association score (crossmodality consistency score and cross-scale consistency score)\n> **crossmodality consistency score** (pose-regression chirality ambiguity): \n> $$\navg(\\max(0, 1 - \\frac{\\|\\text{pose-regression position} - \\text{keypoint-detection position}\\|_2^2}{diag(RoI)}))$$\n>\n> **cross-scale consistency score** (wrist misalignment):\n> $$\navg(\\max(0,1 - \\frac{\\|\\text{body skeleton wrist position} - \\text{local wrist position}\\|_2^2}{diag(RoI)}))$$\n\n### Two-stage Parametric Fitting\n- local intialization: \n    - hand - hand association score\n    - body/head - SMPLify-X\n- total optimization: l2 body 3D position distance + 2d projected keypoints distance + gesture range regularization\n\n### Feedback\n- limb detector\n- re-render human model for visibility -> binary occupancy mask -> smoothed continuous occupancy mask -> occupancy probability of the person\n\n## Questions\n- [ ] 3D -> 2D projection (projection matrix with hnormalize? gradient backprop?)\n- [ ] Body ground truth 3D?\n- [ ] Hand association score for different views balancing (the parameter to refine?)\n- [ ] Distance transformations for smoothing\n- [ ] Human body parameterization\n\n## TODO\n- [ ] *SMPLify-X (regularization term)\n- [ ] Adam\n- [ ] 4D body association\n- [ ] Mean Per Joint Position Error\n- [ ] Heatmap-based detector\n","n":0.061}}},{"i":67,"$":{"0":{"v":"KinectFusion Real-Time Dense Surface Mapping and Tracking","n":0.378},"1":{"v":"\n## Surface Measurement\n### Notations\n- $\\bm{T}_{g,k}$: camera pose at time $k$ ($\\bm{T}_{g,k}$ is camera [[Model Matrix|rendering.camera-mvp-matrix#model-matrix]], $\\bm{T}_{g,k}^{-1}$ is camera [[View Matrix|rendering.camera-mvp-matrix#view-matrix]])\n- $K$: camera intrinsic matrix\n- $\\bm{p}_k$: point in camera frame at time $k$\n- $\\bm{p}_g$: point in world frame\n- $\\bm{q} = \\pi(\\bm{p})$: point in image frame (hnormalized)\n- $R_k$: Raw depth map at time $k$\n- $\\bm{u} = (u, v)$: image pixel\n- $\\dot{\\bm{u}} = [\\bm{u}^T|1]^T$: image pixel in homogenous coordinate\n- $R_k(\\bm{u})$: depth value at pixel $\\bm{u}$\n\n### Known relations\n$$\n\\bm{p}_g = \\bm{T}_{g,k} \\bm{p}_k\n$$\n\n\n$$\n\\bm{p}_k = R_k(\\bm{u})\\bm{K}^{-1}\\dot{\\bm{u}}\n$$\n\n### Steps\n1. Apply [[Bilateral filter|geometry.mesh-smoothing#bilateral-filter]] to obtain denoised depth map $D_k$\n2. Back-project filtered depth to obtain vertex map $\\bm{V}_k(\\bm{u}) = D_k(\\bm{u})\\bm{K}^{-1}\\dot{\\bm{u}}$\n3. Compute normal map $\\bm{N}_k(\\bm{u}) = normalized((\\bm{V}(u+1, v) - \\bm{V}(u, v))\\times(\\bm{V}(u, v+1)- \\bm{V}(u, v)))$\n4. Compute binary mask map $\\bm{M}_k$ for valid vertices\n5. Computer 3 level vertex normal map pyramid via mipmapping $D_k$\n\n## Mapping as Surface Reconstruction\n\n### Notations\n- $F_k(\\bm{p}_g)$: truncated signed distance value\n- $W_k(\\bm{p}_g)$: weight\n- $\\bm{S}_k(\\bm{p}_g)$: [[epipolar-geometry.tsdf]]\n- $\\mu$: measure uncertainty\n- $r$: distance from camera center along depth ray\n- $\\lambda = \\| \\bm{K}^{-1} \\dot{\\bm{u}} \\|_2$: distance along depth ray (distance between projected $\\bm{u}$ and camera center)\n\n> **IMPORTANT** Unproject image point is depth ambiguous\n\n### TSDF representation\n1. Region within uncertainty measurement: $|r - \\lambda R_k(\\bm{u})| \\leq \\mu$\n2. Region within visible space but at distance greater than $\\mu$: $\\mu$\n3. Non-visible points further than $\\mu$: null\n\n### Projective TSDF\n- Easy to compute and trivially parallelizable\n- Approximation but converges to SDF\n\nGiven raw depth map $R_k$ with known pose $\\bm{T}_{g, k}$, for $\\bm{p}_g$ in global frame, compute\n\n$$\n\\Psi(\\lambda^{-1} \\|\\bm{t}_{g, k} - \\bm{p}_g\\|_2 - R_k(\\bm{x}))\n$$\n\n1. Project $\\bm{p}_g$ into depth image coordinate to find closet pixel ($\\lfloor . \\rfloor$ denotes nearest neighbour lookup)\n$$\n\\bm{x} = \\lfloor \\pi(\\bm{K} \\bm{T}_{g, k}^{-1} \\bm{p}_g) \\rfloor\n$$\n2. Distance to camera center\n$$\n\\lambda = \\|\\bm{K}^{-1} \\dot{\\bm{x}} \\|_2\n$$\n3. Distance between $\\bm{p}_{g, k}$ and camera position in global frame\n$$\n\\|\\bm{t}_{g, k} - \\bm{p}_g\\|_2\n$$\n4. Compute projective signed distance\n$$\n\\lambda^{-1} \\|\\bm{t}_{g, k} - \\bm{p}_g\\|_2 - R_k(\\bm{x})\n$$\n5. Truncate signed distance as describe in [[TSDF representation|paper-read.kinect-fusion#tsdf-representation]]. Note its is scale so that non-trucated values before/after zero crossing are stored in discrete volume\n$$\n\\Psi(\\eta) =\n\\begin{cases}\n\\min(1, \\frac{\\eta}{\\mu}) sgn(\\eta) &\\text{iff}\\ \\eta \\geq -\\mu \\\\\n\\text{null} &\\text{otherwise}\n\\end{cases}\n$$\n6. $W_k(\\bm{p}_g)$ is proportional to $\\cos\\theta / R_k(\\bm{x})$, where $\\theta$ is angle between ray and surface normal\n\n### Global fusion\n\nApply [[Incremental calculation (combine depth image one by one)|epipolar-geometry.tsdf#incremental-calculation-combine-depth-image-one-by-one]] pointwise for $\\{\\bm{p}|F_k(\\bm{p}) \\neq \\text{null}\\}$ (unmeasurable region is not updated)\n- Unit weight is good enough\n- Truncate updated weight allow moving average reconstruction\n- Fuse raw depth map as smoothed one removes the high frequency structure\n\n## Surface Prediction from Ray Casting the TSDF\n- Apply per pixel raycast. For ray $\\bm{T}_{g, k} \\bm{K}^{-1} \\dot{\\bm{u}}$, start at minimum depth and stops at zero crossing (front / back) or exit volume\n- Surface normal is estimated with the numerical [[gradient of iso-surface|geometry.signed-distance-function#gradient-of-iso-surface]]\n- March faster with trucated value as non-trucated value must exist before zero crossing, as it is guaranteed by trucate function scaling\n- If $F^{+}_t$ and $F^{+}_{t + \\Delta t}$ are at either side of zero crossing, approximate interpolation point with linear interpolation\n$$\nt^{*} = t - \\frac{\\Delta t F_t^{+}}{F^{+}_{t + \\Delta t} - F_t^{+}}\n$$ \n\n## TODO\n- [ ] Is $\\| \\bm{K}^{-1} \\dot{\\bm{u}} \\|_2 R_k(\\bm{u})$ equivalent to $\\|\\bm{K}^{-1}[u R_k(\\bm{u}), vR_k(\\bm{u}), R_k(\\bm{u})]^T\\|_2$?\n- [ ] Surface normal in fusion\n- [ ] Min/max block acceleration\n- [ ] Ray/trilinear cell intersection","n":0.044}}},{"i":68,"$":{"0":{"v":"Optimization","n":1}}},{"i":69,"$":{"0":{"v":"Newton Method","n":0.707},"1":{"v":"\nApproximate function value via first order derivative\n\n## Root finding\n\n$$\nf(x_n) \\approx f(x_{n-1}) + f'(x_{n-1})(x_n - x_{n-1}) = 0\n\\\\\nx_n = x_{n-1} - \\frac{f(x_{n-1})}{f'(x_{n-1})}\n$$\n\n## Optimization\n\n$$\nf'(x_n) \\approx f'(x_{n-1}) + f''(x_{n-1})(x_n - x_{n-1}) = 0 \n\\\\\nx_n = x_{n-1} - \\frac{f'(x_{n-1})}{f''(x_{n-1})}\n$$\n","n":0.169}}},{"i":70,"$":{"0":{"v":"Least Square (LS)","n":0.577},"1":{"v":"\n> Reference: http://www.nealen.net/projects/mls/asapmls.pdf\n\nUnder the assumption of gaussian likelihood.\nRandom variable $\\bm{X} \\in \\mathbb{R}^{n \\times d}$, parameter $\\bm{\\theta} \\in \\mathbb{R}^{d}$, observation $\\bm{y} \\in \\mathbb{R}^{n}$, the residual\n$$\n\\bm{r} = \\bm{y} - \\bm{X} \\bm{\\theta} \\in \\mathbb{R}^n\n$$\n\n## Least square approach\n\n$$\ny \\sim \\mathcal{N}(\\bm{y}|\\bm{X} \\bm{\\theta},\\sigma^2 \\bm{I})\n\\\\\nL = \\frac{1}{2} \\bm{r}^T\\bm{r} = \\frac{1}{2} (\\bm{y} - \\bm{X} \\bm{\\theta})^T (\\bm{y} - \\bm{X} \\bm{\\theta}) \\in \\mathbb{R}\n\\\\\n\\frac{\\partial L}{\\partial \\bm{\\theta}} = -(\\bm{y} - \\bm{X} \\bm{\\theta})^T \\bm{X} \\in \\mathbb{R}^{1 \\times d}\n$$\n\nSet derivative to be 0\n\n$$\n\\bm{\\theta}^* = (\\bm{X}^T \\bm{X})^{-1} \\bm{X}^T \\bm{y} \\in \\mathbb{R}^d\n$$\n\nMore see [[Maximum Likelihood Estimation (MLE)|probability.parameter-estimation.maximum-likelihood-estimation-mle]]","n":0.111}}},{"i":71,"$":{"0":{"v":"Weighted Least Square","n":0.577},"1":{"v":"\n> Reference https://statweb.stanford.edu/~jtaylo/courses/stats203/notes/robust.pdf\n\n$$\ny \\sim \\mathcal{N}(\\bm{y}|\\bm{X} \\bm{\\theta},\\sigma^2 \\bm{W}^{-1})\n$$\n\nwhere diagonal weight matrix $\\bm{W} \\in \\mathbb{R}^{n \\times n}$\n\n$$\nL = \\frac{1}{2} \\bm{r}^T \\bm{W}\\bm{r} = \\frac{1}{2} (\\bm{y} - \\bm{X} \\bm{\\theta})^T \\bm{W}(\\bm{y} - \\bm{X} \\bm{\\theta}) \\in \\mathbb{R}\n\n\\\\\n\n\\frac{\\partial L}{\\partial \\bm{\\theta}} = -(\\bm{y} - \\bm{X}\\bm{\\theta})^T\\bm{W}\\bm{X} \\in \\mathbb{R}^{1 \\times d}\n$$\n\nSet derivative to be 0\n\n$$\n\\bm{\\theta}^* = (\\bm{X}^T \\bm{W} \\bm{X})^{-1} \\bm{X}^T \\bm{W} \\bm{y} \\in \\mathbb{R}^d\n$$\n","n":0.136}}},{"i":72,"$":{"0":{"v":"Moving Least Square","n":0.577},"1":{"v":"\n> Reference: https://www.ams.org/journals/mcom/1981-37-155/S0025-5718-1981-0616367-1/S0025-5718-1981-0616367-1.pdf\n\nPerform weighted least square at each point $\\bm{x} \\in \\bm{X}$ locally ([[Polynomial basis|probability.parameter-estimation.maximum-likelihood-estimation-mle#polynomial-basis]], over $k$ neighbourhood points within a distance threshold, with weight as a function of distance).\n\n$$\n\\argmin_{\\bm{\\theta}_{\\bm{x}}} \\sum_{i=1}^{k} \\theta(\\| \\bm{x}_i - \\bm{x} \\|_2) (\\bm{y}_i - \\phi(\\bm{x}_i)^T \\bm{\\theta}_{\\bm{x}})^2\n$$\n\nwhere $\\phi(.)$ is multivariate polynomial basis function, $\\theta$ is weight function, $\\bm{\\theta}_{\\bm{x}}$ is parameter to be estimated at $\\bm{x}$\n\n> **IMPORTANT** This method is different from previous method, as it needs $\\bm{\\theta}_{\\bm{x}_i}$ for each $\\bm{x}_i$, so $n$ in total.\n\nFor test data, find $\\bm{\\theta}$ by interpolating neighbourhood $\\bm{\\theta}_{\\bm{x}}$ weighted by same function of distance (interpolation needs weights normalized).\n\n## Derivatives computation\n\nFor each sample $\\bm{x}_i \\in \\bm{X}$ with $k$ neighbour points $\\bm{X}_i = \\begin{bmatrix} \\bm{x}_0 & \\dots & \\bm{x}_j & \\dots & \\bm{x}_k \\end{bmatrix}^T$. Given $\\bm{y}_i = \\begin{bmatrix} y_0 & \\dots & y_k \\end{bmatrix}^T$, $\\bm{\\Phi}_i = \\begin{bmatrix} \\phi(\\bm{x}_0) & \\dots & \\phi(\\bm{x}_k) \\end{bmatrix}^T$; weight $\\bm{W}_i = diag(\\theta(\\|\\bm{x}_i - \\bm{x}_j\\|))$\n\nThe total gradient is:\n\n$$\n\n\\frac{\\partial L}{\\bm{\\partial \\theta}} = \\begin{bmatrix} \\frac{\\partial L}{\\partial \\bm{\\theta}_0}^T & \\dots & \\frac{\\partial L}{\\partial \\bm{\\theta}_i}^T & \\dots & \\frac{\\partial L}{\\partial \\bm{\\theta}_n}^T \\end{bmatrix} \\in \\mathbb{R}^{d \\times n}\n$$\n\nwhere \n\n$$\n\\frac{\\partial L}{\\partial \\bm{\\theta}_i} = -(\\bm{y}_i - \\bm{\\Phi}_i\\bm{\\theta}_i)^T\\bm{W}_i\\bm{\\Phi}_i\n$$\n\n## Wendland function\n> Reference: https://www.researchgate.net/publication/240320304_Geometric_transformation_of_the_RBF_implicit_surface\n\nCompactly-supported Radius basis function $\\phi_k$ for smoothness parameter $k=0,1,2$\n\n$$\n\\phi(r)_0 = (1 - r)_+^2 \\\\\n\\phi(r)_1 = (1 - r)_+^4(4r + 1) \\\\\n\\phi(r)_2 = (1 - r)_+^6(35r^2 + 18r + 3)\n$$\n\nwhere\n$$\n(1 - r)_+ =\n\n\\begin{cases}\n1 - r & r < 1 \\\\\n0     & \\text{otherwise}\n\\end{cases}\n\n$$\n\n### Radial Basis function\n> Related: [[Kernel Trick|linear-algebra.feature-extraction.principle-component-analysis#kernel-trick]]\n\n> Reference https://arxiv.org/pdf/1203.5696.pdf\n\nFunction $\\Phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}$ is radial if $\\Phi(\\bm{x}) = \\phi(\\|\\bm{x}\\|_2)$ for all $\\bm{x} \\in \\mathbb{R}^d$. Radial Basis Function (RBF) can be defined for a given center $\\bm{x}_i \\in \\mathbb{R}^d$ as\n$$\n\\Phi_i(\\bm{x}) = \\phi(\\| \\bm{x} - \\bm{x}_i \\|_2)\n$$\nThe function values depends only on the distance between input $\\bm{x}$ and center $\\bm{x}_i$S\n\n\nRBFs can be categorised as:\n\n- **Global-supported basis function**: consider all distance\n- **Compactly-supported basis function**: ignore point pairs beyond a predefined distance\n","n":0.058}}},{"i":73,"$":{"0":{"v":"Iteratively Reweighted Least Square","n":0.5},"1":{"v":"\n> Reference: http://sepwww.stanford.edu/data/media/public/docs/sep115/jun1/paper_html/node2.html\n\nChoose $diag(\\bm{W}) = |\\bm{r}|^{(p-2)}$, so that the loss function\n$$\n\\bm{r}^T \\bm{W} \\bm{r} = \\bm{r}^T |\\bm{r}|^{p-2} \\bm{r} = \\bm{r}^p\n$$\nwhich minimize the $l_p$ norm of the residual (usually $1 \\le p \\le 2$)\n\n## Steps\n1. Start with $diag(\\bm{W}) = \\bm{1}$\n2. Compute weighted least square fit and residual $\\bm{r}$\n3. Adjust weight $diag(\\bm{W}) = |\\bm{r}|^{(p-2)/2}$\n4. Re-fit and repeat for few iterations\n","n":0.132}}},{"i":74,"$":{"0":{"v":"Gaussian Newton Method","n":0.577},"1":{"v":"\nGiven objective function\n$$\n\\argmin_x F(x) = e(x)^T e(x)\n$$\n\nSimilar to [[optimization.newton-method]], we locally approximate its first order derivative and set it to 0\n\n$$\n\\nabla F(x + \\Delta x) = \\nabla F(x) + \\nabla^2F(x) \\Delta x = 0\n$$\n\nLet $J = \\frac{\\partial e}{\\partial x}$, $H = \\frac{\\partial^2 e}{\\partial x^2}$\n$$\n\\nabla F(x) = \\frac{\\partial F}{\\partial x} = \\frac{\\partial F}{\\partial e} \\frac{\\partial e}{\\partial x} = 2 e^T(x)J\n$$\n\n$$\n\\nabla^2 F(x) = \\frac{\\partial^2 F}{\\partial x^2} = \\frac{\\partial}{\\partial x}(\\frac{\\partial F}{\\partial e} \\frac{\\partial e}{\\partial x}) = \\frac{\\partial 2 e^T(x)J}{\\partial x} = 2(e^T(x)H + J^TJ) \\approx 2J^TJ\n$$\n\nThus we solve\n\n$$\n2 e^T(x)J + 2J^TJ \\Delta x = 0\n$$\nThen set new guess\n$$\nx^* = x + \\Delta x\n$$","n":0.1}}},{"i":75,"$":{"0":{"v":"Metal","n":1}}},{"i":76,"$":{"0":{"v":"Computing Kernel","n":0.707},"1":{"v":"\n## Example\n\nMixing two cubemap texture and write result to another cubemap texture\n\n### Shader\n\n> BlendKernel.metal\n\n```\n#include <metal_stdlib>\nusing namespace metal;\n\nkernel void\nblendKernel(texturecube<half, access::read>  refTexture  [[texture(0)]],\n            texturecube<half, access::read>  curTexture  [[texture(1)]],\n            texturecube<half, access::write> outTexture [[texture(2)]],\n            constant float& alpha [[buffer(0)]],\n            uint2 gid [[thread_position_in_grid]])\n{\n    outTexture.write(mix(refTexture.read(gid, 0), curTexture.read(gid, 0), alpha), gid, 0);\n    outTexture.write(mix(refTexture.read(gid, 1), curTexture.read(gid, 1), alpha), gid, 1);\n    outTexture.write(mix(refTexture.read(gid, 2), curTexture.read(gid, 2), alpha), gid, 2);\n    outTexture.write(mix(refTexture.read(gid, 3), curTexture.read(gid, 3), alpha), gid, 3);\n    outTexture.write(mix(refTexture.read(gid, 4), curTexture.read(gid, 4), alpha), gid, 4);\n    outTexture.write(mix(refTexture.read(gid, 5), curTexture.read(gid, 5), alpha), gid, 5);\n}\n\n```\n\n### Usage\n\n```\nguard let commandQueue = device.makeCommandQueue() else { fatalError(\"Failed to create command queue\") }\nlet defaultLibrary = device.makeDefaultLibrary()\nlet kernelFunction = defaultLibrary?.makeFunction(name: \"blendKernel\")\ndo {\n    try computePipelineState =\n    device.makeComputePipelineState(function: kernelFunction!)} catch {\n        fatalError(\"Failed to create compute kernel pipeline state\")\n}\n\nlet envTextureDescriptor = MTLTextureDescriptor()\nenvTextureDescriptor.textureType = .typeCube\nenvTextureDescriptor.pixelFormat = .rgba16Float\nenvTextureDescriptor.width = width\nenvTextureDescriptor.height = height\nenvTextureDescriptor.depth = 1\nenvTextureDescriptor.arrayLength = 1\nenvTextureDescriptor.mipmapLevelCount = 9\nenvTextureDescriptor.sampleCount = 1\nenvTextureDescriptor.usage = [.shaderRead, .shaderWrite]\n\nlet refEnvTexture = self.device.makeTexture(descriptor: envTextureDescriptor)!\nlet curEnvTexture = self.device.makeTexture(descriptor: envTextureDescriptor)!\nlet outEnvTexture = self.device.makeTexture(descriptor: envTextureDescriptor)!\n\n\n// Set the compute kernel's threadgroup size to 16 x 16.\nthreadgroupSize = MTLSize(width: 16, height: 16, depth: 1)\nthreadgroupCount = MTLSize()\n// Calculate the number of rows and columns of threadgroups given the size of the\n// input image. Ensure that the grid covers the entire image (or more).\nthreadgroupCount.width  = (envTextureDescriptor.width + threadgroupSize.width - 1)\n                            / threadgroupSize.width\nthreadgroupCount.height = (envTextureDescriptor.height + threadgroupSize.height - 1)\n                            / threadgroupSize.height\n// The image data is 2D, so set depth to 1.\nthreadgroupCount.depth = envTextureDescriptor.depth\n\n\nlet commandBuffer = commandQueue.makeCommandBuffer()\nlet computeEncoder = commandBuffer?.makeComputeCommandEncoder()\ncomputeEncoder?.setComputePipelineState(computePipelineState)\ncomputeEncoder?.setTexture(refEnvTexture, index: 0)\ncomputeEncoder?.setTexture(curEnvTexture, index: 1)\ncomputeEncoder?.setTexture(outEnvTexture, index: 2)\ncomputeEncoder?.setBytes(&alpha, length: MemoryLayout.size(ofValue: alpha), index: 0)\ncomputeEncoder?.dispatchThreadgroups(threadgroupCount, threadsPerThreadgroup: threadgroupSize)\ncomputeEncoder?.endEncoding()\nlet blitEncoder = commandBuffer?.makeBlitCommandEncoder()\nblitEncoder?.generateMipmaps(for: outEnvTexture)\nblitEncoder?.endEncoding()\ncommandBuffer?.commit()\n```\n","n":0.064}}},{"i":77,"$":{"0":{"v":"Markdown","n":1}}},{"i":78,"$":{"0":{"v":"Tables","n":1},"1":{"v":"\n> Reference: https://www.markdownguide.org/extended-syntax/\n\n## Basic table\n\n3 or more hyphens `---` for header\n\n| Head 1 | Head 2 |\n| --- | ----------- |\n| Body 1 | Body 2 |\n\n## Alignment\n\n1 colon `:` for alignment\n\n| Head left + placeholder | Head middle + placeholder | Head right + placeholder |\n| :-------- | :---------: | ---------: |\n| Body left | Body middle | Body right |","n":0.127}}},{"i":79,"$":{"0":{"v":"Linux","n":1}}},{"i":80,"$":{"0":{"v":"Docker","n":1}}},{"i":81,"$":{"0":{"v":"Proxy","n":1},"1":{"v":"\n## Build\n\nBuild over localHost proxy\n```\ndocker build -t img_name:tag --network=host --build-arg http_proxy=http://127.0.0.1:7890 .\n```\n\n## Run\n```\ndocker run --env HTTP_PROXY=\"http://127.0.0.1:7890\" --gpus all nvidia/cuda:10.2-base nvidia-smi\n```\n","n":0.224}}},{"i":82,"$":{"0":{"v":"Jupyter Notebook","n":0.707},"1":{"v":"Run docker container with port forwarding and start jupyter server\n```\ndocker run -it --gpus all --env HTTP_PROXY=\"http://127.0.0.1:7890\" --network host image:version\njupyter notebook --ip 0.0.0.0 --no-browser --allow-root\n```\n## Web browser\nOpen link shown in terminal in web browser\n\n## VSCode\n> Jupyer notebook needs to be in the same folder as in VSCode remote container\n\n1. Copy link, open vscode\n2. `Ctrl+Shift+P` choose `Remote-Container:Attach to runningcontainer`, select `image:version`\n3. `Ctrl+Shift+P` choose `Jupyter: Specify local or remote Jupyter server for connections`\n4. choose `Existing` then paste link\n5. open the notebook","n":0.113}}},{"i":83,"$":{"0":{"v":"Docker File Example","n":0.577},"1":{"v":"## No proxy\n```\nFROM nvidia/cuda:11.3.0-runtime-ubuntu20.04\n\n# Prevent stop building ubuntu at time zone selection.  \nENV DEBIAN_FRONTEND=noninteractive\n\n# Prepare and empty machine for building\nRUN apt-get update && \\\n    apt-get install -y git \\\n    build-essential \\\n    cmake \\\n    python3 \\\n    python3-pip \\\n\tlibegl1-mesa-dev \\\n    libglib2.0-0 \\\n    libsm6 \\\n    libxrender1 \\\n    libxext6\n\nRUN pip3 install PySide2 PyOpenGL\n\nRUN git clone https://github.com/PixarAnimationStudios/USD.git\n\nRUN python3 USD/build_scripts/build_usd.py /usr/local/USD\n\nENV PYTHONPATH=\"/usr/local/USD/lib/python:${PYTHONPATH}\"\n\nENV PATH=\"/usr/local/USD/bin:${PATH}\"\n```\n\n## With proxy\n```\nFROM nvidia/cuda:11.3.0-devel-ubuntu20.04\n\n# Prevent stop building ubuntu at time zone selection.  \nENV DEBIAN_FRONTEND=noninteractive\n\n# Prepare and empty machine for building\nRUN apt-get update\nRUN apt-get install -y shared-mime-info\nRUN apt-get install -y git\nRUN apt-get install -y cmake\nRUN apt-get install -y vim\nRUN apt-get install -y build-essential\nRUN apt-get install -y libboost-atomic-dev\nRUN apt-get install -y libboost-date-time-dev\nRUN apt-get install -y libboost-program-options-dev\nRUN apt-get install -y libboost-filesystem-dev\nRUN apt-get install -y libboost-graph-dev\nRUN apt-get install -y libboost-system-dev\nRUN apt-get install -y libboost-test-dev\nRUN apt-get install -y libatlas3-base\nRUN apt-get install -y libatlas-base-dev\nRUN apt-get install -y libeigen3-dev\nRUN apt-get install -y libsuitesparse-dev\nRUN apt-get install -y libfreeimage-dev\nRUN apt-get install -y libgoogle-glog-dev\nRUN apt-get install -y libgflags-dev\nRUN apt-get install -y libglew-dev\nRUN apt-get install -y qtbase5-dev\nRUN apt-get install -y libqt5opengl5-dev\nRUN apt-get install -y libcgal-dev\nRUN apt-get install -y libcgal-qt5-dev\n\n# Build and install ceres solver\nRUN apt-get -y install \\\n    libatlas-base-dev \\\n    libsuitesparse-dev\nRUN git config --global http.proxy http://127.0.0.1:7890\nRUN git clone https://github.com/ceres-solver/ceres-solver.git --branch 1.14.0\nRUN cd ceres-solver && \\\n\tmkdir build && \\\n\tcd build && \\\n\tcmake .. -DBUILD_TESTING=OFF -DBUILD_EXAMPLES=OFF && \\\n\tmake -j12 && \\\n\tmake install\n\n# Build and install COLMAP\n\n# Note: This Dockerfile has been tested using COLMAP pre-release 3.6.\n# Later versions of COLMAP (which will be automatically cloned as default) may\n# have problems using the environment described thus far. If you encounter\n# problems and want to install the tested release, then uncomment the branch\n# specification in the line below\nRUN git clone https://github.com/colmap/colmap.git\n\nRUN cd colmap && \\\n\tgit checkout dev && \\\n\tmkdir build && \\\n\tcd build && \\\n\tcmake .. && \\\n\tmake -j12 && \\\n\tmake install\n\n# Build and install Pixel-Perfect Structure-from-Motion\nRUN apt-get -y install \\\n    libhdf5-dev\nRUN git clone https://github.com/cvg/pixel-perfect-sfm --recursive\nRUN apt install -y python3-pip\n\nRUN pip3 install --proxy=http://127.0.0.1:7890 \\ \n    torch==1.10.1+cu113 \\\n    torchvision==0.11.2+cu113 \\\n    torchaudio==0.10.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n\nRUN cd pixel-perfect-sfm && \\\n    pip3 install --proxy=http://127.0.0.1:7890 -r requirements.txt\n\nRUN git clone --recursive https://github.com/cvg/Hierarchical-Localization/\nRUN pip3 install --proxy=http://127.0.0.1:7890 --upgrade pip\nRUN cd Hierarchical-Localization && \\\n    pip3 --proxy=http://127.0.0.1:7890 install -e .\n\nRUN cd pixel-perfect-sfm && \\\n    pip3 install --proxy=http://127.0.0.1:7890 -e .\n\nRUN pip3 install --proxy=http://127.0.0.1:7890 jupyter notebook\nRUN pip3 install --proxy=http://127.0.0.1:7890 ipython ipykernel\n\nRUN apt-get install -y wget\n\nRUN echo \"use_proxy = on \\nhttp_proxy =  http://127.0.0.1:7890/ \\nhttps_proxy =  http://127.0.0.1:7890/\" >> /etc/wgetrc\nWORKDIR \"/pixel-perfect-sfm\"\nCMD [\"jupyter\", \"notebook\", \"--port=8888\", \"--no-browser\", \"--ip=0.0.0.0\", \"--allow-root\"]\n```\n","n":0.051}}},{"i":84,"$":{"0":{"v":"Xdg-Open Default","n":0.707},"1":{"v":"\n`nano ~/.config/mimeapps.list`\n\nThen add\n\n```\n[Default Applications]\ninode/directory=org.gnome.Nautilus.desktop\n\n```\n","n":0.5}}},{"i":85,"$":{"0":{"v":"Wireguard","n":1},"1":{"v":"\n## Setup\nInstall `wireguard-tools` then put configuration files, e.g. `myconfig.conf`, to `/etc/wireguard/`\n\n## Start config\n```\nwg-quick up myconfig\n```\n\n## Stop config\n```\nwg-quick down myconfig\n```\n","n":0.229}}},{"i":86,"$":{"0":{"v":"Unity","n":1},"1":{"v":"\n## Gnome DPI Scaling\n>References: https://wiki.archlinux.org/title/HiDPI\nhttps://forum.unity.com/threads/unity-hub-and-hidpi.604378/\n\n- scale UI elements by an integer factor: `GDK_SCALE=2.0`\n- undo scaling of text, fractional scale can be used: `GDK_DPI_SCALE=0.5`\n\n```\nGDK_SCALE=2.0 GDK_DPI_SCALE=0.5 path/to/unity/binary -projectpath path/to/unity/project/folder\n```\n","n":0.192}}},{"i":87,"$":{"0":{"v":"Systemd","n":1},"1":{"v":"\n## Basic\n\n- Enable a unit to start automatically at boot: `systemctl enable unit`\n- Enable a unit to start automatically at boot and start it immediately: `systemctl enable --now unit`\n- Disable a unit to no longer start at boot: `systemctl disable unit`\n\n\n- Start a unit immediately: `systemctl start unit`\n- Stop a unit immediately: `systemctl stop unit`\n- Restart a unit: `systemctl restart unit`\n- Reload a unit and its configuration: `systemctl reload unit`\n\n## Custom unit\n> /etc/systemd/system/clash.service\n\n```\n[Unit]\nDescription=Clash daemon, A rule-based proxy in Go.\nAfter=network.target\n\n[Service]\nType=simple\nRestart=always\nExecStart=/usr/bin/clash -f /etc/clash/config.yaml\n\n[Install]\nWantedBy=multi-user.target\n```\n","n":0.111}}},{"i":88,"$":{"0":{"v":"Pyenv","n":1},"1":{"v":"\n## Install\n\n```\ngit clone https://github.com/pyenv/pyenv.git ~/.pyenv\n```\n\n## Setup\n> .zprofile\n\n```\nexport PYENV_ROOT=\"$HOME/.pyenv\"\nexport PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init - )\"  or eval \"$(pyenv init --path)\"\n```\n\nWARNING: [instruction](https://github.com/pyenv/pyenv) states `eval \"$(pyenv init --path)\"` but doesn't always work on ArchLinux\n\n> .zshrc\n\n```\neval \"$(pyenv init -)\"\n```\n\n## Build shared library\n```\nCONFIGURE_OPTS=--enable-shared\n```\n\n## Install 3.6 on ArchLinux\n\n```\nCFLAGS=\"-O2\"\n```\n","n":0.156}}},{"i":89,"$":{"0":{"v":"Proxy","n":1},"1":{"v":"In general, most application respect `HTTP_PROXY` or `ALL_PROXY` environment variable\n```\nexport HTTP_PROXY=http://127.0.0.1:7890\nexport ALL_PROXY=http://127.0.0.1:7890\n```\n\n## Git\n### HTTP/ HTTPS\n```\ngit config --global http.proxy http://127.0.0.1:7890\n...\ngit config --global --unset http.proxy\n```\n\n### SSH\nInstall `openbsd-netcat` then `nano .ssh/config`\n\n```\nHost github.com bitbucket.org\n    ProxyCommand            nc -X 5 -x 127.0.0.1:7891 %h %p\n```\n\n## Pacman\n```\nexport http_proxy=\"http://127.0.0.1:7890\"\nexport https_proxy=\"http://127.0.0.1:7890\"\nsudo -E pacman -Syu\n```\n\n## Pip\n```\npip install --proxy=http://127.0.0.1:7890 numpy\n```\n\n## Svn\nAdd\n```\n[global]\nhttp-proxy-host = 127.0.0.1\nhttp-proxy-port = 7890\nhttp-proxy-compression = no\n```\nto `~/.subversion/servers`\n\n## Wget\n\nAdd\n```\nuse_proxy = on\nhttp_proxy = http://127.0.0.1:7890/\nhttps_proxy = http://127.0.0.1:7890/\n```\nto `/etc/wgetrc` or `~/.wgetrc`\n\n## Snap\n```\nsudo snap set system proxy.http=\"http://127.0.0.1:7890\"\nsudo snap set system proxy.https=\"http://127.0.0.1:7890\"\n\nsnap unset system proxy.http\nsnap unset system proxy.https\n```\n\n## Conda\n\n```\nconda config --set proxy_servers.http http://127.0.0.1:7890/\nconda config --set proxy_servers.https http://127.0.0.1:7890/\n```","n":0.105}}},{"i":90,"$":{"0":{"v":"Grub","n":1},"1":{"v":"\n## Kernel parameters\n`sudo nano /etc/default/grub`\n\nThen append on\n\n`GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash\"` line\n\n## Regenerate boot menu\n```\nsudo grub-mkconfig -o /boot/grub/grub.cfg\n``` \n","n":0.25}}},{"i":91,"$":{"0":{"v":"Commandline Arguments","n":0.707},"1":{"v":"## Pass JSON string\nAdd single quotation mark `'` `'` so string won't be splitted.\n\n## Blender\n\n`blender -b -P script.py -- arg0 arg1`\n\n> script.py\n\n```\nargv = sys.argv\nargv = argv[argv.index(\"--\") + 1:]\narg0 = argv[0]\narg1 = argv[1]\n```","n":0.177}}},{"i":92,"$":{"0":{"v":"Linear Algebra","n":0.707}}},{"i":93,"$":{"0":{"v":"Vector Differentiation","n":0.707}}},{"i":94,"$":{"0":{"v":"Total Derivative","n":0.707},"1":{"v":"> Reference: https://mathworld.wolfram.com/TotalDerivative.html\n\nGiven function $f(x(t), y(t), z(y))$, its total derivative is:\n\n$$\n\\frac{\\partial f}{\\partial t} =\n\\frac{\\partial f}{\\partial x}\n\\frac{\\partial x}{\\partial t} + \n\\frac{\\partial f}{\\partial y}\n\\frac{\\partial y}{\\partial t} + \n\\frac{\\partial f}{\\partial z}\n\\frac{\\partial z}{\\partial t}\n$$\n\nTotal derivative $\\frac{\\partial f}{\\partial t}(x_0, y_0, z_0)$ can be interpreted as [[linear-algebra.vector-differentiation.directional-derivative]] evaluated at $\\begin{bmatrix} x(t_0) & y(t_0) & z(t_0) \\end{bmatrix}^T$ with the direction of $\\begin{bmatrix} x'(t_0) & y'(t_0) & z'(t_0) \\end{bmatrix}^T$\n\n","n":0.128}}},{"i":95,"$":{"0":{"v":"Taylor Series","n":0.707},"1":{"v":"\n$$\nf(x) = \\sum_{k=0}^{\\infin} \\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\n$$\n\n> It can be used to define operator such as matrix exponential and matrix logarithm\n\n\n## Taylor polynomial of degree n\nContains first $n+1$ components of the series.\n>  Used as an approximation of function, at best when $x$ is close to $x_0$. Also an exact representation of polynominal of degree $k$ if $k \\leq n$, as $f^{(i)}, i > k$ are all 0.\n\n$$\nT_n = \\sum_{k=0}^{n} \\frac{f^{(k)}(x_0)}{k!}(x-x_0)^k\n$$\n\n## Local approximation\nGradient of $f$ at $x_0$ can be used for local approximation of $f$ around $x_0$\n$$\nf(x) \\approx f(x_0) + f'(x_0)(x - x_0) + \\frac{1}{2} f''(x_0)(x - x_0)^2\n$$\n\nApplication see [[optimization.gaussian-newton-method]]\n","n":0.102}}},{"i":96,"$":{"0":{"v":"Partial Derivative (Jacobian Matrix)","n":0.5},"1":{"v":"\n\nLet $f: \\mathbb{R}^{n} \\rarr \\mathbb{R}$, $\\pmb{x} \\in \\mathbb{R}^{n}$, $f(\\pmb{x}) \\in \\mathbb{R}$:\n\n$$ \\bm{J} =\n\\nabla_{\\pmb{x}}f = \n\\begin{bmatrix}\n\\frac{\\partial f(\\pmb{x})}{\\partial x_1} & \\frac{\\partial f(\\pmb{x})}{\\partial x_2} & \\dots & \\frac{\\partial f(\\pmb{x})}{\\partial x_n} \n\\end{bmatrix}\n\\in \\mathbb{R}^{1 \\times n}\n$$\n\nLet $f: \\mathbb{R}^{n} \\rarr \\mathbb{R}^{m}$, $\\pmb{x} \\in \\mathbb{R}^{n}$, $\\pmb{f}(\\pmb{x}) \\in \\mathbb{R}^{m}$:\n\n$$\n\\bm{J} = \n\\nabla_{\\pmb{x}}\\pmb{f} = \n\\begin{bmatrix}\n\\frac{\\partial f_1(\\pmb{x})}{\\partial x_1} & \\frac{\\partial f_1(\\pmb{x})}{\\partial x_2} & \\dots & \\frac{\\partial f_1(\\pmb{x})}{\\partial x_n} \\\\\n\\frac{\\partial f_2(\\pmb{x})}{\\partial x_1} & \\frac{\\partial f_2(\\pmb{x})}{\\partial x_2} & \\dots & \\frac{\\partial f_2(\\pmb{x})}{\\partial x_n} \\\\\n\\vdots & & & \\vdots \\\\\n\\frac{\\partial f_m(\\pmb{x})}{\\partial x_1} & \\frac{\\partial f_m(\\pmb{x})}{\\partial x_2} & \\dots & \\frac{\\partial f_m(\\pmb{x})}{\\partial x_n}\n\\end{bmatrix}\n\\in \\mathbb{R}^{m \\times n}\n$$\n\n$\\bm{J}$ is known as the Jacobian Matrix","n":0.102}}},{"i":97,"$":{"0":{"v":"Hessian","n":1},"1":{"v":"\n> Reference: https://www.deeplearningbook.org/contents/numerical.html\n\n## Second order derivative\n\nSecond order derivative determines the quadratic term in function [[Local approximation|linear-algebra.vector-differentiation.taylor-series#local-approximation]], it is the measurement of curvature (convexity)\n- $f'' = 0$ : Flat\n- $f'' > 0$ : Convex\n- $f'' < 0$ : Concave\n- $\\|f''\\|$ : Determine curvature\n\n## Matrix form definition\n\nLet $f: \\mathbb{R}^{n} \\rarr \\mathbb{R}$, $\\pmb{x} \\in \\mathbb{R}^{n}$, $f(\\pmb{x}) \\in \\mathbb{R}$:\n\n$$\n\\bm{H} =\n\\bm{J}(\\nabla_{\\bm{x}} f)^T =\n\\begin{bmatrix}\n\\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_1 x_n} \\\\\n\\frac{\\partial^2 f}{\\partial x_2 x_1} & \\frac{\\partial^2 f}{\\partial x_2^2} & \\dots & \\frac{\\partial^2 f}{\\partial x_2 x_n} \\\\\n\\vdots &&& \\vdots \\\\\n\\frac{\\partial^2 f}{\\partial x_n x_1} & \\frac{\\partial^2 f}{\\partial x_n x_2} & \\dots & \\frac{\\partial^2 f}{\\partial x_n^2}\n\\end{bmatrix} \\in \\mathbb{R}^{n \\times n}\n$$\n\n\n> Hessian is Jacobian of Jacobian, **NOT** the dot product of Jacobian\n\nLet $f: \\mathbb{R}^{n} \\rarr \\mathbb{R}^{m}$, $\\pmb{x} \\in \\mathbb{R}^{n}$, $\\pmb{f}(\\pmb{x}) \\in \\mathbb{R}^{m}$\n\n$$\\bm{H} \\in \\mathbb{R}^{m \\times n \\times n}$$\n\n## Directional derivative view (bilinear form)\n\nConsider $f: \\mathbb{R}^{n} \\rarr \\mathbb{R}$, $\\pmb{x} \\in \\mathbb{R}^{n}$, $f(\\pmb{x}) \\in \\mathbb{R}$, $\\bm{H} \\in \\mathbb{R}^{n \\times n}$\n\n$\\bm{H}$ is symmetric, a.k.a. $\\bm{H}_{ij} = \\bm{H}_{ji}$, wherever the second order partial derivatives are continuous. Thus, it can be decomposed into orthogonal eigen basis.\n\n$$\n\\bm{H} = \\bm{Q} \\bm{\\Lambda} \\bm{Q}^T \\\\\n\n\\bm{\\Lambda} = \\bm{Q}^T \\bm{H} \\bm{Q}\n$$\n\n> More see [[linear-algebra.decomposition.eigendecomposition]] and [[Eigenbasis|linear-algebra.eigenvector-eigenvalue#eigenbasis]]\n\nThe second order [[linear-algebra.vector-differentiation.directional-derivative]] of direction vector $\\bm{d}$ is given by\n\n$$\n\\bm{d}^T \\bm{H} \\bm{d}\n$$\n\nIf $\\bm{d}$ is eigenvector, the result is its corresponding eigenvalue.\n\n## Convexity\n\nIf $H$ is [[Symmetric, Positive definite|linear-algebra.inner-product#symmetric-positive-definite]], then $f$ is strictly convex (single minuma)","n":0.066}}},{"i":98,"$":{"0":{"v":"Directional Derivative","n":0.707},"1":{"v":"\n> Reference: https://www.deeplearningbook.org/contents/numerical.html\n\nRate of change of function value in the direction of $\\bm{u}$. \n\n$$\n\\nabla _{\\bm{u}} f = \\nabla_{\\bm{x}} f \\cdot \\frac{\\bm{u}}{\\|\\bm{u}\\|}\n$$\n\nIt is the derivative of $f(\\bm{x}+ \\alpha\\bm{u})$ w.r.t. $\\alpha$, evaluated at $\\alpha$.\n\nLet $\\bm{y} = \\bm{x} + \\alpha \\bm{u}$, with chain rule\n\n$$\n\\frac{\\partial f}{\\partial \\alpha} = \\frac{\\partial f}{\\partial y} \\frac{\\partial y}{\\partial \\alpha} = \\nabla_{\\bm{x}} f \\cdot \\bm{u}\n$$\n","n":0.135}}},{"i":99,"$":{"0":{"v":"Differentiation Rules","n":0.707},"1":{"v":"\n> $\\circ$ means composition of function\n\n$$\n\\begin{aligned}\n&\\text{Product Rule:} \\ \\frac{\\partial}{\\partial } (fg)' = f'g + fg'\n\\\\\n&\\text{Quotient Rule:} \\ (\\frac{f}{g})' = \\frac{f'g - fg'}{g^2}\n\\\\\n&\\text{Sum Rule:} \\ (f + g)' = f' + g'\n\\\\\n&\\text{Chain Rule:} \\ (g(f))' = (g \\circ f)' = g'(f)f'\n\\end{aligned}\n$$\n","n":0.158}}},{"i":100,"$":{"0":{"v":"Differentiation Formulas","n":0.707},"1":{"v":"\n$$\n\\newcommand{\\x}{\\pmb{x}}\n\\newcommand{\\s}{\\pmb{s}}\n\\newcommand{\\a}{\\pmb{a}}\n\\newcommand{\\b}{\\pmb{b}}\n\\newcommand{\\A}{\\pmb{A}}\n\\newcommand{\\W}{\\pmb{W}}\n\\newcommand{\\X}{\\pmb{X}}\n\\newcommand{\\fX}{f(\\pmb{X})}\n\\newcommand{\\finvX}{f^{-1}(\\pmb{X})}\n\\newcommand{\\d}{\\partial}\n\n\\begin{aligned}\n&\\frac{\\d}{\\d\\X}\\fX^T = (\\frac{\\d}{\\d \\X}\\fX)^T \\\\\n&\\frac{\\d}{\\d\\X}tr(\\fX) = tr(\\frac{\\d}{\\d \\X}\\fX) \\\\\n&\\frac{\\d}{\\d\\X}det(\\fX) = tr(adj(\\fX)\\frac{\\d}{\\d \\X} \\fX) = det(\\fX)tr(\\finvX\\frac{\\d}{\\d \\X} \\fX) \\\\\n&\\frac{\\d}{\\d\\X}\\finvX = - \\finvX \\frac{\\d}{\\d \\X} \\fX \\finvX \\\\\n&\\frac{\\d}{\\d\\X} \\a^T \\X \\b = -(\\X^{-1})^T\\a\\b^T(\\X^{-1})^T \\\\\n&\\frac{\\d}{\\d\\x}\\x^T \\a = \\a^T \\\\\n&\\frac{\\d}{\\d\\x}\\a^T \\x = \\a^T \\\\\n&\\frac{\\d}{\\d\\x}\\x^T \\A \\x = 2 \\x^T \\A \\\\\n&\\frac{\\d}{\\d\\s}(\\x - \\A\\s)^T\\W(\\x - \\A\\s) = -2(\\x - \\A\\s)^T \\W \\A \\\\\n\\end{aligned}\n$$\n","n":0.131}}},{"i":101,"$":{"0":{"v":"Feature Extraction","n":0.707}}},{"i":102,"$":{"0":{"v":"Principle Component Analysis","n":0.577},"1":{"v":"\n\nGiven $\\bm{X} = \\begin{bmatrix} \\bm{x}_0, \\dots, \\bm{x}_n \\end{bmatrix} \\in \\mathbb{R}^{f \\times n}$, $n \\ll f$, find a set of basis $\\bm{W} = \\begin{bmatrix} \\bm{w}_1, \\dots, \\bm{w}_k \\end{bmatrix}^T \\in \\mathbb{R}^{f \\times d}$, such that variance of the linear projection of $\\bm{x}_i$ on $\\bm{w}$ is maximized. See [[Dot product as projection|linear-algebra.dot-product#geometry-view]]\n\n## Brief Derivation\n\n$$\n\\bm{y}_i = \\bm{W}^T \\bm{x}_i \\\\\n\n\\bm{y}_i - \\mu_{\\bm{y}} = \\bm{W}^T \\bm{x}_i - \\bm{W}^T \\bm{\\mu}_{\\bm{x}} = \\bm{W}^T \\overline{\\bm{x}}_i\\\\\n\n\\sigma_{\\bm{Y}} = \\frac{1}{n} \\sum_{i=1}^{n} (\\bm{y}_i - \\mu_{\\bm{y}})(\\bm{y}_i - \\mu_{\\bm{y}})^T =  \\frac{1}{n} \\sum_{i=1}^{n} \\bm{W}^T \\overline{\\bm{x}}_i \\overline{\\bm{x}}_i^T \\bm{W} = tr(\\bm{W}^T \\bm{S}_t \\bm{W}) \\\\\n$$\nSo for objective function:\n$$\n\\begin{aligned}\n\\bm{W}_o &= \\argmax_{\\bm{W}} \\sigma_{\\bm{y}} \\\\\n&= \\argmax_{\\bm{W}} tr(\\bm{W}^T \\bm{S}_t \\bm{W})\n\\end{aligned}\n$$\nWe incorperate with constraint $\\bm{W}^T \\bm{W} = \\bm{I}$ and fomulate with Lagrangian:\n$$\nL(\\bm{W}, \\bm{\\Lambda}) = tr(\\bm{W}^T \\bm{S}_t \\bm{W}) - tr(\\bm{\\Lambda}(\\bm{W}^T \\bm{W} - \\bm{I}))\n$$\n\nCompute its derivative over $\\bm{W}$ and set it to 0. See: [[linear-algebra.vector-differentiation.differentiation-formulas]]. Note $\\bm{S}_t$ is positive semi-definte matrix (non-negative eigenvalues), $\\bm{\\Lambda}$ is diagonal matrix.\n\n$$\n\\frac{\\partial L}{\\partial \\bm{W}} = tr(2\\bm{W}^T \\bm{S}_t) - tr(2 \\bm{\\Lambda} \\bm{W}^T) = 0 \\\\\n\n\\bm{S}_t \\bm{W} = \\bm{W} \\bm{\\Lambda} \\\\\n$$\n\n$\\bm{S}_t \\bm{w}_k = \\lambda_k \\bm{w}_k$ for $i=1,\\dots,k$. Thus, $\\bm{W}$ column-wise are $d$ eignvectors of $\\bm{S}_t$. \n\nFrom [[linear-algebra.decomposition.eigendecomposition]], we have $\\bm{S}_t = \\bm{U} \\bm{\\Lambda} \\bm{U}^T$, $\\bm{U}_d = \\bm{W}$. Go back to objective function\n\n$$\ntr(\\bm{W}^T \\bm{S}_t \\bm{W}) = tr(\\bm{W}^T \\bm{U} \\bm{\\Lambda} \\bm{U}^T \\bm{W}) = tr(\\bm{\\Lambda}_d) = \\sum_{k=1}^{d} \\lambda_k\n$$\n\nWe choose $d$ eignvectors that corresponds to $d$ largest eigenvalues\n\n## Computation\n\nFor $\\bm{X} \\in \\mathbb{R}^{f \\times n}$, $n \\ll f$, $\\bm{X}\\bm{X}^T \\in \\mathbb{R}^{f \\times f}$ is more computational expensive to compute than $\\bm{X}^T \\bm{X} \\in \\mathbb{R}^{n \\times n}$.\n\nSince they [[share the same eigenvalues|linear-algebra.decomposition.singular-value-decomposition#proof-bmx-bmxt-and-bmxt-bmx-share-the-same-bmlambda]], we eigendecompose $\\bm{X}^T \\bm{X} = \\bm{V} \\bm{\\Lambda} \\bm{V}^T$ and compute $\\bm{U} = \\bm{X} \\bm{V} \\bm{\\Lambda} ^ {-\\frac{1}{2}}$. See [[proof|linear-algebra.decomposition.singular-value-decomposition#proof-bmu_k--bmx-bmv_k-bmlambda---frac12]].\n\nWe pick $d$ eigenvectors (columns of $\\bm{U}$) and projected feature\n$$\n\\bm{Y} = \\bm{U}_d^T \\bm{X}\n$$\n\nNote the covariance of projected feature\n$$\n\\bm{Y} \\bm{Y}^T = \\bm{U}_d^T \\bm{X} \\bm{X}^T \\bm{U}_d = \\bm{\\Lambda}_d\n$$\nwhich means projected features are uncorrelated\n\n### Whiten\nIf we wish $\\bm{Y} \\bm{Y}^T = \\bm{I}$\n$$\n\\bm{W} = \\bm{U}_d \\bm{\\Lambda}_d^{-\\frac{1}{2}}\n$$\n\n## Kernel Trick\nInstead of explicitly define $\\phi(.)$, define it implicitly using similarity function via [[linear-algebra.inner-product]]\n\n$$\nk(\\bm{x}_i, \\bm{x}_j) = \\langle \\phi(\\bm{x}_i), \\phi(\\bm{x}_j) \\rangle\n$$\n\n### PCA example\nSuppose $f$ is very large (e.g. infinity) that is intractable, but $\\bm{X}^T \\bm{X}$ is solvable\n$$\n\\bm{X}^T \\bm{X} = \n\\begin{bmatrix} \nk(x_1, x_1) & \\dots & k(x_1, x_n) \\\\\n\\vdots & \\ddots & \\\\\nk(x_n, x_1) &        & k(x_n, x_n) \n\\end{bmatrix}\n\\in \\mathbb{R}^{n \\times n}\n$$\nWe can still compute PCA\n\n$$\n\\bm{X}^T \\bm{X} = \\bm{V} \\bm{\\Lambda} \\bm{V}^T\n$$\n\n$$\n\\bm{Y} = \\bm{U}_d^T \\bm{X} = (\\bm{X} \\bm{V} \\bm{\\Lambda} ^ {-\\frac{1}{2}})^T \\bm{X} = \\bm{V}^T \\bm{\\Lambda} ^ {-\\frac{1}{2}} (\\bm{X}^T \\bm{X})\n$$\n\nDuring inference, for test sample $\\bm{x}_t \\in \\mathbb{R}^{f \\times 1}$\n\n$$\ny_t = \\bm{V}^T \\bm{\\Lambda} ^ {-\\frac{1}{2}} (\\bm{X}^T \\bm{x}_t)\n$$\n\nwhere\n\n$$\n\\bm{X}^T \\bm{x}_t = \n\\begin{bmatrix}\nk(x_1, x_t) \\\\\n\\vdots \\\\\nk(x_n, x_t)\n\\end{bmatrix}\n$$\n\n","n":0.049}}},{"i":103,"$":{"0":{"v":"Data Centering","n":0.707},"1":{"v":"\n> Reference: https://mml-book.github.io/book/mml-book.pdf\n\nGoal: center data by subtracting mean\n\n\nGiven $\\bm{X} = \\begin{bmatrix} \\bm{x}_0, \\dots, \\bm{x}_n \\end{bmatrix} \\in \\mathbb{R}^{f \\times n}$, $n \\ll f$\n\n### Vector form\n See [[Expected value and covariance for dataset|probability.expected-value-and-covariance#for-dataset]]\n \n### Matrix form\n\nNote we assume $\\bm{X} \\in \\mathbb{R}^{f \\times n}$\n\n#### Mean\n\n$\\bm{1}$ vector can be used to sum of axis\n\n$$\n\\bm{m} = \\frac{1}{n} \\bm{X} \\bm{1}_n \\in \\mathbb{R}^f\\\\\n\n\\bm{M} = (\\frac{1}{n}\\bm{X} \\bm{1}_n) \\bm{1}_n^T \\in \\mathbb{R}^{f \\times n} \\\\\n\n\\overline{\\bm{X}} = \\bm{X} - \\bm{M} = \\bm{X}(\\bm{I} - \\frac{1}{n}\\bm{1}_n\\bm{1}_n^T)\n$$\n\n## Covariance\n$$\n\\bm{S}_t = \\frac{1}{n} \\overline{\\bm{X}} \\overline{\\bm{X}}^T\n$$\n","n":0.113}}},{"i":104,"$":{"0":{"v":"Decomposition","n":1}}},{"i":105,"$":{"0":{"v":"Singular Value Decomposition (SVD)","n":0.5},"1":{"v":"\n> Reference: https://mml-book.github.io/book/mml-book.pdf\n\n\nFor matrix $\\bm{A} \\in \\mathbb{R}^{m \\times n}$\n$$\n\\bm{A} = \\bm{U} \\bm{\\Sigma} \\bm{V}^T\n$$\nwhere $\\bm{U} \\in \\mathbb{R}^{m \\times m}$, $\\bm{V} \\in \\mathbb{R}^{n \\times n}$ are orthogonal basis, $\\bm{\\Sigma} = diag\\{\\sigma_1, \\sigma_2, \\dots, \\sigma_p\\} \\in \\mathbb{R}^{m \\times n}$, $p = \\min\\{m, n\\}$. $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_p \\geq 0$ are singular values of $\\bm{A}$\n\nDefine r as $\\sigma_1 \\geq \\sigma_2 \\geq \\dots \\geq \\sigma_r \\geq \\sigma_{r+1} = \\dots = \\sigma_p = 0$, $rank(\\bm{A}) = r$\n\n$$\n\\bm{A} = \\sum_{i=1}^{r} \\sigma_i \\bm{u}_i \\bm{v}_i^T\n$$\n\nFull rank means $r = p$\n\n## Thin SVD\n\nIf $m \\geq n$\n$$\n\\bm{A} = \\bm{U}_1 \\bm{\\Sigma}_1 \\bm{V}^T\n$$\nwhere $\\bm{U}_1 \\in \\mathbb{R}^{m \\times n}$, $\\bm{\\Sigma}_1 \\in \\mathbb{R}^{n \\times n}$. Note that the basis are no longer orthogonal $$\\bm{U}_1^T \\bm{U}_1 = \\bm{I} \\in \\mathbb{R}^{n \\times n}$$, $\\bm{U}_1 \\bm{U}_1^T \\neq \\bm{I} \\in \\mathbb{R}^{m \\times m}$\n\n## Feature vector\n\nGiven $\\bm{X} \\in \\mathbb{R}^{f \\times n}$, $n \\ll f$\n\nLet $rank(\\bm{X}) = k \\leq \\min(f, n) = n$\n\n$$\n\\bm{X} \\bm{X}^T = \\bm{U} \\bm{\\Sigma} \\bm{V}^T \\bm{V} \\bm{\\Sigma} \\bm{U}^T = \\bm{U} \\bm{\\Sigma}^2 \\bm{U}^T = \\bm{U}_k \\bm{\\Sigma}_k^2 \\bm{U}_k^T \\in \\mathbb{R}^{k \\times k}\n\\\\\n\\bm{X}^T \\bm{X} = \\bm{V} \\bm{\\Sigma} \\bm{U}^T \\bm{U} \\bm{\\Sigma} \\bm{V}^T = \\bm{V} \\bm{\\Sigma}^2 \\bm{V}^T = \\bm{V}_k \\bm{\\Sigma}_k^2 \\bm{V}_k^T \\in \\mathbb{R}^{k \\times k}\n$$\n\nIt is equivalent to Eigen decompostion, where $\\bm{\\Lambda} = \\bm{\\Sigma}_k^2$, $\\bm{U}_k \\in \\mathbb{R}^{f \\times k}$, $\\bm{V}_k \\in \\mathbb{R}^{n \\times k}$\n\n$$\n\\bm{X} \\bm{X}^T = \\bm{U}_k \\bm{\\Lambda} \\bm{U}_k^T \\\\\n\\bm{X}^T \\bm{X} = \\bm{V}_k \\bm{\\Lambda} \\bm{V}_k^T\n$$\n\n### Proof $\\bm{X} \\bm{X}^T$ and $\\bm{X}^T \\bm{X}$ share the same $\\bm{\\Lambda}$\n\nLet $\\bm{\\Lambda} = \\bm{\\Sigma}_k^2$\n\nAssume $\\bm{v}$ is any eigenvector of $\\bm{X}^T \\bm{X}$ with non-zero eigenvalue $\\lambda$\n\n$$\n\\bm{X}^T \\bm{X} \\bm{v} = \\lambda \\bm{v} \\\\\n\\bm{X} \\bm{X}^T \\bm{X} \\bm{v} = \\lambda \\bm{X} \\bm{v} \\\\\n(\\bm{X} \\bm{X}^T) \\bm{X} \\bm{v} = \\lambda (\\bm{X} \\bm{v}) \\\\\n\\therefore \\bm{X} \\bm{v} \\text{ is eigenvector of }\\bm{X} \\bm{X}^T \\text{ corresponds to same eigenvalue } \\lambda \\\\\n\\therefore \\bm{X}^T \\bm{X}, \\bm{X} \\bm{X}^T \\text{ have the same } \\bm{\\Lambda}\n$$\n\n### Proof $\\bm{U}_k = \\bm{X} \\bm{V}_k \\bm{\\Lambda} ^ {-\\frac{1}{2}}$\n\n$$\n\\bm{X} \\bm{X}^T \\bm{X} \\bm{X}^T =\\bm{X} (\\bm{X}^T \\bm{X}) \\bm{X}^T = \\bm{X} \\bm{V}_k \\bm{\\Lambda} \\bm{V}_k^T \\bm{X}^T = (\\bm{X} \\bm{V}_k \\bm{\\Lambda}^{\\frac{1}{2}})(\\bm{X} \\bm{V}_k \\bm{\\Lambda}^{\\frac{1}{2}})^T \\\\\n\n\\bm{X} \\bm{X}^T \\bm{X} \\bm{X}^T = (\\bm{X} \\bm{X}^T) (\\bm{X} \\bm{X}^T) = \\bm{U}_k \\bm{\\Lambda} \\bm{U}_k^T \\bm{U}_k \\bm{\\Lambda} \\bm{U}_k^T = \\bm{U}_k \\bm{\\Lambda}^2 \\bm{U}_k^T = (\\bm{U}_k \\bm{\\Lambda})(\\bm{U}_k \\bm{\\Lambda})^T \\\\\n\n\\therefore \\bm{X} \\bm{V}_k \\bm{\\Lambda}^{\\frac{1}{2}} = \\bm{U}_k \\bm{\\Lambda} \\\\\n\\therefore \\bm{U}_k = \\bm{X} \\bm{V}_k \\bm{\\Lambda} ^ {-\\frac{1}{2}}\n$$","n":0.053}}},{"i":106,"$":{"0":{"v":"QR Decomposition","n":0.707},"1":{"v":"\n> Reference: https://mml-book.github.io/book/mml-book.pdf\n\n\nFor any real matrix $\\bm{A} \\in \\mathbb{R}^{n \\times n}$ can be decomposed into\n$$\n\\bm{A} = \\bm{Q} \\bm{R}\n$$\nwhere $\\bm{Q}$ is orthogonal matrix, $\\bm{R}$ is upper triangle matrix\n\n### Linear system\n$$\n\\bm{A} \\bm{x} = \\bm{b} \\\\\n\\bm{Q} \\bm{R} \\bm{x} = \\bm{b}\n$$\nSuch that\n$$\n\\bm{R} \\bm{x} = \\bm{Q} ^ T \\bm{b}\n$$\nwhich is easier to solve as $\\bm{R}$ is upper triangle matrix\n\n### Eigenvalue computation\nFor matrix $\\bm{A}$, create series of $\\bm{A}_k$, start with $k=0$, $\\bm{A}_0 = \\bm{A}$\n$$\n\\bm{A}_k = \\bm{Q}_k \\bm{R}_k \\\\\n\\bm{A}_{k+1} = \\bm{R}_k \\bm{Q}_k\n$$\nSo that\n$$\n\\bm{A}_{k+1} = \\bm{R}_k \\bm{Q}_k = \\bm{Q}_k^T \\bm{Q}_k \\bm{R}_k \\bm{Q}_k = \\bm{Q}_k^T \\bm{A}_k \\bm{Q}_k\n$$\n$\\bm{A}_{k+1}$ converge to a triangular matrix with diagonal containing eigenvalues","n":0.102}}},{"i":107,"$":{"0":{"v":"Eigendecomposition","n":1},"1":{"v":"\n> Reference: https://mml-book.github.io/book/mml-book.pdf\n\n## General\nFor square matrix $\\bm{A} \\in \\mathbb{R}^{n \\times n}$ with $n$ linear independent eigenvectors $\\bm{q}_i$ and n eigenvalues $\\lambda_i$\n\n$$\n\\bm{A} = \\bm{Q} \\bm{\\Lambda} \\bm{Q}^{-1}\n$$\n\nwhere $\\bm{Q} = \\begin{bmatrix} \n\\bm{q}_1 & \\bm{q}_2 & \\dots & \\bm{q}_n \n\\end{bmatrix}$, $\\bm{\\Lambda} = diag \\{ \\lambda_1, \\lambda_2,\\dots, \\lambda_n \\}$, $tr(\\bm{\\Lambda}) = \\sum_{i=1}^{n} \\lambda_i$\n\n## Symmetry\n\n> A.k.a eigenanalysis\n\nIf $\\bm{A}$ is symmetric, $\\bm{q}_i$ are orthonormal, $\\bm{Q}$ is orthogonal ($\\bm{Q}^{-1} = \\bm{Q}^{T}$, $\\bm{Q}\\bm{Q}^{T} = \\bm{Q} \\bm{Q}^{T} = \\bm{I}$)\n$$\n\\bm{A} = \\bm{Q} \\bm{\\Lambda} \\bm{Q}^{T} = \\sum_{i=1}^{n} \\lambda_i \\bm{q}_i \\bm{q}_i^T\n$$\n\nIf  $\\bm{A}$ is singular, $rank(\\bm{A}) = r < n$\n\n$$\n\\bm{A} = \\bm{Q}_r \\bm{\\Lambda}_r \\bm{Q}_r^{T}\n$$\n\nwhere $$\\bm{Q}_r \\in \\mathbb{R} ^ {n \\times r}$$ is the matrix of the r eigenvectors with nonzero eigenvalues, $\\bm{\\Lambda}_r$ is diagonal matrix with corresponding nonzero eigenvalues. Note $$\\bm{Q}_r^T \\bm{Q}_r = \\bm{I} \\in \\mathbb{R}^{r \\times r}$$, $\\bm{Q}_r \\bm{Q}_r^T \\neq \\bm{I} \\in \\mathbb{R}^{n \\times n}$, hence $$\\bm{Q}_r$$ is no longer orthogonal","n":0.085}}},{"i":108,"$":{"0":{"v":"Matrix Vector Dot Product","n":0.5},"1":{"v":"\nFor a vector $\\bm{v} = \\begin{bmatrix} m \\\\ n \\end{bmatrix} = m \\bm{i} + n \\bm{j}$\n\n$$\n\\bm{A} =\n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n$$\n\n$$\n\\bm{A} \\bm{v} = \n\\begin{bmatrix}\na & b \\\\\nc & d\n\\end{bmatrix}\n\\begin{bmatrix}\nm \\\\ \nn \n\\end{bmatrix}\n=\nm\n\\begin{bmatrix}\na \\\\ \nc \n\\end{bmatrix}\n+\nn\n\\begin{bmatrix}\nb \\\\ \nd \n\\end{bmatrix}\n$$\n\n## Change of basis\n\nThe transformation $\\bm{A}$ of $\\bm{v}$ can be viewed as transformation of basis from $<\\bm{i}, \\bm{j}>$ to $<\\begin{bmatrix} a \\\\ c \\end{bmatrix}, \\begin{bmatrix} b \\\\ d \\end{bmatrix}>$ with linear conbination $(m, n)$ perserved. \n\n## Change of coordinate system\n\n> **IMPORTANT** The result of $\\bm{A} \\bm{v}$ can also be viewed as how the $\\bm{v}' = \\begin{bmatrix} m \\\\ n \\end{bmatrix}$ in the coordinate system $<\\begin{bmatrix} a \\\\ c \\end{bmatrix}, \\begin{bmatrix} b \\\\ d \\end{bmatrix}>$ seen in coordinate system $<\\bm{i}, \\bm{j}>$.\n\nIn general, suppose matrix $\\bm{A} \\in C$ was formed by the basis of $C'$ in $C$ coordinate system, assume $\\bm{v} \\in C$ and $\\bm{v}' \\in C'$ represent the same vector in different coordinate system\n\n$$\n\\bm{A} \\bm{v}' = \\bm{v}\n$$\n\n$\\bm{A}$ transfrom $\\bm{v}'$ to the correct interpretation in $C$\n\n$$\n\\bm{A}^{-1} \\bm{v} = \\bm{v}'\n$$\n\n$\\bm{A}^{-1}$ transfrom $\\bm{v}$ to the correct interpretation in $C'$\n\n## Symmetric matrix\nIf $A$ is symmetric matrix\n$$\n(\\bm{A} \\cdot \\bm{b})^T = \\bm{b}^T \\bm{A}^T = \\bm{b}^T \\bm{A}\n$$","n":0.073}}},{"i":109,"$":{"0":{"v":"Matrix Matrix Dot Product","n":0.5},"1":{"v":"\n## Change of basis\n\n$$\n\\bm{B} =\n\\begin{bmatrix}\ne & f \\\\\ng & h\n\\end{bmatrix}\n$$\n\n$\\bm{A} \\bm{B}$ can be viewed as transform of basis of $\\bm{B}$ into new basis of $\\bm{A} \\bm{B}$\n\n$$\n\\bm{A} \\bm{B} =\n\\begin{bmatrix}\n\ne\n\\begin{bmatrix}\na \\\\ \nc \n\\end{bmatrix}\n+\ng\n\\begin{bmatrix}\nb \\\\ \nd \n\\end{bmatrix}\n\n&\nf\n\\begin{bmatrix}\na \\\\ \nc \n\\end{bmatrix}\n+\nh\n\\begin{bmatrix}\nb \\\\ \nd \n\\end{bmatrix}\n\n\\end{bmatrix}\n$$\n\n> **IMPORTANT** The coordinates of new basis defined by $\\bm{AB}$ is still in original coordinate system\n\n## Change of coordinate system\n\n$$\n\\bm{B} = \\bm{A}^{-1} \\bm{M} \\bm{A} \\\\\n\n\\begin{aligned}\n\\\\\n&\\bm{A}: \\text{transformation of basis matrix } C \\rarr C'\\\\\n&\\bm{M}: \\text{transformation } \\in C \\\\\n&\\bm{A}^{-1}: \\text{inverse transformation of basis matrix } C' \\rarr C \\\\\n&\\bm{B}: \\text{equivalent transformation of } \\bm{M} \\in C'\n\\end{aligned}\n$$\n\n$\\bm{M}$ is transformation you see it, $\\bm{A}$ represents shift in perspective, $\\bm{A}^{-1} \\bm{M} \\bm{A}$ represents same transformation someelse sees it\n","n":0.094}}},{"i":110,"$":{"0":{"v":"Inner Product","n":0.707},"1":{"v":"> Reference: https://mml-book.github.io/book/mml-book.pdf\n\n## Definition\n\nLet $V$ be a vector space, the inner product on $V$ is positive definite, symmetrix bilinear mapping to real number\n$$\n\\langle.,.\\rangle: V \\times V \\to \\mathbb{R}\n$$\n\n### Symmetric\n$$\n\\forall \\bm{x}, \\bm{y} \\in V:\n\\Omega(\\bm{x}, \\bm{y}) = \\Omega(\\bm{y}, \\bm{x})\n$$\n\n### Positive definite\n$$\n\\forall \\bm{x} \\in V \\backslash \\{\\bm{0}\\} : \\Omega(\\bm{x}, \\bm{x}) > 0 \\\\\n\\Omega(\\bm{0}, \\bm{0}) = 0\n$$\n\n### Bilinear\nLinear mapping for two arguments\n$$\n\\Omega(\\lambda \\bm{x} + \\phi \\bm{y}, \\bm{z}) = \\lambda \\Omega(\\bm{x}, \\bm{z}) + \\phi \\Omega(\\bm{y}, \\bm{z}) \\\\\n\\Omega(\\bm{x}, \\lambda \\bm{y} + \\phi \\bm{z}) = \\lambda \\Omega(\\bm{x}, \\bm{y}) + \\phi \\Omega(\\bm{x}, \\bm{z})\n$$\n\n## Symmetric, Positive definite matrix\n$\\forall \\bm{x}, \\bm{y} \\in V$ can be written as linear combination of basis vectors $B = (b_1, \\dots b_n)$. \n\n$$\\bm{x} = \\sum_{i=1}^{n} \\phi_i \\bm{b}_i \\\\\n\\bm{y} = \\sum_{i=1}^{n} \\lambda_i \\bm{b}_i$$\n\nThus the inner product\n\n$$\n\\langle\\bm{x},\\bm{y}\\rangle = \\sum_{i=1}^{n} \\sum_{i=1}^{n} \\phi_i \\langle\\bm{b}_i,\\bm{b}_j\\rangle \\lambda_i = \\hat{\\bm{x}}^T \\bm{A} \\hat{\\bm{y}}^T\n$$\n\nBy definition $\\bm{A}$ is symmetric\n\n### Symmetric, Positive definite\n$$\n\\forall \\bm{x} \\in V \\backslash \\{\\bm{0}\\}: \\bm{x}^T \\bm{A} \\bm{x} >0\n$$\n\nAll of its eigenvalue are positive\n\nIts determinant is strictly positive\n\n### Symmetric, Positive semidefinite\n\n$$\n\\forall \\bm{x} \\in V \\backslash \\{\\bm{0}\\}: \\bm{x}^T \\bm{A} \\bm{x} \\geq0\n$$\n\nNon-negative eigenvalue\n\n## Length\nThe length of vector $\\bm{x}$\n\n$$\n\\| \\bm{x} \\| := \\sqrt{\\langle \\bm{x}, \\bm{x} \\rangle}\n$$\n\nIt satisfies Cauchy-Schwarz inequality\n$$\n|\\langle \\bm{x}, \\bm{y} \\rangle| \\leq \\| \\bm{x} \\| \\| \\bm{y} \\|\n$$\n\n## Integration form\n> Reference: https://math.stackexchange.com/questions/1143886/the-integral-form-of-inner-product\n\nInner product of continuous function f, g on $x \\in \\Omega$\n$$\n\\langle f, g \\rangle = \\int_\\Omega f(x) g(x) dx\n$$","n":0.068}}},{"i":111,"$":{"0":{"v":"Eigenvector Eigenvalue","n":0.707},"1":{"v":"\nFor non-zero $\\bm{v}$ that satisfy\n$$\n\\bm{A} \\bm{v} = \\lambda \\bm{v} = \\lambda \\bm{I} \\bm{v} \\\\\n$$\nSo\n$$\n(\\bm{A} - \\lambda \\bm{I}) \\bm{v} = \\bm{0}\n\\\\\n\\det{(\\bm{A} - \\lambda \\bm{I})} = 0\n$$\n\n$\\bm{v}$ is a eigenvector of $\\bm{A}$ with corresponding eigenvalue $\\lambda$. Note eigenvectors only scales during transformation\n\n\n## Eigenvalue properties\n\n**Complex eigenvalues**: correspond to rotation in transformation, no eigenvectors\n\n**Eigenvalue euqal to 1**: fix in place during transformation\n\n## Eigenbasis\nChoose a set of eigenvectors that span whole space, use eigenvectors as basis vector\n\n$$\n\\bm{Q}^{-1} \\bm{A} \\bm{Q} = \\bm{\\Lambda} \\\\\n\\bm{A} = \\bm{Q} \\bm{\\Lambda} \\bm{Q}^{-1}  \n$$\n\n> Eigen basis $T_{ne}$ that transform eigen coordinate system into normal coordinate system\n\nSee: [[linear-algebra.decomposition.eigendecomposition]]\n\nTransformation applied on eigen basis is guaranteed to be diagonal\n\nTo compute chain multiplication of same matrix n times, its easier to change it to eigenbasis, transform n times then change it back. See [[Change of basis|linear-algebra.matrix-vector-dot-product#change-of-basis]]\n\n$$\n\\bm{A}^n = \\bm{Q} \\bm{\\Lambda}^n \\bm{Q}^{-1}\n$$\n\n## Relation to rank\n\n> Reference: https://math.stackexchange.com/questions/1349907/what-is-the-relation-between-rank-of-a-matrix-its-eigenvalues-and-eigenvectors\n\nFor square matrix $\\bm{A} \\in \\mathbb{R}^{n \\times n}$, $n = \\text{rank} + \\text{nullity}$, kernel of $\\bm{A}$ is the eigenspace with 0 eigenvalue\n","n":0.079}}},{"i":112,"$":{"0":{"v":"Dot Product","n":0.707},"1":{"v":"Given \n$$\n\\bm{a} = a_1 \\bm{i} + a_2 \\bm{j} + a_3 \\bm{k} \\\\\n\\bm{b} = b_1 \\bm{i} + b_2 \\bm{j} + b_3 \\bm{k}\n$$\nwhere ($\\bm{i}$, $\\bm{j}$, $\\bm{k}$) are orthonormal basis\n\n\n## Algebra View\n$$\n\\bm{a} \\cdot \\bm{b} = \n\n\\begin{bmatrix}\na_1 & a_2 & a_3\n\\end{bmatrix}\n\n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ b_3\n\\end{bmatrix}\n\n\n= a_1 b_1 + a_2 b_2 + a_3 b_3\n$$\n\nRow vector ($1 \\times n$ matrix) as linear transformation of vector to number, it projects each basis vector to number line\n\n$$\n\\bm{i} \\rarr a_1 \\\\\n\\bm{j} \\rarr a_2 \\\\\n\\bm{k} \\rarr a_3\n$$\n\n\n## Geometry view\n\n$$\n\\begin{aligned}\n\n\\bm{a} \\cdot \\bm{b} &= \\| \\text{projection of } \\bm{a}\\text{ on }\\bm{b} \\| \\cdot \\|\\bm{b}\\| = \\| \\bm{a} \\| \\cos{\\theta} \\| \\bm{b} \\| \\\\\n&= \\| \\text{projection of } \\bm{b}\\text{ on }\\bm{a} \\| \\cdot \\|\\bm{a}\\| = \\| \\bm{b} \\| \\cos{\\theta} \\| \\bm{a} \\| \\\\\n\n\\end{aligned}\n$$\n","n":0.091}}},{"i":113,"$":{"0":{"v":"Cross Product","n":0.707},"1":{"v":"\nGiven \n$$\n\\bm{a} = a_1 \\bm{i} + a_2 \\bm{j} + a_3 \\bm{k} \\\\\n\\bm{b} = b_1 \\bm{i} + b_2 \\bm{j} + b_3 \\bm{k}\n$$\nwhere ($\\bm{i}$, $\\bm{j}$, $\\bm{k}$) are orthonormal basis\n\n\n## Vector-Vector\n\n$$\n\\begin{aligned}\n\n\\bm{a} \\times \\bm{b} &= \n\\begin{vmatrix}\n\\bm{i} & \\bm{j} & \\bm{k} \\\\\na_1 & a_2 & a_3 \\\\\nb_1 & b_2 & b_3\n\\end{vmatrix} \\\\\n\n&=\n\n\\begin{vmatrix}\na_2 & a_3 \\\\\nb_2 & b_3\n\\end{vmatrix}\n\n\\bm{i} -\n\n\\begin{vmatrix}\na_1 & a_3 \\\\\nb_1 & b_3\n\\end{vmatrix}\n\n\\bm{j} +\n\n\\begin{vmatrix}\na_1 & a_2 \\\\\nb_1 & b_2\n\\end{vmatrix}\n\n\\bm{k} \\\\\n\n&= (a_2 b_3 - a_3b_2) \\bm{i} - (a_1 b_3 - a_3 b_1) \\bm{j} + (a_1 b_2 - a_2 b_1) \\bm{k}\n\n\\end{aligned}\n\n$$\n\n## Geometry view\n$$\n\\bm{p} \\cdot \\bm{x} = \n\\det{(\n\\begin{bmatrix}\n\\bm{x} & \\bm{a} & \\bm{b}\n\\end{bmatrix})}\n$$\n\nfor any vector $\\bm{x}$\n\nAssume:\n- $\\bm{c}$ is the vector that perpendicular to $\\bm{a}$ and $\\bm{b}$\n- $a$ is the area of parallelogrom spanned out by $\\bm{a}$ and $\\bm{b}$\n- $V$ is the volume of space spanned out by $\\bm{x}$, $\\bm{a}$ and $\\bm{b}$\n\nWe have\n$$\nV = \\| \\text{projection of } \\bm{x} \\text{ on } \\bm{c} \\| \\cdot a =\n\\bm{x} \\cdot (a \\cdot \\bm{c})\n$$\n\nSo $\\bm{p} = a \\cdot \\bm{c}$, hence the cross product\n\n## Matrix-Vector\n\n### skew-symmetric matrix\n\n$$\n\\bm{A}^{T} = - \\bm{A}\n$$\n\nDefine skew-symmetric matrix $[\\bm{a}]_{\\times}$ as\n\n$$\n[\\bm{a}]_{\\times} = \n\\begin{bmatrix}\n0 & -a_3 & a_2 \\\\\na_3 & 0 & -a_1 \\\\\n-a_2 & a_1 & 0\n\\end{bmatrix}\n$$\n\n$$\n\\begin{aligned}\n\n\\bm{a} \\times \\bm{b} = [\\bm{a}]_{\\times} \\bm{b} &= \n\n\\begin{bmatrix}\n0 & -a_3 & a_2 \\\\\na_3 & 0 & -a_1 \\\\\n-a_2 & a_1 & 0\n\\end{bmatrix}\n\n\\begin{bmatrix}\nb_1 \\\\\nb_2 \\\\\nb_3\n\\end{bmatrix} \\\\\n\n&= \\begin{bmatrix}\n0 & -a_3 b_2 & a_2 b_3 \\\\\na_3 b_1 & 0 & -a_1 b_3 \\\\\n-a_2 b_1 & a_1 b_2 & 0\n\\end{bmatrix}\n\n\\end{aligned}\n$$\n\n\n## Properties\n\n1. Magnitude of cross product $\\|\\bm{a} \\times \\bm{b}\\|$ defines the positive area of parallelogram with $\\bm{a}$ and $\\bm{b}$ as sides. Equivalent of the determinant of the matrix form\n\n2. Cross product of 2 linear dependent vectors\n$\\bm{a}$ and $\\bm{b}$ is zero vector (area is 0)\n$$\n\\bm{a} \\times \\bm{b} = \\bm{0}\n$$\n","n":0.06}}},{"i":114,"$":{"0":{"v":"Cholesky Decomposition","n":0.707},"1":{"v":"\n> Reference: https://www.math-linux.com/mathematics/linear-systems/article/cholesky-decomposition\n\nSymmetric positive definite $\\bm{A}$ can be decomposed into $\\bm{A} = \\bm{L}\\bm{L}^T$, where $\\bm{L}$ is lower triangle matrix\n\n## Application\nTo solve $\\bm{A} \\bm{x} = \\bm{b}$, first solve $\\bm{L} \\bm{y} = \\bm{b}$ then $\\bm{y} = \\bm{L}^T \\bm{x}$. Note linear system with lower/upper triangle matrix can be easily solve with [[forward substitution|linear-algebra.cholesky-decomposition#forward-substitution]] / [[backward substitution|linear-algebra.cholesky-decomposition#backward-substitution]]\n\n### forward substitution\n> Reference: https://en.wikipedia.org/wiki/Triangular_matrix#:~:text=In%20the%20mathematical%20discipline%20of,the%20main%20diagonal%20are%20zero.\n\nFor lower triangle matrix system $\\bm{L} \\bm{x} = \\bm{b}$, in linear equation form, we have\n\n$$\n\\begin{aligned}\n&l_{1, 1} x_1 &= b_1 \\\\\n&l_{2, 1} x_1 + l_{2, 2} x_2 &= b_2 \\\\\n&l_{3, 1} x_1 + l_{3, 2} x_2 + l_{3, 3} x_3 &= b_3 \\\\\n&\\dots \\\\\n&l_{m, 1} x_1 + l_{m, 2} x_2 + l_{m, 3} x_3 + \\dots + l_{m, m} x_m &= b_m\n\\end{aligned}\n$$\n$x_i$ can be solved recursively with $x_{i-1}$\n\n### backward substitution\nSimilar to [[forward substitution|linear-algebra.cholesky-decomposition#forward-substitution]] but going backwards","n":0.087}}},{"i":115,"$":{"0":{"v":"Basis","n":1},"1":{"v":"\n> Reference: https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n\n## Basis vectors\n\nLinear independent vectors that span the whole vector space. All other vectors in same space can be interpreted as linear combinations of basis vectors\n\n## Matrix\n### Square Matrix\n\n#### Determinant\nMeasure how area changes due to transformation, 0 means squished into lower-dimension, no inversion\n\n#### Rank\nNumber of dimension in outputs (column space)\n\n- Full rank: dimension of column space equals number of columns\n\n-  Null space (kernel); space of all vectors land on zero vector (origin)\n\n### Non-square Matrix\nTransformation between dimensions","n":0.113}}},{"i":116,"$":{"0":{"v":"Affine Transformation","n":0.707},"1":{"v":"\n## Definition\n\n### Linear transformation\n\nLinear function, transform input vector to output vector\n\n- Preserves parallelism for all lines (grid line parallel evenly spaced)\n- Origin remain fixed\n\n### Affine transformation\nLinear transformation + translation\n\n> No longer perserves origin\n\n> Not necessarily perserves distances and angles\n\n## Properity\n\nAssume affine transformation matrix $\\bm{M} = \\begin{bmatrix} \\bm{A} & \\bm{b} \\\\ 0 & 1 \\end{bmatrix} \\in \\mathbb{R}^{4 \\times 4}$, where $\\bm{A} = \\left[\\begin{array}{c|c|c} &  &  \\\\ \\bm{x} & \\bm{y} & \\bm{z} \\\\&  & \\end{array}\\right] = \\left[\\begin{array}{c|c|c} x_{0} & y_{0} & z_{0} \\\\ x_{1} & y_{1} & z_{1} \\\\ x_{2} & y_{2} & z_{2} \\end{array}\\right]$, $\\bm{b} = \\begin{bmatrix} w_{0} \\\\ w_{1} \\\\ w_{2}  \\end{bmatrix}$. \n$\\frac{\\bm{x}}{\\lVert \\bm{x} \\rVert}$, $\\frac{\\bm{y}}{\\lVert \\bm{y} \\rVert}$, $\\frac{\\bm{z}}{\\lVert \\bm{z} \\rVert}$ define the new coordinate basis, $\\lVert \\bm{x} \\rVert$, $\\lVert \\bm{y} \\rVert$, $\\lVert \\bm{z} \\rVert$ define scaling along basis and $\\bm{b}$ defines the new origin.\n\nAssume vector $\\bm{v^{'}} = \\begin{bmatrix} \\bm{v} & 1 \\end{bmatrix}$, where $\\bm{v} = \\begin{bmatrix} v_0 & v_1 & v_2 \\end{bmatrix}$ , $\\bm{M} \\cdot \\bm{v^{'}} = \\bm{A} \\cdot \\bm{v} + \\bm{b}$\n","n":0.078}}},{"i":117,"$":{"0":{"v":"Storage","n":1},"1":{"v":"\nUsually (like in openGL) contiguous in memory: \n\n$$\n\\begin{bmatrix} x_{0} & x_{1} & x_{2} & 0 & y_{0} & y_{1} & y_{2} & 0 & z_{0} & z_{1} & z_{2} & 0 & w_{0} & w_{1} & w_{2} & 1 \\end{bmatrix}\n$$\n\nIn practice, the multiplication is equivalent to \n$$\nv_0\\begin{bmatrix} x_0 & x_1 & x_2 \\end{bmatrix} + v_1\\begin{bmatrix} y_0 & y_1 & y_2 \\end{bmatrix} + v_2\\begin{bmatrix} z_0 & z_1 & z_2 \\end{bmatrix}\n$$\n","n":0.12}}},{"i":118,"$":{"0":{"v":"Multiplication","n":1},"1":{"v":"## Row Major\n$$\n\\bm{v} \\cdot \\bm{A} = \\begin{bmatrix} v_0 & v_1 & v_2 \\end{bmatrix} \\left[\\begin{array}{ccc} x_0 & x_1 & x_2 \\\\ \\hline y_0 & y_1 & y_2 \\\\ \\hline z_0 & z_1 & z_2 \\end{array}\\right] = \\begin{bmatrix} x_0 v_0 + y_0 v_1 + z_0 v_2 & x_1 v_0 + y_1 v_1 + z_1 v_2 & x_2 v_0 + y_2 v_1 + z_2 v_2 \\end{bmatrix}\n$$\n\n## Column Major\n$$\n\\bm{A} \\cdot \\bm{v} =\n\\begin{bmatrix}\n\\bm{x} & \\bm{y} & \\bm{z}\n\\end{bmatrix}\n\n\\cdot \\bm{v}\n\n=\n\\left[\\begin{array}{c|c|c} x_0 & y_0 & z_0 \\\\ x_1 & y_1 & z_1 \\\\ x_2 & y_2 & z_2 \\end{array}\\right] \\begin{bmatrix} v_0 \\\\ v_1 \\\\ v_2 \\end{bmatrix} = \\begin{bmatrix} x_0 v_0 + y_0 v_1 + z_0 v_2 \\\\ x_1 v_0 + y_1 v_1 + z_1 v_2 \\\\ x_2 v_0 + y_2 v_1 + z_2 v_2 \\end{bmatrix} = \\bm{v}[0] \\cdot \\bm{x} + \\bm{v}[1] \\cdot \\bm{y} + \\bm{v}[2] \\cdot \\bm{z}\n$$\n","n":0.085}}},{"i":119,"$":{"0":{"v":"Transform Normal Vector","n":0.577},"1":{"v":"\n$$\n\\bm{n}^{*} = \\bm{M}^{-T} \\bm{n}\n$$\n\n### proof\nAssume $\\bm{v}$ is a vector on surface, $\\bm{n}$ is the surface normal vector, it satisfied \n\n$$\n\\bm{n}^{T} \\bm{v} = 0\n$$\n\n$$\n\\bm{n}^{T} \\bm{M}^{-1} \\bm{M} \\bm{v} = 0\n$$\n\n$$\n(\\bm{M}^{-T} \\bm{n})^{T} \\bm{M} \\bm{v} = 0\n$$\n\nLet the newly transformed vector on surface be $\\bm{v}^{'} = \\bm{M} \\bm{v}$\n$$(\\bm{M}^{-T} \\bm{n})^{T} \\bm{v}^{'} = 0$$\n\nSo the transformed surface norm is \n\n$$\n\\bm{M}^{-T} \\bm{n}\n$$\n","n":0.135}}},{"i":120,"$":{"0":{"v":"Chain Multiplication","n":0.707},"1":{"v":"\n[Jupyer demo](/assets/documents/affine_2d.ipynb)\n\n## Transformation of coordinate system\n$\\bm{T_{cb}} \\bm{T_{ba}}$ transform coordinate from $a$ to $b$ to $c$, $\\bm{{T_{ba}}}^{-1}\\bm{M}\\bm{T_{ba}}$ transform coordinate from $a$ to $b$ then back to $a$\n\n## Non-uniform scale\nThe upper left submatrix is no longer guaranteed to be orthogonal (has non-orthonormal basis), so $\\left[ \\frac{\\bm{x}}{\\lVert \\bm{x} \\rVert}^{T} \\frac{\\bm{y}}{\\lVert \\bm{y} \\rVert}^{T} \\frac{\\bm{z}}{\\lVert \\bm{z} \\rVert}^{T} \\right] \\notin SO3$\n\n","n":0.135}}},{"i":121,"$":{"0":{"v":"Bone Hierarchy Transform","n":0.577},"1":{"v":"\nGiven Bone A, B, C, D with their corresponding local tranformation matrix ${T_{A}}$, $T_B = {T_{AB}}$, $T_C = {T_{BC}}$, $T_D = {T_{CD}}$\n```\nparent -> child\nA -> B -> C -> D\n```\nTheir world transformation matrix can be represented by\n\n$$\n{T_{WA} = T_A}\n$$\n\n$$\n{T_{WB} = T_A * T_B}\n$$\n\n$$\n{T_{WC} = T_A * T_B * T_C}\n$$\n\n$$\n{T_{WD} = T_A * T_B * T_C * T_D}\n$$\n\nIf we only want to modify the local transformation of B to B' by\n\n$$\n{T_{B'} = T_{B} * T_{BB'}}\n$$\n\nIt causes chain reaction that B's child bones' world transformation no long hold\n\n$$\n{T_{WC} \\neq T_A * T_{B'} * T_C}\n$$\n\n$$\n{T_{WD} \\neq T_A * T_{B'} * T_C * T_D}\n$$\n\nIt can be fixed by\n\n$$\n{T_{C} \\rightarrow T_{C'} = T_{B'B} * T_C}\n$$\n\n$$\n{T_{D} \\rightarrow T_{D'} = T_{C'C} * T_D}\n$$\n","n":0.093}}},{"i":122,"$":{"0":{"v":"Minor Cofactor Adjoint","n":0.577},"1":{"v":"\n## Minor\nMinor $\\bm{M}_{ij}$ of matrix $\\bm{A}$ is the determinant of submatrix of $\\bm{A}$ by deleting row $i$ column $j$\n\n## Cofactor\nCofactor $\\bm{c}_{ij} = {(-1)}^{i+j}\\bm{M}_{ij}$\n\n## Cofactor Matrix\nCofactor matrix $\\bm{C} = \\begin{bmatrix} c_{00} & c_{01} & \\dots \\\\ \\vdots & \\ddots & \\\\ c_{n0} & & c_{cc} \\end{bmatrix}$\n\n## Adjoint\nAdjoint matrix $adj(\\bm{A}) = \\bm{C}^{T}$\n\n## Inverse\nInverse $\\bm{A}^{-1} = \\frac{adj(\\bm{A})}{det(\\bm{A})}$\n","n":0.136}}},{"i":123,"$":{"0":{"v":"Latex","n":1}}},{"i":124,"$":{"0":{"v":"Math Symbols","n":0.707},"1":{"v":"\n- Bold math symbol: $\\pmb{x}$\n- Real number set: $\\mathbb{R}$\n- In: $\\in$, not in: $\\notin$\n- Normal distribution: $\\pmb{x} \\sim \\mathcal{N}(\\mu,\\sigma^2)$\n- Fraction: $\\frac{x}{y}$\n- Derivative $\\nabla$, partial derivative: $\\frac{\\partial f}{\\partial x}$\n- Expectation: $\\mathrm{E}$, Covariance: $\\mathrm{Cov}$, Variance $\\mathrm{Var}$\n- Define macro: $\\newcommand{\\x}{\\pmb{x}} \\x$ or macro with arguments $\\newcommand{\\b}[1]{\\pmb{#1}} \\b{x}$\n- Alignment: $\\begin{aligned} x &= 1 + 3 \\\\ &= 4 \\end{aligned}$\n- Matrix: $\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$ $\\begin{bmatrix} x_0 &y_0 &z_0 \\\\ x_1 &y_1 &z_1 \\\\ x_2 &y_2 &z_2 \\end{bmatrix}$\n- Derivatives: $f'(x)$, $f''(x)$, $\\frac{\\partial f}{\\partial x}$\n- Arrow: $\\larr$, $\\rarr$\n- Sum: $\\sum$ product: $\\prod$\n- Dot $\\cdot$, cross $\\times$\n- Inequality: $<$, $>$, $\\ll$, $\\gg$, $\\leq$, $\\geq$, $\\neq$\n- Mean $\\bar{\\bm{x}}$ or $\\overline{\\bm{X}}$\n- Forall: $\\forall$ Exist: $\\exist$","n":0.096}}},{"i":125,"$":{"0":{"v":"Git","n":1}}},{"i":126,"$":{"0":{"v":"Set Remote URL","n":0.577},"1":{"v":"```\ngit remote set-url origin https://github.com/google/filament.git\n```\n","n":0.447}}},{"i":127,"$":{"0":{"v":"Resolve Lfs Issue","n":0.577},"1":{"v":"\n```\ngit rm --cached -r .\ngit reset --hard\ngit rm .gitattributes\ngit reset .\ngit checkout .\n```","n":0.277}}},{"i":128,"$":{"0":{"v":"Pull Upstream","n":0.707},"1":{"v":"\n```\ngit remote add upstream https://github.com/google/filament.git\ngit pull upstream v1.12.0\n```\n","n":0.354}}},{"i":129,"$":{"0":{"v":"Geometry","n":1}}},{"i":130,"$":{"0":{"v":"Winding Number","n":0.707},"1":{"v":"\n[Triangulations](https://wias-berlin.de/software/tetgen/1.5/doc/manual/manual002.html#sec3)\n\n## Simplex\n- 0-simplex: point\n- 1-simplex: line segment\n- 2-simplex: triangle\n- 3-simplex: tetrahedron\n\nSimplicial complex: a set composed of Simplex\nSteiner point: not part of input geometry, but added during solution of the problem\n\n[A Survey of Unstructured Mesh Generation Technology](http://ima.udg.edu/~sellares/comgeo/owensurv.pdf)\n\n[Voronoi diagrams – a study of fundamental geometric data structure](https://www.cs.jhu.edu/~misha/Spring20/Aurenhammer91.pdf)\n\n[Optimality of the Delaunay triangulation in ℝd](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.413.7806&rep=rep1&type=pdf)\n\n[Generalized Winding Number](https://www.cs.utah.edu/~ladislav/jacobson13robust/jacobson13robust.html)\n\n## Todos:\n- [ ] graphcut segmentation\n- [ ] constrained Delaunay \n\n## tessellation\n- [ ] energy minimization\n- [ ] convex hull\n\n## 3D offset\n- Minkowski sum\n- Ray-repos\n\nLayered depth Image (dexel)\n","n":0.112}}},{"i":131,"$":{"0":{"v":"Subdivision","n":1},"1":{"v":"> Reference: https://www.graphics.rwth-aachen.de/media/papers/sqrt31.pdf\n\n## Concepts\n### Valence\nNumber of incident edges a.k.a. vertex degree\n\n### Extraordinary Vertices\nVertices where the mesh is not regular, e.g. valence 3 or 5 for quad mesh\n\n###  $C^1$ $C^2$ Smoothness\nContinuous first / second order derivatives. For mesh is derivative of its limit surface\n\n### n-adic Split\nIntersect n new vertices for all edges\n\nDy-adic split: bi-sects, 4 subfaces\nTri-adic split: tri-sects, 9 subfaces\n\n### Permutation matrix\nSquare binary matrix, exactly one entry of 1 each row and each column and 0s elsewhere\n\n## Basic idea\n- Use coarse control surface instead of parametric surface\n- Refine control surface that eventually converge to a smooth limit surface\n\n### Operator\n- Split: introduce new vertex\n- Smoothing: change old vertex position by computing neighbouring old vertices\n\n### Subdivision matrix\nSubdivision matrix maps k-ring neighbour to next level, useful for convergence check. Assume n vertices for k-ring neighbour, $\\bm{S} \\in \\mathbb{R}^{(n+1) \\times (n+1)}$, as it also needs to update old vertex.\n\n#### eigenanalysis\nIn order to keep multiply subdivision matrix, it's easier to apply eigenanalysis, convert basis to [[Eigenbasis|linear-algebra.eigenvector-eigenvalue#eigenbasis]], apply all transformation and convert back.\n\n- Converge to tangent: single largest eigenvalue of 1 with corresponding eigenvector $\\bm{1}$\n\n## TODO\n- [x] Dyadic split\n- [x] Smoothness $C^1$ $C^2$\n- [x] Extraordinary vertices\n- [ ] Stencil\n- [x] Permutation matrix\n- [x] Eigen structure\n- [ ] Converge condition\n","n":0.07}}},{"i":132,"$":{"0":{"v":"Signed Distance Function","n":0.577},"1":{"v":"\nSigned distance function is a function defined in $\\bm{x} \\in \\mathbb{R}^3$, such that the function value $f(\\bm{x})$  equals to the signed distance from point $\\bm{x}$ to the closest surface. To fit it, we need:\n\n- A set of surface points $\\bm{x}_i \\in \\mathbb{R}^3$ (zero-level set $f(\\bm{x}_i) = 0$)\n- Normal vectors $\\bm{n}_i \\in \\mathbb{R}^3$ for those points\n\n## Point cloud normal estimation\n\n> Reference: https://cs.nyu.edu/~panozzo/gp/04%20-%20Normal%20Estimation,%20Curves.pdf\n\nTo assign normal vector $\\bm{n}$ for point $\\bm{x}$, we need\n- Estimate direction by fitting local plane\n- Find consistent global orientation\n\n### Find local plane\n> Reference: https://www.graphics.rwth-aachen.de/media/papers/p_Pau021.pdf\n\nFor each point $\\bm{x}$, we pick the $n-1$ nearest neighbour points. The objective is to fit a plane for these $n$ points, such that the sum of distance for each point to the plane is minimized.  The distance of point to surface can be modeled as [[linear-algebra.dot-product]] with surface normal $\\bm{n}$, given a point $\\bm{c}$ the plane pass through.\n\n$$\nmin \\sum_{i=1}^{n} ((\\bm{x}_i - \\bm{c})^T\\bm{n})^2\n$$\n\nNote that it is equivalent to [[linear-algebra.feature-extraction.principle-component-analysis]], by performing eigenanalysis on the covariance matrix ($\\mathrm{Cov} \\in \\mathbb{R}^{3 \\times 3}$) of those points. We pick $(\\pm)\\bm{n}$ as the eigenvector that corresponds the smallest eigenvalue (We want to minimized the variance as opposed to PCA)\n\n### Find consistent global orientation\n#### Viewpoint known\nOrient all normals consistently towards viewpoint\n#### Viewpoint unknown\nSee http://mediatum.ub.tum.de/doc/800632/941254.pdf\n\n## Gradient of iso-surface\n> Reference: https://ocw.mit.edu/courses/mathematics/18-02sc-multivariable-calculus-fall-2010/2.-partial-derivatives/part-b-chain-rule-gradient-and-directional-derivatives/session-36-proof/MIT18_02SC_notes_19.pdf\n\nGiven iso-surface $S$, $f(\\bm{p}) = c$ for any point $\\bm{p} \\in S$. With any curve on surface\n\n$$\n\\bm{r}(t) = \\begin{bmatrix} x(t) & y(t) & z(t) \\end{bmatrix}^T\n$$\ngoes through point $\\bm{r}(t_0) = \\begin{bmatrix} x(t_0) & y(t_0) & z(t_0) \\end{bmatrix}^T$, we have\n\n$$\nf(\\bm{r}(t_0)) = c\n$$\n\n$$\n\\therefore f'(\\bm{r}(t_0)) = 0\n$$\n\nwhich means its [[linear-algebra.vector-differentiation.total-derivative]] evaluated at $\\bm{r}(t_0)$ is 0\n\n$$\n\\therefore\n\\frac{\\partial f}{\\partial t}|_{t_0} = \\frac{\\partial f}{\\partial x} |_{\\bm{p}} \\frac{\\partial x}{\\partial t}|_{t_0} + \n\\frac{\\partial f}{\\partial y}|_{\\bm{p}} \\frac{\\partial y}{\\partial t}|_{t_0} + \n\\frac{\\partial f}{\\partial z}|_{\\bm{p}} \\frac{\\partial z}{\\partial t}|_{t_0} = 0\n$$\n\n$$\n\\therefore\n\\begin{bmatrix}\n\\frac{\\partial f}{\\partial x} |_{\\bm{p}} &\n\\frac{\\partial f}{\\partial y} |_{\\bm{p}} &\n\\frac{\\partial f}{\\partial z} |_{\\bm{p}}\n\\end{bmatrix}\n\n\\begin{bmatrix}\n\\frac{\\partial x}{\\partial t}|_{t_0} &\n\\frac{\\partial y}{\\partial t}|_{t_0} &\n\\frac{\\partial z}{\\partial t}|_{t_0}\n\\end{bmatrix} ^T = 0\n$$\n\n$$\n\\therefore \\nabla_{\\bm{p}} f \\cdot \\bm{r}'(t_0) = 0\n$$\n\nIts means the partial derivative $\\nabla f(\\bm{p})$ is perpendicular to tangent of any curve goes through $\\bm{p}$, a.k.a. the tangent plane. Thus, normal of $S$ at point $\\bm{p} \\in S$ is proportional to $\\nabla f(\\bm{p})$.\n\n## Function fitting with constraints\n\n### Spatial Index\n- Uniform grid\n- RTree: hierarchy of aabbs (Python example see: [[polygon.connect-polygons-with-their-offset-ones]])\n- K-d tree: binary division of whole space\n\n### Build constraints\nCompute non zero-level data samples $f(\\bm{x}_i) \\neq 0$\n\n> May cause undesired effects when distance of evaluation point to surface is much larger than offset distance\n\nInward / outward offsets each surface points along its normals for a distance such that the closest point of the offset point is the original point\n\n### Surface fiting\nFit with [[optimization.least-square.moving-least-square]]\n\n### Inference\n- Build grid samples points\n- Perform [[linear-algebra.feature-extraction.principle-component-analysis]] on grid samples points' **convex hull**\n- Rotate grid samples points based on priciple axis\n- Inference (weighted sum of neighourhood fitted parameters, with weight depends on distance) for each grid samples\n- Apply [[code-read.igl.marching-cubes]]\n\n## Function fitting with normal\n> Reference: https://people.eecs.berkeley.edu/~jrs/papers/cartons.pdf\n\n- Each point was paired with a function, which define the signed distance to this points' tangent plane\n- Fit and interpolate MLS using that function value\n\n\n## TODO\n- [x] Fir point cloud normals\n- [x] Moving least square\n- [ ] Why use polynomial basis?\n- [x] Marching cube\n- [ ] Screened Poisson\n- [ ] RIMLS\n","n":0.044}}},{"i":133,"$":{"0":{"v":"Mesh Smoothing","n":0.707},"1":{"v":"\n## Concepts\n\n### Bilateral filter\n> Reference: https://en.wikipedia.org/wiki/Bilateral_filter\n\nEdge-perserving + noise-reducing \n\n$$\nI^{filtered} (x) = \\frac{1}{W_p} \\sum_{x_i \\in \\Omega} I(x_i) f_r(\\|I(x_i)-I(x)\\|)g_s(\\|x_i - x\\|)\n$$\nwhere\n$$\nW_p = \\sum_{x_i \\in \\Omega} f_r(\\|I(x_i)-I(x)\\|)g_s(\\|x_i - x\\|)\n$$\n$\\Omega$ is the window centered at $x$, $f_r$ is intensity range kernel, $g_s$ is spatial distance kernel\n\n> Larger the intensity difference, smaller the weight it has on bluring. (Blur pixel with similar intensity while perserves sharp intensity changes)\n\n## Explicit laplacian smoothing\n> Reference: http://multires.caltech.edu/pubs/ImplicitFairing.pdf\n\nMove vertex along the opposite of its 1-ring neighbour normal $\\bm{n}$\n\n$$\n\\bm{v}_{i+1} = (I - \\lambda L) \\bm{v}_i\n$$\n\n\n### Uniform weighted\n> Reference: https://graphics.stanford.edu/courses/cs468-12-spring/LectureSlides/06_smoothing.pdf\n\n$$\nL_V(\\bm{v}) = (\\frac{1}{n} \\sum_{} \\bm{v}_i) - \\bm{v}\n$$\n\n### Cotangent weighted\n\nThe movement is proportioonal to the curvature. See [[Laplace-Beltrami|geometry.discrete-laplace-operator#laplace-beltrami]]\n\n$$\nL(\\bm{v}) = 2\\kappa_H \\bm{n}_i\n$$\n\n## Bilateral denoising\n> Reference: https://www.cs.tau.ac.il/~dcor/articles/2003/Bilateral-Mesh-Denoising.pdf\n\nGeneralization of Bilateral filter for mesh, with distance to tangent plane as intensity difference.\n\n### Notes\n\n#### Denoising mesh over point cloud\n- connetivity\n- fast neighbourhood access\n\n#### Mesh info decomposition\n- tangential: parametric info\n- nomral: geometric info\n\n> Move vertex along the direction of normal only modify the geometry of the mesh\n\n#### Comparison with image processing\n-  Irregular in both connectivity and sampling\n- Non-enegry perserving leads to mesh shrinkage\n- Vertex drift, mesh regularity decreases\n\n## TODO\n- [ ] [Implicit Laplacian smoothing](http://mesh.brown.edu/taubin/pdfs/taubin-sg95.pdf)\n- [ ] [Fourier transform on graph as linear combination of eigenvectors (frequencies) of laplacian operator](https://www.math.ucla.edu/~tao/preprints/fourier.pdf)\n- [ ] Mean curvature as divergence of normal vector $\\kappa_H = div \\bm{n}$\n- [ ] Vertex Drifting and mesh regularity\n- [x] Bilateral filter\n- [ ] Bilateral filter the Bayesian approach","n":0.066}}},{"i":134,"$":{"0":{"v":"Mesh Parameterization","n":0.707},"1":{"v":"\n> Reference: https://graphics.stanford.edu/courses/cs468-05-fall/Papers/param-survey.pdf\n\nMap points bijectively on surface to another domain (2D, sphere)\n\nSurfaces that are [[Homeomorphic to a disk|geometry.mesh-parameterization#homeomorphic-to-a-disk]] are mapped to 2D plane\n\nApplication\n- Texture mapping\n- Surface approximation and remeshing\n\n## Concepts\n### Homeomorphic to a disk\nTopologically same as a disk\n#### Disk\nA region in plane bounded by a circle\n\n#### Homeomorphic\nBicontinuous function (its inverse function is also continuous) maps between topological space. It's [[Isomorphism|geometry.mesh-parameterization#isomorphism]] in topology, a.k.a. preserve same topological properities\n\n#### Isomorphism\nStructure-preserving bijective mapping\n\n### Cauchy-Riemann equations\nFor complex number $z = x + iy$ and $w = u + iv$, $w = f(z)$ a.k.a.\n$u(x, y) + iv(x, y) = f(x + iy)$, \n\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y}\n$$\n\n$$\n\\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x}\n$$\n\n\n### Developable surface\nSmooth surface with zero [[Gaussian curvature|geometry.discrete-laplace-operator#gaussian-curvature]], a.k.a. one of the [[Principle curvatures|geometry.discrete-laplace-operator#principle-curvatures]] is 0\n\n## Mapping\nSuppose a suface $S \\in \\mathbb{R}^3$ parameterized as\n\n$$\n\\bm{x}(u, v) = (x_1(u, v), x_2(u, v), x_3(u, v))\n$$\n\nwhere $(u, v) \\in \\mathbb{R}^2$\n\n> Parameterization almost always introduces distortion in angles or areas\n\n\n### Regular surface\n1. $x1, x2, x3$ are differentiable\n2. $\\frac{\\partial \\bm{x}}{\\partial u}$ and $\\frac{\\partial \\bm{x}}{\\partial v}$ are linear dependent (nonzero cross product)\n\n### First fundamental form\nThe [[linear-algebra.inner-product]] of tangent vectors of [[Regular surface|geometry.mesh-parameterization#regular-surface]]\n\n$$\n\\bm{I} = \n\\begin{bmatrix} \ng_{11} & g_{12} \\\\\ng_{12} & g_{22}\n\\end{bmatrix}\n=\n\\begin{bmatrix} \n\\frac{\\partial \\bm{x}}{\\partial u}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial u} & \\frac{\\partial \\bm{x}}{\\partial u}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial v} \\\\\n\\frac{\\partial \\bm{x}}{\\partial v}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial u} & \\frac{\\partial \\bm{x}}{\\partial v}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial v}\n\\end{bmatrix}\n$$\n\n$\\bm{I}$ is positive definite, a.k.a. has strictly positive determinant $g_{11} g_{22} - g_{12}^2 > 0$\n\nIn [[Riemannian Metric|geometry.laplacian#riemannian-metric]] form\n\n$$\n\\begin{aligned}\nds^2 &= \n\\frac{\\partial \\bm{x}}{\\partial u}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial u} (du)^2 + 2\\frac{\\partial \\bm{x}}{\\partial u}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial v} (du dv) + \\frac{\\partial \\bm{x}}{\\partial v}^T \\cdot \\frac{\\partial \\bm{x}}{\\partial v} (dv)^2 \\\\\n&=\n\\begin{bmatrix} du & dv \\end{bmatrix} \\bm{I} \\begin{bmatrix} du \\\\ dv \\end{bmatrix}\n\\end{aligned}\n$$\n\n### Conformal Mapping\nAngle-preserving for corresponding angle between intersection arcs\n\n### Equiareal Mapping\nArea-preserving\n\n### Isometric Mapping\nLength-preserving, Conformal + Equiareal\n\n## Surface parameterization methods\n[[Isometric Mapping|geometry.mesh-parameterization#isometric-mapping]] is idea, but only hold for [[Developable surface|geometry.mesh-parameterization#developable-surface]]. The goal is usually minimizing angle distortion or area distortion or both.\n\n### Planar mapping\n$f: \\mathbb{R}^2 \\to \\mathbb{R}^2$ $f(x, y) = (u(x, y), v(x, y))$\n\n$$\n\\bm{I} = \\bm{J}^T \\bm{J}\n$$\n\nThe eigenvalues $\\lambda_1$ $\\lambda_2$ of $\\bm{I}$ satisfied\n$$\n\\begin{cases}\nf\\text{ is isometric} & \\bm{I} = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix} & \\lambda_1 = \\lambda_2 = 1 \\\\\nf\\text{ is conformal} & \\bm{I} = \\begin{bmatrix}n & 0 \\\\ 0 & n \\end{bmatrix} & \\lambda_1 / \\lambda_2 = 1 \\\\\nf\\text{ is equiareal} & \\det\\bm{I} = 1 & \\lambda_1\\lambda_2 = 1 \\\\\n\\end{cases}\n$$\n\n### Conformal mapping\n[[Conformal Mapping|geometry.mesh-parameterization#conformal-mapping]] satisfied [[Cauchy-Riemann equations|geometry.mesh-parameterization#cauchy-riemann-equations]]. By taking derivative w.r.t. to $x$ and $y$, we have\n\n$$\n\\frac{\\partial^2 u}{\\partial x^2} = \\frac{\\partial^2 v}{\\partial y \\partial x}\n, \\\n\\frac{\\partial^2 u}{\\partial y^2} = -\\frac{\\partial^2 v}{\\partial x \\partial y}\n, \\\n\\frac{\\partial^2 u}{\\partial x \\partial y} = \\frac{\\partial^2 v}{\\partial y^2}\n, \\\n\\frac{\\partial^2 u}{\\partial y \\partial x} = -\\frac{\\partial^2 v}{\\partial x^2}\n$$\nthen we have\n$$\n\\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0 , \\\n\\frac{\\partial^2 v}{\\partial x^2} + \\frac{\\partial^2 v}{\\partial y^2} = 0\n$$\nwhich are two [[Laplace equations|geometry.laplacian#laplace-equation]]\n$$\n\\Delta u = 0, \\ \\Delta v = 0\n$$\n\nIt means [[Conformal Mapping|geometry.mesh-parameterization#conformal-mapping]] is also harmonic\n\n#### Harmonic benefits\n- Solution to linear Elliptic partial differential equation (PDE)\n- Guaranteed one-to-one for convex region\n\n## Uniform / Cotangent Laplacian mapping\nSee: [[Data fitting (interpolating)|geometry.laplacian#data-fitting-interpolating]]\n\n> Local transform applied to one-ring neighbourhood\n\n1. Find boundary vertices, map it to unit circle uv\n2. Compute Uniform / Cotanagent Laplacian weight matrix $\\bm{L}$\n3. For each boundary row of $\\bm{L}$, set it boundary index value to $1$ and others to 0 (Laplacian does not hold for boundary)\n3. Build target vector $\\bm{b}$, with boundary row equals to computed uv (boundary condition), non-boundary rows equals to $\\bm{0}$ (Laplacian)\n4. Solve $\\bm{L} \\bm{X} = \\bm{b}$\n\n\n## Least square conformal mapping (LSCM)\n> Reference: https://members.loria.fr/Bruno.Levy/papers/LSCM_SIGGRAPH_2002.pdf\n\nDefine criterion $C$ that minimize the violation of [[Cauchy-Riemann equations|geometry.mesh-parameterization#cauchy-riemann-equations]]\n\n$$\nC(T) = \\int_T |\\frac{\\partial \\mathcal{U}}{\\partial x} + i \\frac{\\partial \\mathcal{U}}{\\partial y}|^2 dA = |\\frac{\\partial \\mathcal{U}}{\\partial x} + i \\frac{\\partial \\mathcal{U}}{\\partial y}|^2 A_T\n$$\nwhere $\\mathcal{U} = u + iv$, $T$ is a triangle, $A_T$ is the area of the triangle, $|z|$ is the modulus of complex number $z$ ($\\sqrt{x^2 + y^2}$ for $z=x+iy$)\n\nThe criterion summed over whole triangulation\n$$\nC(\\mathcal{T}) = \\sum_{T \\in \\mathcal{T}} C(T)\n$$\nwhere $\\mathcal{T}$ the set of all triangles\n\n### Jacobian view\n> Reference: http://crl.ethz.ch/teaching/shape-modeling-18/lectures/05_Mappings.pdf\n\nWe want the jacobian \n$$\n\\bm{J} =\n\\begin{bmatrix}\n\\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\\n\\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y}\n\\end{bmatrix}\n$$\nto be a similarity matrix\n$$\n\\begin{bmatrix}\n\\alpha & -\\beta \\\\\n\\beta & \\alpha\n\\end{bmatrix}\n$$\n\nThus we have\n$$\n\\frac{\\partial u}{\\partial x} = \\frac{\\partial v}{\\partial y} \\\\\n\\frac{\\partial u}{\\partial y} = -\\frac{\\partial v}{\\partial x}\n$$\nwhich is aligned with [[Cauchy-Riemann equations|geometry.mesh-parameterization#cauchy-riemann-equations]]\n\n### Side notes\n\nTexture atlas generation steps\n- Segementation: Partition model into a set of charts, with boundary that minimize texture artifacts\n- Parameterization: Unfold charts into $\\mathbb{R}^2$, such that the sampling is as uniform as possible\n- Packing: Gather charts optimally in texture space\n\nMinimize angle deformation and non-uniform scaling\n\n## TODO\n- [x] Homeomorphic to a disk\n- [x] Cauchy-Riemann equations\n- [x] Angle deformations\n- [x] Conformal Map\n- [x] Isotropic\n- [ ] Matrix form of LSCM solution\n","n":0.036}}},{"i":135,"$":{"0":{"v":"Mesh Deformation","n":0.707},"1":{"v":"\n> Reference: https://lgg.epfl.ch/publications/2006/botsch_2006_DTD.pdf\n\n## Variational minimization approach\nSuface $\\mathcal{S}$ = low-frequency based surface $\\mathcal{B}$ + high-frequency geometry details $\\mathcal{D}$\n\n### Typical steps\n1. Compute $\\mathcal{B}$ from $\\mathcal{S}$\n2. Encode details $\\mathcal{D}$ as displacement w.r.t. $\\mathcal{B}$\n3. Deform $\\mathcal{B} \\to \\mathcal{B}'$\n4. Apply the details $\\mathcal{S}' =\\mathcal{B}' + \\mathcal{D}$\n\n\n### Limitations\n1. Difference between $\\mathcal{S}$ and $\\mathcal{B}$ needs to be small enough that the a height field w.r.t. $\\mathcal{B}$ is sufficient to represent it (otherwise more hierarchies are needed)\n2. Displacement may cause self-intersection\n\n## Deformation Transfer\n\n### Definition\nFor a source mesh in original state $\\mathcal{S}$ (with vertex $\\bm{q}_i$) and deformed state $\\mathcal{S}'$ (with vertex $\\bm{q}_i'$), a source deformation is computed (with gradient $\\bm{S}_j$) for each triangle. \n\nThe per-triangle deformation $\\bm{S}_j$ can be calculated as \n$$\n\\bm{S}_j = (\\bm{q}_1' - \\bm{q}_3', \\bm{q}_2' - \\bm{q}_3', \\bm{n}') \\cdot (\\bm{q}_1 - \\bm{q}_3, \\bm{q}_2 - \\bm{q}_3, \\bm{n})^{-1}\n$$\nwhere $\\bm{q}_i$ $\\bm{q}_i'$ and $\\bm{n}$ $\\bm{n}'$ are triangle vertex position and normal in $\\mathcal{S}$ or $\\mathcal{S}'$.\n\nThe goal is to find new deformed vertex position for a target mesh, such that the target deformation gradient matches the source ones $\\bm{S}_j$.\n\nThe new vertex position $\\bm{p}_i'$ can be calculated as\n$$\n\\bm{G}^T \\bm{D} \\bm{G}\n\\begin{bmatrix}\n{\\bm{p}_1'}^T \\\\\n\\vdots \\\\\n{\\bm{p}_n'}^T\n\\end{bmatrix}\n=\n\\bm{G}^T \\bm{D}\n\\begin{bmatrix}\n{\\bm{S}_1'}^T \\\\\n\\vdots \\\\\n{\\bm{S}_m'}^T\n\\end{bmatrix}\n$$\n\nwhere $\\bm{G} \\in \\mathbb{R}^{3m \\times n}$ as gradient operator (More see [[grad|code-read.igl.mesh-parameterization#grad]]), $\\bm{D} \\in \\mathbb{R}^{3m \\times 3m}$ is diagonal triangle-based weight matrix, $\\bm{G}^T \\bm{D}$ is divergence operator, $\\bm{G}^T \\bm{D} \\bm{G}$ is divergence of gradient, a.k.a. discrete laplacian operator.\n\n> **IMPORTANT** Above equation only constraints gradient. Translational contraints are also needed when performing mesh deformation\n\n### Application in mesh deformation\n\n1. Compute $\\mathcal{B}$ from $\\mathcal{S}$\n2. Deform $\\mathcal{B} \\to \\mathcal{B}'$ and compute per-triangle deformation gradient $\\bm{S}_j$\n3. Transfer $\\mathcal{S} \\to \\mathcal{S}'$ by complying $\\bm{S}_j$\n\n## TODO:\n- [ ] Thin shells energy\n- [ ] Gradient based deformation (suffers from translation insensitivity)\n- [ ] Discrete divergence operator","n":0.06}}},{"i":136,"$":{"0":{"v":"Local Global Mesh Parameterization","n":0.5},"1":{"v":"\n> Reference: http://www.cs.harvard.edu/~sjg/papers/arap.pdf\n\n## Concepts\n### Frobenius matrix norm\n$$\n\\|\\bm{A}\\|_F = \\sqrt{tr(\\bm{A}^T\\bm{A})}\n$$\n\n$$\n\\|\\bm{A}\\|_F = (\\sum_{i,j} |\\bm{A}_{ij}|^2)^{\\frac{1}{2}}\n$$\n\n## Objective function\n\nFind optimal local transformation for each individual triangle then stitching transformed triangle to 2D mesh\n\nDefine energy function\n$$\nE(u, \\bm{L}) = \\sum_{t=1}^T A_t \\|\\bm{J}_t(u) - \\bm{L}_t\\|_F^2\n$$\nwhere $A_t$ is triangle area, $\\bm{J}_t(u)$ is Jacobian of triangle, $\\bm{L}_t$ is auxiliary linear transformation\n\n$$\n(u, \\bm{L}) = \\argmin_{(u, \\bm{L})} E(u, \\bm{L}) \\ s.t. u, \\bm{L}_t \\in \\bm{M}\n$$\nwhere $\\bm{M}$ is the set of allowed linear transformation (similarity and rotation)\n\n## Matrix approximation\nTo approximate $\\bm{J} \\in \\mathbb{R}^{2 \\times 2}$ with $\\bm{L} \\in \\mathbb{R}^{2 \\times 2}$, define distance function with [[Frobenius matrix norm|geometry.local-global-mesh-parameterization#frobenius-matrix-norm]]\n$$\nd(\\bm{J}, \\bm{L}) = \\| \\bm{J} - \\bm{L} \\|_F^2\n$$\n\n### Signed SVD solution\n\nRecall [[linear-algebra.decomposition.singular-value-decomposition]], $\\bm{J} \\in \\mathbb{R}^{2 \\times 2}$ can be decomposed into\n$$\n\\bm{J} = \\bm{U} \\bm{\\Sigma} \\bm{V}^T\n$$\nwhere $\\bm{U}$ and $\\bm{V}$ are equally sized orthogonal basis matices (a.k.a. rotation or reflection)\n\nWe constraint $\\det (\\bm{U} \\bm{V}^T)$ to be positive by defining\n$$\n\\Sigma =\n\\begin{bmatrix}\n\\sigma_1 & 0 \\\\\n0 & -\\sigma_2\n\\end{bmatrix}\n$$\nwhere $sign (\\det J) = sign (\\sigma_2)$\n\n## As Similar As Possible (ASAP)\n> Reference: http://crl.ethz.ch/teaching/shape-modeling-18/lectures/05_Mappings.pdf\n\n> More see: [[Similarity|epipolar-geometry.euclidean-and-projective#similarity]]\n\n$$\nd = \\| \\bm{J} - \\bm{L}_S \\|_F^2\n$$\nwhere $\\bm{L}_S = \\begin{bmatrix} \\alpha & -\\beta \\\\ \\beta & \\alpha \\end{bmatrix}$ denotes the closest similarity matrix  to Jacobian $\\bm{J}$\n\nUsing [[Signed SVD solution|geometry.local-global-mesh-parameterization#signed-svd-solution]]\n$$\n\\begin{aligned}\nd = \\| \\bm{J} - \\bm{L}_S \\|_F^2 \n=& \\| \\bm{U} \\bm{\\Sigma} \\bm{V}^T - \\bar{\\sigma} \\bm{U} \\bm{V}^T \\|_F^2 \\\\\n=& \\| \\bm{U} (\\bm{\\Sigma} - \\bar{\\sigma} \\bm{I}) \\bm{V}^T\\|_F^2 \\\\\n=& \\| \\bm{\\Sigma} - \\bar{\\sigma} \\bm{I} \\|_F^2 \\\\\n=& (\\sigma_1 - \\bar{\\sigma})^2 + (\\sigma_2 - \\bar{\\sigma})^2\n\\end{aligned}\n$$\n\nThe optimal $\\bar{\\sigma} = \\frac{1}{2}(\\sigma_1 + \\sigma_2)$ so\n$$\nd = (\\sigma_1 - \\sigma_2)^2\n$$\n\nThe trivial solution is collapses all of vertices to a single point (0 energy). It can be avoided by pinning two vertices (usually most distant away) to two pre-determined positions in plane.\n\n### Relation to Comformal Mapping\n\nLet\n$$\n\\bm{J} = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} & \\frac{\\partial u}{\\partial y} \\\\ \\frac{\\partial v}{\\partial x} & \\frac{\\partial v}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n$$\nwhich can be decomposed into\n$$\n\\bm{J} =\n\\frac{1}{2} \\begin{bmatrix} a + d & c - b \\\\ b - c & a + d \\end{bmatrix} + \\frac{1}{2} \\begin{bmatrix} a - d & c + b \\\\ b + c & d - a \\end{bmatrix}\n$$\nwhere $\\begin{bmatrix} a + d & c - b \\\\ b - c & a + d \\end{bmatrix}$ is the similarity part and $\\begin{bmatrix} a - d & c + b \\\\ b + c & d - a \\end{bmatrix}$ is the anti-similarity part. Thus, the goal become\n$$\n\\begin{aligned}\nd =& \\| \\begin{bmatrix} a - d & c + b \\\\ b + c & d - a \\end{bmatrix} \\|_F^2 \\\\\n=& (a - d)^2 + (b + c)^2 \\\\\n=& (\\frac{\\partial u}{\\partial x} - \\frac{\\partial v}{\\partial y})^2 + (\\frac{\\partial u}{\\partial y} + \\frac{\\partial v}{\\partial x})^2\n\\end{aligned}\n$$\n\nwhich is equivalent to [[Least square conformal mapping (LSCM)|geometry.mesh-parameterization#jacobian-view]]\n\n## As Rigid As Possible (ARAP)\n> Reference: http://crl.ethz.ch/teaching/shape-modeling-18/lectures/05_Mappings.pdf\n\n> More see: [[Rigid|epipolar-geometry.euclidean-and-projective#rigid]]\n\n> We only consider Rotation(angle and area preserving) here\n\n$$\nd = \\| \\bm{J} - \\bm{L}_R \\|_F^2github\n$$\nwhere $\\bm{L}_R = \\begin{bmatrix} \\cos \\theta & \\sin \\theta \\\\ -\\sin \\theta & \\cos \\theta \\end{bmatrix}$ denotes the closest rotation matrix to Jacobian $\\bm{J}$\n\n\nUsing [[Signed SVD solution|geometry.local-global-mesh-parameterization#signed-svd-solution]]\n$$\n\\begin{aligned}\nd = \\| \\bm{J} - \\bm{L}_R \\|_F^2 \n=& \\| \\bm{U} \\bm{\\Sigma} \\bm{V}^T - \\bm{U} \\bm{V}^T \\|_F^2 \\\\\n=& \\| \\bm{U} (\\bm{\\Sigma} - \\bm{I}) \\bm{V}^T\\|_F^2 \\\\\n=& \\| \\bm{\\Sigma} - \\bm{I} \\|_F^2 \\\\\n=& (\\sigma_1 - 1)^2 + (\\sigma_2 - 1)^2\n\\end{aligned}\n$$\n\n## TODO\n- [ ] Triangle Jacobian\n- [x] Jacobian as distortion\n","n":0.043}}},{"i":137,"$":{"0":{"v":"Laplacian","n":1},"1":{"v":"\n> Reference: https://www.youtube.com/watch?v=oEq9ROl9Umk\n\n## Concepts\n\n### Isometry\n> Reference: https://mathworld.wolfram.com/Isometry.html\n\nDistance perserving mapping\n\n$$\nd(f(x), f(y)) = d(x, y)\n$$\n\n2d case: rigid transformation + reflection\n\n#### Rigid transformation\nrotation + translation\n\n### Divergency theorem\n> Reference: https://www.youtube.com/watch?v=rB83DpBJQsE\n\n#### Vector field\nAssociate each point on space with a vector\n\n#### Divergence\nMeasure how much it locally behaves like a sink/source \n\n##### 2D case\nhow much $(x, y)$ generate fluid\n\n$$\nF(x, y) = F_x \\bm{i} + F_y \\bm{j}\n$$\n\n$$\n\\nabla \\cdot F(x, y) = \\frac{\\partial F_x}{\\partial x} + \\frac{\\partial F_y}{\\partial y}\n$$\n\n> **NOT** equivalent to the sum of partial derivatives\n\nDivergence operator $\\nabla \\cdot$ maps vector to scalar $\\mathbb{R}^n \\to \\mathbb{R}$\n\n#### Incompressible\nLike fluid flow, no source, no sink\n$$\n\\nabla \\cdot F = 0\n$$\n\n#### Divergence of gradient\nGradient as a function of vector field, maxima becomes sinks, minima becomes sources\n\n#### Divergence Theorem\n> Reference: https://mathworld.wolfram.com/DivergenceTheorem.html\n\nFor a volume V with boundary $\\partial V$\n\n$$\n\\int_V \\nabla \\cdot F dV = \\int_{\\partial V} F da\n$$\n\nThe volume intergration of divergence is equivalent to the surface integration of volume boundary.\n\nIt means the density within the volume can only be changed by flowing in/out through boundary\n\n### Laplace equation\n\n$$\n\\Delta u = 0\n$$\n\n$u = g$ on boundary\n\n### Poisson equation\n\n$$\n\\Delta u = -f\n$$\n\n$u = g$ on boundary\n\n### Riemannian manifold\nReal smooth manifold with positive-definite inner product on tangent space for each point\n\n### Riemannian Metric\nRiemannian Metric is the collection of all inner products in Riemannian manifold\n\n$$\ng : T_pM \\times T_pM \\to \\mathbb{R}\n$$\n\nMaps two tangent vectors to a real number (inner product)\n\n### Random walk\n1. Average of many random walks -> time varying gaussian distributed (heat kernel)\n2. Evaluation at time t -> integration of domain at time t -> expected value at time t\n3. Laplacian -> derivative of expected value over time\n\n### Dirichlet energy\n> Reference: https://math.stackexchange.com/questions/3213598/what-are-the-use-cases-of-the-dirichlet-energy-in-computer-vision\n\nMeasurement of how function change over some region $\\Omega$ (smoothness, 0 for constant)\n\n$$\nE(u) = \\frac{1}{2} \\int_{\\Omega} \\| \\nabla u(x) \\|_2^2 dx\n$$\n\n$E(u)$ is convex\n\nDirichlet boundary means fixed boundary condition\n\n### Harmonic Function\n$$\n\\Delta u = 0\n$$\n\n- Mean value property\n- Min/max must be found on boundary\n\n## Motive\n- Reduce problem to solve sparse linear equation\n- Describe **Mean curvature**\n- **Isometry invariance**\nProperty preserves if transformation doesn't change point to point distance\n- **Self-adjoint** (behave like symmetric)\n- Positive definite, convex, unique minimizer\n- Allow **Frequency decomposition**, fourier basis\n\n## Definition\n### Euclidean space\n$u:\\mathbb{R}^n \\to \\mathbb{R}$\n\nLaplacian is the sum of the second order derivative evaluated at the target point\n\n$$\n\\Delta u = \\sum_{i=1}^{n} \\frac{\\partial^2}{\\partial x_i^2} u\n$$\n\nWhich is also the trace of [[linear-algebra.vector-differentiation.hessian]]\n\n$$\n\\Delta u = tr(H)\n$$\n\n#### Deivation from local average view\n\n##### Discrete\nGiven graph $G=(V,E)$, with value $u_i$ at each vertex i\n\nGraph Laplacian $L$ gives deviation from average value of all neighbours $j$\n\n$$\n(Lu)_i := (\\frac{1}{deg(i)}\\sum_{ij \\in E}u_j) - u_i\n$$\n\n- $deg$ means [[Valence|geometry.subdivision#valence]] of vertex $i$\n\n##### Continuous\nDifference between the value at point $x_0$ and the average value over a small sphere centered at $x_0$\n\n$$\n\\Delta u(x_0) \\propto \\lim_{\\epsilon \\to 0} \\frac{1}{\\epsilon^2}(\\frac{1}{|S_{\\epsilon}(x_0)|} \\int_{S_{\\epsilon}(x_0)} u(x)dx - u(x_0))\n$$\n\n- $\\epsilon$ is the radius of sphere\n- $|S_{\\epsilon}(x_0)|$ is the area of the sphere\n- $\\int_{S_{\\epsilon}(x_0)} u(x)dx$ is the integration of value over sphere\n- $u(x_0)$ value at center\n\n## Data fitting (interpolating)\nFit function to interpolate missing data\n\nGiven:\n- region $\\Omega \\in \\mathbb{R}^2$\n- boundary values $g:\\partial \\Omega \\to \\mathbb{R}$\n\nFind function $u$:\n- equal to boundary\n- fill interior as smooth (close to constant) as possible\n\nMinimizing [[Dirichlet energy|geometry.laplacian#dirichlet-energy]]\n$$\n\\min_{u:\\Omega \\to \\mathbb{R}} E (u) \\\\\ns.t. u = g|_{\\partial \\Omega}\n$$\n\n\nMinimizing Dirichlet energy is equivalent of solving Laplace equation\n\n$$\n\\begin{aligned}\n\\Delta u &= 0 \\ on \\ \\Omega \\\\\nu &= g \\ on \\ \\partial \\Omega\n\\end{aligned}\n$$\n\n## boundary condition\nSolution may not always exist for given boundary condition\n- Dirchlet boundary: fixed value\n- Neumann boundary: fixed derivative\n- Mixed: fixed value + fixed derivative\n\n## TODO:\n- [x] Divergence of gradient\n- [x] Divergence theorem\n- [x] Trace of Hessian\n- [x] Directional derivative\n- [x] Hessian in bilinear form\n- [x] Brownian motion (Randon walk)\n- [x] Laplace equation, poisson equation\n- [x] Riemannian Metric\n- [x] Dirichlet energy\n- [ ] Minimize Dirichlet energy equals Laplace proof\n- [ ] Exterior derivative\n- [ ] Covariant derivative\n- [ ] Self-adjoint\n- [ ] Spectral theorem\n- [ ] Harmonic function\n- [ ] Curvature, mean curvature\n- [ ] Discrete Laplacian\n","n":0.04}}},{"i":138,"$":{"0":{"v":"Discrete Laplace Operator","n":0.577},"1":{"v":"\n## Concepts\n### Curvature\n> Reference: https://mathworld.wolfram.com/Curvature.html\n\nMeasure local bending of the surface\n\n$$\nr d \\phi = d s \\\\\n\\phi = \\int_0^t s'(t) \\kappa(t) dt \\\\\nr = \\frac{1}{|\\kappa|}\n$$\n\nwhere $\\kappa$ is curvature, $s$ is arc length (length along a curve), $\\phi$ is tangential angle, $r$ is radius of curvature\n\n\nFor 2d curve $y = f(x)$, its curvature is defined by\n$$\n\n\\kappa = \\frac{\\frac{\\partial^2 f}{\\partial x^2}}{(1 + (\\frac{\\partial f}{\\partial x})^2)^{\\frac{3}{2}}}\n$$\n\n### Obtuse triangle\nTriangle with one angle bigger than $\\pi/2$, so its circumcenter point is outside triangle\n\n### Mass matrix\n\nDiscretization of inner-product (kinda like [[kernel|linear-algebra.feature-extraction.principle-component-analysis#kernel-trick]]?)\n\n## Discrete differential geometry operators\n\n> Reference: http://multires.caltech.edu/pubs/diffGeoOps.pdf\n\nSurface $S$ embedded in $\\mathbb{R}^3$ can be locally approximated by tangent plane, orthogonal to normal vector $n$.\n\n### Normal curvature\nFor unit direction $\\bm{e_{\\theta}}$ in the tangent plane, starting from the point where the tangent plane is evaluated, the normal curvature $\\kappa_N(\\theta)$ is defined as the curvature of curve that wraps $\\bm{e_{\\theta}}$ onto $S$. (like how matrix exponetiation wraps vector on tangent plane to manifold)\n\n### Principle curvatures\nTwo extremum [[Normal curvature|geometry.discrete-laplace-operator#normal-curvature]] $\\kappa_1$ and $\\kappa_2$ (2 cause $S$ is 2 dimensional manifold) with associated orthogonal direction $\\bm{e_1}$ and $\\bm{e_2}$.\n\n### Mean curvature\naverage of [[Normal curvature|geometry.discrete-laplace-operator#normal-curvature]]\n$$\n\\kappa_H = \\frac{1}{2\\pi}\\int_{0}^{2\\pi} \\kappa^N(\\theta)d\\theta\n$$\nIt can also be defined with [[Principle curvatures|geometry.discrete-laplace-operator#principle-curvatures]] as\n$$\n\\kappa_H = (\\kappa_1 + \\kappa_2)/2\n$$\n\n\n### Gaussian curvature\nproduct of the two [[Principle curvatures|geometry.discrete-laplace-operator#principle-curvatures]]\n\n$$\n\\kappa_G = \\kappa_1 \\kappa_2\n$$\n\n\nRelation between surface area minimization and mean curvature flow\n\n$$\n2\\kappa_H\\bm{n} = \\lim_{diameter(A) \\to 0} \\frac{\\nabla A}{A}\n$$\n\n### Laplace-Beltrami\nMaps point on surface to vector\n\nFor $\\bm{p} \\in S$\n$$\n\\bm{K}(\\bm{p}) = 2 \\kappa_H(\\bm{p})\\bm{n}(\\bm{p})\n$$\n\n\nDefine properies of surface at each vertex as spatial average of 1-ring neighbourhood of this vertex\n\n#### 1-ring neighbourhood boundary\nFor a given vertex, find all barycenter/circumcenter for all incident triangles, piecewisely connect centers and incident edge midpoints.\n\nNote that 1-ring neighbourhood for each vertex won't overlaps\n\n### Discrete Mean curvature Normal\n\n$$\n\\bm{K}(\\bm{x}_i) = \\frac{1}{2 A_{Mixed}} \\sum_{j \\in N_1(i)}(\\cot \\alpha_{ij} + \\cot \\beta_{ij})(\\bm{x}_i - \\bm{x}_j)\n$$\n\nwhere $\\alpha_{ij}$ and $\\beta_{ij}$ are angles opposite to shared edge $(\\bm{x}_i, \\bm{x}_j)$, $N_1(i)$ is the set of 1-ring neighbour vertex, $A_{Mixed}$ is\n\n$$\n\\begin{equation}\nA_{Mixed} = \n\\begin{cases}\nA_{voronoi}   & \\text{non-obtuse triangle}\\\\\n\\frac{1}{4} A & \\text{non-obtuse angle of obtuse triangle}\\\\\n\\frac{1}{2} A & \\text{obtuse angle of obtuse triangle}\n\\end{cases}\n\\end{equation}\n$$\n\nthat can be stored in diagonal [[Mass matrix|geometry.discrete-laplace-operator#mass-matrix]]\n\n### Discrete Gaussian curvature\n\n$$\n\\kappa_G(\\bm{x}_i) = (2\\pi - \\sum_{j=i}^{\\#f} \\theta_j)/A_{Mixed}\n$$\n\nwhere $\\theta_j$ the angle at the $j$-th face at vertex $\\bm{x}_i$ $\\#f$ is number of faces around this vertex\n\n### Discrete Principle curvatures\n\nUse equation defined in [[Mean curvature|geometry.discrete-laplace-operator#mean-curvature]] and [[Gaussian curvature|geometry.discrete-laplace-operator#gaussian-curvature]]\n\n$$\n\\kappa_1(\\bm{x}_i) = \\kappa_H(\\bm{x}_i) + \\sqrt{\\Delta(\\bm{x}_i)} \\\\\n\\kappa_2(\\bm{x}_i) = \\kappa_H(\\bm{x}_i) - \\sqrt{\\Delta(\\bm{x}_i)}\n$$\nwith $\\Delta(\\bm{x}_i) = \\kappa_H^2(\\bm{x}_i) - \\kappa_G(\\bm{x}_i)$ and $\\kappa_H(\\bm{x}_i) = \\frac{1}{2}\\|\\bm{K}(\\bm{x}_i)\\|$. $\\Delta(\\bm{x}_i)$ must be positive for numerical reason\n\n### Pre-Vertex Principle curvatures\n\n[[Discrete Mean curvature Normal|geometry.discrete-laplace-operator#discrete-mean-curvature-normal]] can be written in quadratic form\n\n$$\n\\bm{K}(\\bm{x}_i) = \\sum_{j\\in N_1(i)} w_{ij} \\kappa_{i,j}^N\n$$\nwhere $w_{ij} = \\frac{1}{A_{Mixed}} (\\frac{1}{8}(\\cot \\alpha_j + \\cot \\beta_j)\\|\\bm{x}_i - \\bm{x}_j\\|^2)$, $\\kappa_{i,j}^n = 2 \\frac{(\\bm{x}_i - \\bm{x}_j)\\cdot \\bm{n}}{\\|\\bm{x}_i - \\bm{x}_j\\|^2}$, which defines the [[Normal curvature|geometry.discrete-laplace-operator#normal-curvature]] along edge $\\bm{x}_i\\bm{x}_j$\n\nDefine a symmetric curvature tensor $B = \\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}$ that can be used to compute [[Normal curvature|geometry.discrete-laplace-operator#normal-curvature]] in any directions of the tangent plane\n(kinda like the [[linear-algebra.inner-product]])\n\nThe unit direction of the projection of edge $\\bm{x}_i\\bm{x}_j$ on tangent plane can be computed as\n$$\n\\bm{d}_{i,j}^T B \\bm{d}_{i,j} = \\kappa_{i,j}^N\n$$\n\n$$\n\\bm{d}_{i,j} = \\frac{(\\bm{x}_j - \\bm{x}_i) - ((\\bm{x}_j - \\bm{x}_i) \\cdot \\bm{n}) \\bm{n}}{\\|(\\bm{x}_j - \\bm{x}_i) - ((\\bm{x}_j - \\bm{x}_i) \\cdot \\bm{n}) \\bm{n}\\|}\n$$\n\nQuadratic fit $B$ with constraints\n\n$$\na + b = 2\\kappa_H \\\\\nac - b^2 = \\kappa_G\n$$\n\n> why with those constraint?\n\nPerform eigenanalysis on $B$ to find the two [[Principle curvatures|geometry.discrete-laplace-operator#principle-curvatures]] for given vertex\n\n## TODO\n- [ ] Express normal curvature in terms of principle curvature\n- [ ] Euler-Lagrange equation\n- [ ] Surface area minimization\n- [ ] Image of gaussian map\n- [ ] Conformal space parameters\n- [x] Obtuse triangle\n- [x] Mass matrix\n- [x] [[Cotangent|code-read.igl.laplacian#cotangent]]\n- [x] Curvature tensor\n- [ ] Curvature tensor fit constraints\n\n","n":0.041}}},{"i":139,"$":{"0":{"v":"Basics","n":1},"1":{"v":"\n## Triangle\n- Triangle contains 3 vertices ($v_0$, $v_1$, $v_2$) and 3 directed edges ($e_{01}$, $e_{12}$, $e_{20}$)\n- Vertices are counter-clockwise ordered, define face normal with right hand rule\n\n## Euler Characteristics\n> Reference: https://en.wikipedia.org/wiki/Euler_characteristic\n\n$$\n\\chi = |V| - |E| + |F|\n$$\nwhere $|V|$ is number of vertices, $|E|$ is number of edges and $|F|$ is number of faces\n\nFor convex polyhedron's surface\n$$\n|V| - |E| + |F| = 2\n$$\n\n> Reference: https://en.wikipedia.org/wiki/Topological_property\n\nEuler Characteristics is toplological invariant (invariant under [[Homeomorphic|geometry.mesh-parameterization#homeomorphic]])\n\n## TODO\n- [ ] Toplological genus","n":0.115}}},{"i":140,"$":{"0":{"v":"Epipolar Geometry","n":0.707}}},{"i":141,"$":{"0":{"v":"3d Pose Recovery","n":0.577}}},{"i":142,"$":{"0":{"v":"Recover Translation Given IMU Rotation and 3D to 2D Correspondence","n":0.316},"1":{"v":"\n## Algorithm\nGiven 3D landmark $\\bm{X} = (x, y, z)$ corresponds to 2D landmark $\\bm{x} = (x, y)$, camera intrinsic matrix $\\bm{K} = \\begin{bmatrix} f_x & 0 & c_x \\\\ 0 & f_y & c_y \\\\ 0 & 0 & 1 \\end{bmatrix}$, it satisfies:\n$$\\bm{K}[\\bm{R}|\\bm{t}]\\bm{X_h} = \\bm{K}(\\bm{R} \\bm{X} + \\bm{t})=\\bm{x_h}$$\nwhere $[\\bm{R}|\\bm{t}]$ is camera intrinsic matrix, $\\bm{x_h}$ is homogeneous coordinate of $\\bm{x}$, $\\bm{X_h}$ is homogeneous coordinate of $\\bm{X}$\n\nAssume $\\bm{R}$ is known from IMU, let\n$$\\bm{R} \\bm{X} + \\bm{t} = \\begin{bmatrix} a_1 \\\\ a_2 \\\\  a_3 \\end{bmatrix} + \\begin{bmatrix} t_1 \\\\ t_2 \\\\  t_3 \\end{bmatrix}, {\\bm{K}}^{-1}\\bm{x_h} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\ 1 \\end{bmatrix}$$\n\nWe have\n$$\\cfrac{t_1 + a_1}{t_3 + a_3}=b_1, \\cfrac{t_2 + a_2}{t_3 + a_3}=b_2$$\n\nWith matrix form\n$$\\begin{bmatrix} 1 & 0 & -b_1 \\\\ 0 & 1 & -b_2 \\end{bmatrix} \\begin{bmatrix} t_1 \\\\ t_2 \\\\ t_3  \\end{bmatrix} = \\begin{bmatrix} -a_1 + b_1a_3 \\\\ -a_2 + b_2 a_3 \\end{bmatrix}$$\n\nIt has the form $\\bm{A} \\bm{t} = \\bm{y}$, and can be solved via DLS $\\bm{t} = {({\\bm{A}}^{T} \\bm{A})}^{-1} {\\bm{A}}^{T} \\bm{y}$ given sufficient points (2 equation for 1 point, needs at least 2 points for 3 unknown)\n\n## DLS implementation\n```\nvoid SolveTranslationDLS(const std::vector<cv::Point3f>& v_p3_matched,\n                         const std::vector<cv::Point2f>& v_p2_matched,\n                         const Eigen::Matrix3d& K_inv,\n                         const Sophus::Matrix3d& gt_R,\n                         Sophus::Vector3d* est_t) {\n  assert(v_p3_matched.size() == v_p2_matched.size() &&\n         v_p3_matched.size() >= 0);\n\n  Eigen::MatrixXd A(2 * v_p3_matched.size(), 3);\n  Eigen::MatrixXd y(2 * v_p2_matched.size(), 1);\n  for (size_t i = 0; i < v_p3_matched.size(); i++) {\n    const Sophus::Vector3d X(v_p3_matched[i].x, v_p3_matched[i].y,\n                             v_p3_matched[i].z);\n    const Sophus::Vector2d x(v_p2_matched[i].x, v_p2_matched[i].y);\n\n    const Sophus::Vector3d a = gt_R * X;\n    const Sophus::Vector2d b = (K_inv * x.homogeneous()).hnormalized();\n\n    A.row(2 * i) << 1, 0, -b.x();\n    A.row(2 * i + 1) << 0, 1, -b.y();\n\n    y.row(2 * i) << -a.x() + b.x() * a.z();\n    y.row(2 * i + 1) << -a.y() + b.y() * a.z();\n  }\n  *est_t = (A.transpose() * A).inverse() * A.transpose() * y;\n}\n```\n## RANSAC implementation\n```\nbool SolveTranslationRANSAC(const std::vector<cv::Point3f>& v_p3_matched,\n                            const std::vector<cv::Point2f>& v_p2_matched,\n                            const cv::Mat& K_mat, const Sophus::Matrix3d& gt_R,\n                            Sophus::Vector3d* t, double threshold) {\n  Eigen::Matrix3d K;\n  cv::cv2eigen(K_mat, K);\n  auto K_inv = K.inverse();\n\n  assert(v_p3_matched.size() == v_p2_matched.size());\n\n  std::vector<int> v_idx;\n  v_idx.reserve(v_p3_matched.size());\n  for (int i = 0; i < v_p3_matched.size(); i++) {\n    v_idx.push_back(i);\n  }\n\n  Sophus::Vector3d est_t;\n\n  if (v_p3_matched.size() < 2) {\n    LOG(ERROR) << \"Insufficient 3d-2d matches\";\n    return false;\n  } else if (v_p3_matched.size() == 2) {\n    LOG(INFO) << \"Only 2 matches available, may produce wrong results\";\n    SolveTranslationDLS(v_p3_matched, v_p2_matched, K_inv, gt_R, &est_t);\n  } else {\n    std::vector<int> best_inliers_indices;\n    uint iters = ComputeRansacIteration(0, v_p3_matched.size(), 2);\n\n    uint i;\n    for (i = 0; i < iters; i++) {\n      // random sample 4 points to fit a model\n      // Fixed random seed for reproducibility\n      std::shuffle(std::begin(v_idx), std::end(v_idx), std::mt19937(0));\n\n      Sophus::Vector3d tmp_t;\n      std::vector<cv::Point3f> v_p3_tmp = {v_p3_matched[v_idx[0]],\n                                           v_p3_matched[v_idx[1]]};\n      std::vector<cv::Point2f> v_p2_tmp = {v_p2_matched[v_idx[0]],\n                                           v_p2_matched[v_idx[1]]};\n\n      SolveTranslationDLS(v_p3_tmp, v_p2_tmp, K_inv, gt_R, &tmp_t);\n\n      std::vector<int> inlier_indices;\n\n      // Evaulate results over whole dataset\n      for (int j = 0; j < v_p3_matched.size(); j++) {\n        Sophus::Vector3d X(v_p3_matched[j].x, v_p3_matched[j].y,\n                           v_p3_matched[j].z);\n        Sophus::Vector2d x(v_p2_matched[j].x, v_p2_matched[j].y);\n\n        Sophus::Vector2d x_proj = (K * (gt_R * X + tmp_t)).hnormalized();\n\n        if ((x_proj - x).norm() < threshold) {\n          inlier_indices.push_back(j);\n        }\n      }\n\n      if (inlier_indices.size() > best_inliers_indices.size()) {\n        best_inliers_indices = std::move(inlier_indices);\n        est_t = tmp_t;\n        iters = ComputeRansacIteration(best_inliers_indices.size(),\n                                       v_p3_matched.size(), 2);\n      }\n    }\n\n    LOG(INFO) << \"DLT ransac inlier size: \" << best_inliers_indices.size();\n\n    if (best_inliers_indices.size() < 2) {\n      LOG(INFO) << \"Inlier after ransac less than 2\";\n      return false;\n    } else if (best_inliers_indices.size() == 2) {\n      LOG(INFO) << \"Only 2 points available, may produce wrong results\";\n    } else {\n      // compute least square solution for all inliers\n      std::vector<cv::Point3f> v_p3_inliers;\n      std::vector<cv::Point2f> v_p2_inliers;\n\n      v_p3_inliers.reserve(best_inliers_indices.size());\n      v_p2_inliers.reserve(best_inliers_indices.size());\n\n      for (int idx : best_inliers_indices) {\n        v_p3_inliers.push_back(v_p3_matched[idx]);\n        v_p2_inliers.push_back(v_p2_matched[idx]);\n      }\n\n      SolveTranslationDLS(v_p3_inliers, v_p2_inliers, K_inv, gt_R, &est_t);\n    }\n  }\n\n  *t = -gt_R.transpose() * est_t;\n\n  return true;\n}\n```","n":0.042}}},{"i":143,"$":{"0":{"v":"Pipeline","n":1},"1":{"v":"\n## FIRST_FRAME\n1. Consider the first frame as the first keyframe.\n\n2. For the following frame, match 2D landmarks and compute average pixel distance w.r.t. first keyframe. If it is larger than a threshold, consider it as the second keyframe and start INITIALIZATION, otherwise go back to 2.\n\n## INITIALIZATION\n1. Given 2D to 2D landmarks correspondence, fit and decompose essential matrix. Replace $T_{rc}$ with IMU rotation. (Assuming the first keyframe pose as the world origin, cv fit/decompose essential matrix computes camera matrix $T_{cr}$, while we would like $T_{rc}$ for pose representation)\n\n2. Initialize local map by triangulating 3D landmarks with poses and 2D to 2D landmarks correspondence. If valid 3D landmarks count is low, go back to FIRST_FRAME-2. (To triangulate, we need camera matrix $T_{1w}$ and $T_{2w}$, so the triangulated 3D landmarks are in world coordinate. We consider a 3D landmark validly triangulated only if it has low symmetric reprojection error and sufficient cosparallax).\n\n## TRACKING\n1. For the following frame, we match the 2D landmarks with 3D landmarks in local map, as well as a match ratio, indicating how well the tracking is. If match ratio is 0, tracking is lost, go back to FIRST_FRAME-1 (Caching current pose for initialization has not been implemented yet).\n\n2. Compute $t_{wc}$ using IMU $R_{wc}$, obtain $T_{wc}$ . Verify if the translation is valid. If it's invalid, use last frame pose. If the translation is valid and match ratio is smaller than a threshold, we consider it a keyframe and redo INITIALIZATION-2. However, we scale the translation, and reject if valid 3D landmarks count is low and continue TRACKING-1.","n":0.062}}},{"i":144,"$":{"0":{"v":"TSDF","n":1},"1":{"v":"\n> Reference: https://graphics.stanford.edu/papers/volrange/volrange.pdf\n\n- Continuous implicit function $D(\\bm{x})$, denotes the weighted signed distance from $\\bm{x}$ to the nearest surface along viewing ray. Note that it is not but an approximation to the signed distance to nearest surface.\n- Cumulative signed distance function $D(\\bm{x})$ and cumulative weight $W(\\bm{x})$ are formed by combining $d(\\bm{x})$ and $w(\\bm{x})$ for each depth images.\n- $D(\\bm{x})$ is evaluted at discrete voxel grid to extract iso surface\n- Weight incorporates uncertainty in measurement (i.e. cosine of angle between vertex normal and viewing angle)\n- Weight needs to fall off at the max uncertainty interval in front of / behind the surface (restrict at the vicinity of the surface)\n\n## Combine rule\n$$\nD(\\bm{x}) = \\frac{\\sum w_i(\\bm{x}) d_i(\\bm{x})}{\\sum w_i(\\bm{x})}\n$$\n$$\nW(\\bm{x}) = \\sum w_i (\\bm{x})\n$$\n\n### Incremental calculation (combine depth image one by one)\n$$\nD_{i+1}(\\bm{x}) = \\frac{W_i(\\bm{x})D_i(\\bm{x}) + w_{i+1}(\\bm{x})d_{i+1}(\\bm{x})}{W_i(\\bm{x}) + w_{i+1}(\\bm{x})}\n$$\n$$\nW_{i+1}(\\bm{x}) = W_i(\\bm{x}) + w_{i+1}(\\bm{x})\n$$","n":0.086}}},{"i":145,"$":{"0":{"v":"Triangulation","n":1},"1":{"v":"> Reference: https://www.robots.ox.ac.uk/~vgg/hzbook/\n\n> Note it's under the assumption that error only occur in image, while the projection transformation is accurate\n\n![](/assets/images/2022-05-13-13-36-43.png){width: 250px}\n\nStart from reference frame (left), let its projection matrix be $\\bm{P}$,\n\n$$\n\\bm{x} = (\\alpha)\\bm{P} \\bm{X}\n$$\n\nwhich is valid up to a scaling factor. To eliminate scale ambiguity, we use cross product\n$$\n\\bm{x} \\times \\bm{P} \\bm{X} = \\bm{0}\n$$\n\nAssume $\\bm{P} = \\begin{bmatrix}\n\\bm{p}_1^T \\\\\n\\bm{p}_2^T \\\\\n\\bm{p}_3^T\n\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}$, where $\\bm{p}_i^T$, $\\bm{p}_2^T$, $\\bm{p}_3^T$ are rows of $\\bm{P}$, we have\n\n$$\n\\bm{P} \\bm{X} = \\begin{bmatrix}\n\\bm{p}_1^T \\bm{X} \\\\\n\\bm{p}_2^T \\bm{X} \\\\\n\\bm{p}_3^T \\bm{X} \\\\\n\\end{bmatrix}\n$$\n\nGiven $\\bm{x} = (x, y, 1)^T$\n$$\n\\begin{aligned}\n\\bm{x} \\times \\bm{P} \\bm{X} &=\n\\begin{bmatrix}\n0 & -1 & y \\\\\n1 & 0 & -x \\\\\n-y & x & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n\\bm{p}_1^T \\bm{X} \\\\\n\\bm{p}_2^T \\bm{X} \\\\\n\\bm{p}_3^T \\bm{X} \\\\\n\\end{bmatrix} \\\\ &=\n\\begin{bmatrix}\ny \\bm{p}_3^T \\bm{X} - \\bm{p}_2^T \\bm{X} \\\\\n\\bm{p}_1^T \\bm{X} - x \\bm{p}_3^T \\bm{X} \\\\\nx \\bm{p}_2^T \\bm{X} - y \\bm{p}_1^T \\bm{X} \\\\\n\\end{bmatrix} = \n\\begin{bmatrix}\n0 \\\\\n0 \\\\\n0\n\\end{bmatrix}\n\\end{aligned}\n$$\n\nThus with two equations (the third line is a linear combination of first two)\n\n$$\ny \\bm{p}_3^T \\bm{X} - \\bm{p}_2^T \\bm{X} = 0 \\\\\n\\bm{p}_1^T \\bm{X} - x \\bm{p}_3^T \\bm{X} = 0\n$$\n\nwith matrix form\n\n$$\n\\begin{bmatrix}\ny \\bm{p}_3^T - \\bm{p}_2^T \\\\\n\\bm{p}_1^T - x \\bm{p}_3^T\n\\end{bmatrix} \\bm{X} = \\bm{0}\n$$\n\nTake current frame into consideration, we have\n\n$$\n\\begin{bmatrix}\ny \\bm{p}_3^T - \\bm{p}_2^T \\\\\n\\bm{p}_1^T - x \\bm{p}_3^T \\\\\ny' \\bm{p}_3'^T - \\bm{p}_2'^T \\\\\n\\bm{p}_1'^T - x' \\bm{p}_3'^T\n\\end{bmatrix} \\bm{X} = \\bm{0}\n$$\n\nthat can be solved with [[optimization.least-square]] (3 unknowns with 4 equations)","n":0.069}}},{"i":146,"$":{"0":{"v":"RANSAC","n":1},"1":{"v":"> Reference: https://rpg.ifi.uzh.ch/docs/teaching/2021/09_multiple_view_geometry_3.pdf\n\nRandom Sample Consensus (RANSAC) is used for robust estimation (with large potion of outliers)\n\n> RANSAC is **non-deterministic**\n\n## Steps\n1. Sample minimum data samples required to fit the model and fit it\n2. Compute cost/loss for each data point, mark the one supporting current model as inliers\n3. Repeat $k$ times\n4. Select the set with maximum number of inliers, use that model / fit new model using all inliers\n\n## $k$ Selection\n\n$$\n1-p = (1-w^S)^K \\\\\nk = \\frac{\\log(1-p)}{\\log(1-w^S)}\n$$\n\n- $N$ total number of data points\n- $w$ fraction of lnliers in dataset, the probability of selecting an inlier in dataset\n- $S$ minimum points to fit the model\n    - $w^S$ probability of all selected points are inliers\n    - $1 - w^S$ probability of not all inliers\n- $k$ RANSAC iteration executed so far\n    - $(1-w^S)^K$ probability of not all inlier for all iteration executed\n- $p$ probability of success","n":0.085}}},{"i":147,"$":{"0":{"v":"Point Line and Plane","n":0.5},"1":{"v":"\n> Reference: https://www.robots.ox.ac.uk/~vgg/hzbook/\n\n## 2D (planar)\n\n### Point\n#### Euclidean\n$\\bm{x} = (x, y)^T \\in \\mathbb{R}^2$\n\n#### Homogenous \nNormalized $\\bm{x} = (x, y, 1)^T$, unnormalized $\\bm{x} = (kx, ky, k)^T$\n\n### Line\n$\\bm{l} = (a, b, c)^T$, unnormalized $\\bm{l} = (ka, kb, kc)^T$\n\n> Can be written as $y = -\\frac{a}{b} x -\\frac{c}{b}$. determined by gradient $-\\frac{a}{b}$ and $y$ interception $-\\frac{c}{b}$, 2 DoF\n\n### Point on line\nFor point $\\bm{x} = (x, y)^T$ on line $\\bm{l} = (a, b, c)^T$, $ax + by + c = 0$\n\nvector form with [[linear-algebra.dot-product]]\n$$\n\\bm{x}^T \\bm{l} = \\begin{bmatrix} x & y & 1 \\end{bmatrix} \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = 0\n$$\n\n### Intersection of lines\nFor lines $\\bm{l} = (a, b, c)^T$, $\\bm{l'} = (a', b', c')^T$\n\nDefine $\\bm{x} = \\bm{l} \\times \\bm{l'}$ with [[linear-algebra.cross-product]]\n\n$$\n\\bm{l} \\cdot (\\bm{l} \\times \\bm{l'}) = \\bm{l'} \\cdot (\\bm{l} \\times \\bm{l'}) = 0 \\\\\n\\bm{l} \\cdot \\bm{x} = \\bm{l'} \\cdot \\bm{x} = 0\n$$\nThus, $\\bm{x}$ is the intersection of $\\bm{l}$ and $\\bm{l'}$\n\n### Line passing through points\nFor points $\\bm{x} = (x, y, z)^T$, $\\bm{x'} = (x', y', z')^T$\n\nDefine $\\bm{l} = \\bm{x} \\times \\bm{x'}$ with [[linear-algebra.cross-product]]\n\n$$\n\\bm{x} \\cdot (\\bm{x} \\times \\bm{x'}) = \\bm{x'} \\cdot (\\bm{x} \\times \\bm{x'}) = 0 \\\\\n\\bm{x} \\cdot \\bm{l} = \\bm{x'} \\cdot \\bm{l} = 0\n$$\nThus, $\\bm{l}$ is the line that passing through $\\bm{x}$ and $\\bm{x'}$\n\n### Intersection of parallel lines (idea points)\nFor parallel lines $\\bm{l} = (a, b, c)^T$, $\\bm{l'} = (a, b, c')^T$\n\nTheir [[Intersection|epipolar-geometry.point-line-and-plane#intersection-of-lines]] can be computed as\n$$\n\n\\begin{aligned}\n\n\\bm{l} \\times \\bm{l'} &= \n\\begin{vmatrix}\ni & j & k \\\\\na & b & c \\\\\na & b & c'\n\\end{vmatrix}\n\\\\&=\n\\begin{vmatrix}\nb & c \\\\\nb & c'\n\\end{vmatrix} i\n-\n\\begin{vmatrix}\na & c \\\\\na & c'\n\\end{vmatrix} j\n+\n\\begin{vmatrix}\na & b \\\\\na & b\n\\end{vmatrix} k\n\\\\&=\nb(c'-c)i - a(c'-c)j + 0k\n\\\\&=\n(c'-c)(b, a, 0)^T\n\\end{aligned}\n$$\n\n$\\bm{x} = (b, a, 0)^T$ is the [[Point at infinity (Idea points)|epipolar-geometry.euclidean-and-projective#points-at-infinity-idea-points]], $(c'-c)$ is the scaling factor\n\n### Line passing through idea points (line at infinity)\nFor idea points $\\bm{x} = (x, y, 0)^T$, $\\bm{x'} = (x', y', 0)^T$\n\n[[Line passing through points|epipolar-geometry.point-line-and-plane#line-passing-through-points]] can be computed as\n\n$$\n\\begin{aligned}\n\\bm{x} \\times \\bm{x'}& = \n\\begin{vmatrix}\ni & j & k \\\\\nx & y & 0 \\\\\nx' & y' & 0\n\\end{vmatrix}\n\\\\&=\n\\begin{vmatrix}\ny & 0 \\\\\ny & 0\n\\end{vmatrix} i\n-\n\\begin{vmatrix}\nx & 0 \\\\\nx & 0\n\\end{vmatrix} j\n+\n\\begin{vmatrix}\nx & y \\\\\nx' & y'\n\\end{vmatrix} k\n\\\\& = 0i + 0j + (xy' - x'y)k\n\\\\& = (xy' - x'y) (0, 0, 1)^T\n\\end{aligned}\n$$\n\n$\\bm{l} = (0, 0, 1)^T$ is the [[Line at infinity|epipolar-geometry.euclidean-and-projective#line-at-infinity]], $(xy' - x'y)$ is the scaling factor\n\n## 3D\n### Point\n#### Euclidean\n$\\bm{x} = (x, y, z)^T \\in \\mathbb{R}^3$\n\n#### Homogenous\n$\\bm{x} = (x, y, z, 1)^T$\n\n### Plane\n$\\bm{\\pi} = (\\pi_1, \\pi_2, \\pi_3, \\pi_4)^T$\n\n> Similar to line in planar case, determined by gradient $(-\\frac{\\pi_1}{\\pi_3}, -\\frac{\\pi_2}{\\pi_3})$ and intersection with $z$ axis $-\\frac{\\pi_4}{\\pi_3}$, 3 DoF\n\n### Point on plane\n$$\n\\pi_1 x + \\pi_2 y + \\pi_3 z + \\pi_4 = 0\n$$\n#### Euclidean\n$$\n\\bm{n}^T \\bm{x} + d = 0\n$$\nwhere $\\bm{n} = (\\pi_1, \\pi_2, \\pi_3)^T$, $\\frac{\\bm{n}}{\\|\\bm{n}\\|}$ is plane normal, $d = \\pi_4$, $d/\\|\\bm{n}\\|$ is the distance of plane from origin\n\n##### Distance from a point $\\bm{p}$ to plane $(\\bm{n}, d)$\n\nDenote $\\bm{q}$ as any point on plane, distance of $\\bm{p}$ on plane is the projection of segement $\\bm{p} - \\bm{q}$ on plane normal $\\bm{n}$\n\n$$\n\\|\\frac{\\bm{n}}{\\|\\bm{n}\\|}^T (\\bm{p} - \\bm{q})\\|_2^2\n$$\n\n#### Homogenous\n$$\n\\bm{\\pi}^T\\bm{x} = 0\n$$\nwhere $\\bm{\\pi} = (\\pi_1, \\pi_2, \\pi_3, \\pi_4)^T$\n\n### Point, Plane, Line relation\n- 3 distinct points, or 1 line and 1 point (the point must not be incident with the line) define a plane\n- 2 distict planes join 1 line\n- 3 distict planes intersect 1 point\n","n":0.043}}},{"i":148,"$":{"0":{"v":"Pinhole Camera Model","n":0.577},"1":{"v":"\n## Base case\n\n> Reference: http://www.songho.ca/opengl/gl_projectionmatrix.html\n\nBegin with the assumption that \n- Camera with its center $c$ placed at orgin and focal length in pixel $f$\n- Image plane coordinate origin is the principal point\n- World coordinate and Image coordinate are evenly scaled along all axis\n\n> Under square pixel assumption, $\\text{focal length in pixel} = \\text{camera frame width} * \\text{focal length in mm} / \\text{sensor width in mm}$\n\nFor a point $\\bm{X} = (X, Y, Z)^T$ and it projection on image plane $\\bm{x} = (x, y)^T$\n\n![](/assets/images/2022-05-11-11-22-39.png){width: 500px}\n\nAs is shown in figure, we have\n$$\n\\frac{f}{Z} = \\frac{x}{X} = \\frac{y}{Y}\n$$\n\n$$\n\\therefore\n\\begin{cases}\nx = \\frac{fX}{Z} \\\\\ny = \\frac{fY}{Z}\n\\end{cases}\n$$\n\nThe projection matrix $\\bm{P}$ is the one that transforms $\\bm{X} = (X, Y, Z)^T$ to $\\bm{x} = (x, y)^T = (\\frac{fX}{Z}, \\frac{fY}{Z})^T$\n\n$$\n\\begin{bmatrix}\nx \\\\ y \\\\ 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{fX}{Z} \\\\ \\frac{fY}{Z} \\\\ 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nfX \\\\ fY \\\\ Z\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf & 0 & 0 & 0 \\\\ \n0 & f & 0 & 0 \\\\ \n0 & 0 & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nX \\\\ Y \\\\ Z \\\\ 1\n\\end{bmatrix}\n$$\n\nUnder the assumption that projection center is at origin, all points $(X, Y, Z, T)^T$ with fixed $X$, $Y$, $Z$ and varing $T$ forms a ray, which all project to image point $(fX, fY, Z)$, regardless of $T$.\n\n## Image plane coordinate not at principal center\n\nAssume principal point offset w.r.t. image plane coordinate is $(p_x, p_y)$\n\n![Image plane](/assets/images/2022-05-11-11-24-00.png){width: 300px}\n\nSimilar as before, but with $\\bm{x} = (x, y)^T = (\\frac{fX}{Z} + p_x, \\frac{fY}{Z} + p_y)^T$\n\n$$\n\\begin{bmatrix}\nx \\\\ y \\\\ 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\frac{fX}{Z} + p_x \\\\ \\frac{fY}{Z} + p_y \\\\ 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nfX + Zp_x \\\\ fY + Zp_y \\\\ Z\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nf & 0 & p_x & 0 \\\\ \n0 & f & p_y & 0 \\\\ \n0 & 0 & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\nX \\\\ Y \\\\ Z \\\\ 1\n\\end{bmatrix}\n$$\n\n## Camera center not at the origin of world coordinate\n\nAssume the world transformation w.r.t. camera ($\\bm{T}_{CW}$) is $[\\bm{R}|\\bm{t}]$\n\n![Camera in world coordinate](/assets/images/2022-05-11-11-24-54.png){width: 150px}\n\nWe can then transform $\\bm{X}$ from world coordinate system to camera coordinate system (with camera center/projection center at origin) then apply the projective transformation\n$$\n\\bm{x} = \\bm{K}[\\bm{R}|\\bm{t}]\\bm{X}\n$$\n\n$\\bm{K} = \\begin{bmatrix}\nf & 0 & p_x \\\\ \n0 & f & p_y \\\\ \n0 & 0 & 1\n\\end{bmatrix}$ in camera intrinsic matrix, $[\\bm{R}|\\bm{t}] \\in \\mathbb{R}^{3 \\times 4}$ is top 3 rows of the [[View Matrix|rendering.camera-mvp-matrix#view-matrix]]\n\n> **IMPORTANT** [[View Matrix|rendering.camera-mvp-matrix#view-matrix]] is the inverse of its [[Model Matrix|rendering.camera-mvp-matrix#model-matrix]], a.k.a. the camera's transform in world coordinate system\n\n## Non-square pixel\nIf the CMOS has non-square pixel, the image plane coordinate are not even scaled along $x$, $y$ axis.\n\nDenote focal lengths (in pixel) in $x$, $y$ directions are $f_x$, $f_y$\n\n$\\bm{K} = \\begin{bmatrix}\nf_x & 0 & p_x \\\\ \n0 & f_y & p_y \\\\ \n0 & 0 & 1\n\\end{bmatrix}$\n\n### Relation to field-of-view\n\n![field of view](/assets/images/2022-05-11-11-25-13.png){width: 150px}\n\nAs is illustrated in the figure\n\n$$\nfov_x = 2 \\arctan (\\frac{0.5 * w}{f_x})\n$$\n$$\nfov_y = 2 \\arctan (\\frac{0.5 * h}{f_y})\n$$\nwhere $w$, $h$ are image width, height\n\n## Change of image coordinate\n\n> Reference: https://dsp.stackexchange.com/questions/6055/how-does-resizing-an-image-affect-the-intrinsic-camera-matrix\n\nAssume original $\\bm{K} = \\begin{bmatrix}\nf_x & 0 & p_x \\\\ \n0 & f_y & p_y \\\\ \n0 & 0 & 1\n\\end{bmatrix}$\n\nScaling example, $(x, y)^T \\to (ax, by)^T$\n\n![Change of image coordinate](/assets/images/2022-05-11-11-25-32.png){width: 250px}\n\nLet scaling matrix\n$\\bm{S} = \n\\begin{bmatrix}\na & 0 &0 \\\\\n0 & b & 0 \\\\\n0 & 0 & 1 \n\\end{bmatrix}$\n\n$\\bm{K'} = \\bm{S} \\bm{K} = \\begin{bmatrix}\nf_x / a & 0 & p_x / a \\\\ \n0 & f_y / b & p_y / b \\\\ \n0 & 0 & 1\n\\end{bmatrix}$\n\nIn general, find a $3 \\times 3$ transformation matrix that transforms the image coordinate, then right multiply it with $\\bm{K}$, a.k.a. apply the transformation\n","n":0.042}}},{"i":149,"$":{"0":{"v":"Perspective 3 Points","n":0.577},"1":{"v":"\nPerspective 3 points (P3p) is used for pose estimation given 2D-3D correspondence\n\n## Points fitting approach\n\n$$\n\\bm{x} = \\bm{K} [\\bm{R} | \\bm{t}] \\bm{X}\n$$\n\nDirect fit by pairing [[optimization.least-square]] with [[epipolar-geometry.ransac]], suboptimal\n\n## Algebraic approach\n\n![](/assets/images/2022-05-14-08-00-55.png){width: 350px}\n\n$$\n\\begin{cases}\ns_1^2 = l_A^2 + l_B^2 - 2 l_A l_B \\cos\\theta_{AB} \\\\\ns_2^2 = l_A^2 + l_C^2 - 2 l_A l_C \\cos \\theta_{AC} \\\\\ns_3^2 = l_B^2 + l_C^2 - 2 l_B l_C \\cos \\theta_{BC}\n\\end{cases}\n$$\n\n4 positive solution for 3 points, use 4th point to determine correct solution\n\n## Other approaches\n- AP3p: directly solve camera pose\n- EPnp: for $n \\geq 4$, $n$ points as weighted sum of 4 control points","n":0.103}}},{"i":150,"$":{"0":{"v":"Homography Formation","n":0.707},"1":{"v":"\n> Reference: https://www.robots.ox.ac.uk/~vgg/hzbook/\n\nHomography decomposition ise used for pose estimation given 2D-2D correspondence\n\n## Derivation\nGiven tbe diagram of reference frame (left) and current frame (right)\n![](/assets/images/2022-05-12-10-09-24.png){width: 300px}\n\n- $\\bm{\\pi}$ is the reference plane, with norm $\\bm{n}$\n- $\\bm{c}$, $\\bm{c'}$ are camera centers\n- $d$, $d'$ are distances from camera centers to reference plane respectively\n- $\\bm{x}_{\\pi}$ is landmark on $\\bm{\\pi}$ in 3D space\n- $\\bm{x}$, $\\bm{x}'$ are respective projections of $\\bm{x}_{\\pi}$ on image planes\n\nAssume coordinate origin at $\\bm{c}$, the plane $\\bm{\\pi}$ can be represented by \n$$\n\\bm{n}^T \\bm{x} + d = 0\n$$\n\nLet camera intrinsic matrix $\\bm{K}$, from projective transform, we have\n$$\n\\bm{x} = \\bm{K} [\\bm{I}|\\bm{0}] \\bm{x}_{\\pi}\n$$\nLet $\\bm{K}^{-1} \\bm{x} = \\bm{\\hat{x}} = (x, y, 1)^T$, $\\bm{x}_{\\pi}$ lies on the ray, so $\\bm{x}_{\\pi} = (x, y, 1, \\rho)^T = (\\bm{\\hat{x}}, \\rho)^T$. since it also lies on the plane $\\bm{\\pi}$, therefore\n$$\n\\bm{n}^T \\bm{\\hat{x}} + d \\rho = 0 \\\\\n\\rho = -\\frac{\\bm{n}^T}{d} \\bm{\\hat{x}}\n\\\\\n\\therefore \\bm{x}_{\\pi} = (\\bm{\\hat{x}}, -\\frac{\\bm{n}^T}{d} \\bm{\\hat{x}})^T\n$$\nAssume camera $\\bm{c}$ (reference frame) has the transform $[\\bm{R} | \\bm{t}]$ w.r.t. $\\bm{c}'$ (current frame) ($\\bm{T}_{CR}$), let $\\bm{\\hat{x}'} = \\bm{K}^{-1} \\bm{x}'$, we have\n\n> Note $[\\bm{R} | \\bm{t}] \\bm{x}_{\\pi}$ transforms $\\bm{x}_{\\pi}$ in reference frame to current frame\n\n$$\n\\bm{\\hat{x}'} = [\\bm{R} | \\bm{t}] \\bm{x}_{\\pi} = (\\bm{R} - \\bm{t} \\frac{\\bm{n}^T}{d}) \\bm{\\hat{x}}\n$$\nThus, $\\bm{\\hat{x}'} = \\bm{H} \\bm{\\hat{x}}$, $\\bm{x}' = \\bm{KHK}^{-1} \\bm{x}$ where $\\bm{H} = \\bm{R} - \\bm{t} \\frac{\\bm{n}^T}{d}$ is the Homography Matrix\n\n> Note $\\bm{H}$ is in Euclidean space while $\\bm{KHK}^{-1}$ is in projective space\n\n## Fitting\nHomography Matrix $\\bm{H} \\in \\mathbb{R}^{3 \\times 3}$ has 8 DoF ($H_{33} = 1$) that can be fit with 4 pairs of coplanar points.\n\n### Robust fitting\nusing\n[[optimization.least-square]] with [[epipolar-geometry.ransac]], or [[optimization.least-square.iteratively-reweighted-least-square]]\n\n## Decomposition\nHomography Matrix $\\bm{H} = \\bm{R} - \\bm{t} \\frac{\\bm{n}^T}{d}$ can be decomposed into rotation $\\bm{R}$, normalized translation $\\frac{\\bm{t}}{d}$ and plane normal $\\bm{n}$\n\n### Solution ambiguity\n> Reference: https://hal.inria.fr/inria-00174036/PDF/RR-6303.pdf\n\nHomography decomposition usually has 8 solutions\n\n#### Non crossing contraints\n4 of 8 can be filtered by checking if two cameraa are on the opposite of plane $\\bm{\\pi}$\n$$\n\\frac{\\bm{t}}{d}^T \\bm{n} < d'\n$$\n\n#### Point visibility constraint\n2 of remaining 4 can be filtered by checking if unprojected image points are in front of the plane\n$$\n\\bm{\\hat{x}}^T \\bm{n} > 0\n\\\\\n\\bm{\\hat{x}'}^T (\\bm{R} \\bm{n}) > 0\n$$\n\nFind the most suitable one from the two needs more information, such as the knowledge of camera $z$ axis and plane normal","n":0.053}}},{"i":151,"$":{"0":{"v":"Euclidean and Projective","n":0.577},"1":{"v":"\n> Reference: https://www.robots.ox.ac.uk/~vgg/hzbook/\n\n## Points at infinity (Idea points)\nPoints where parallel line meets\n\n## Line at infinity\nLine where all idea points lies upon\n\n## Transformation in Euclidean Space\n\n> **IMPORTANT** [[Points at infinity (Idea points)|epipolar-geometry.euclidean-and-projective#points-at-infinity-idea-points]] preserving, parallel lines remain parallel\n\n### Rigid\nangle, area preserving (translation, rotation, reflection)\n\n### Similarity\nangle preserving (translation, rotation, reflection, scaling)\n\n### Affine\nangle, area are not preserved (translation, rotation, reflection, scaling, skew)\n\n\n## Projective Transformation\n[[Points at infinity (Idea points)|epipolar-geometry.euclidean-and-projective#points-at-infinity-idea-points]] are not preserved, only straightness is preserved\n","n":0.119}}},{"i":152,"$":{"0":{"v":"Coding","n":1}}},{"i":153,"$":{"0":{"v":"Jni","n":1}}},{"i":154,"$":{"0":{"v":"Tutorial","n":1}}},{"i":155,"$":{"0":{"v":"Create Java Class","n":0.577},"1":{"v":"\n> ApplyWeights.java\n\n```\npublic class ApplyWeights {\n\n    static {\n        System.loadLibrary(\"applyWeights\");\n    }\n    \n    public static void main(String[] args) {\n\n        float weights[] = new float[8];\n\n        weights[0] = 10f;\n        weights[4] = 6f;\n        \n        new ApplyWeights().applyWeights(weights);\n    }\n\n    // Declare a native method sayHello() that receives no arguments and returns void\n    private native void applyWeights(float[] weights);\n}\n```\n","n":0.143}}},{"i":156,"$":{"0":{"v":"Create Cpp Definition","n":0.577},"1":{"v":"\n```\njavac -h . ApplyWeights.java\n```\nIt will auto generate `ApplyWeights.class` and `ApplyWeights.h`\n\n> ApplyWeights.h\n\n```\n/* DO NOT EDIT THIS FILE - it is machine generated */\n#include <jni.h>\n/* Header for class ApplyWeights */\n\n#ifndef _Included_ApplyWeights\n#define _Included_ApplyWeights\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n/*\n * Class:     ApplyWeights\n * Method:    applyWeights\n * Signature: ([F)V\n */\nJNIEXPORT void JNICALL Java_ApplyWeights_applyWeights\n  (JNIEnv *, jobject, jfloatArray);\n\n#ifdef __cplusplus\n}\n#endif\n#endif\n```\nFill in the argument name and finish the cpp counterpart\n```\nJNIEXPORT void JNICALL Java_ApplyWeights_applyWeights\n  (JNIEnv* env, jobject, jfloatArray weights_);\n```\n\n```\n#include \"ApplyWeights.h\"\n#include <ostream>\n#include <iostream>\n\nvoid applyWeightsNested(float const* weights, size_t count) noexcept {\n  std::cout << \"Weight 0: \" << weights[0] << std::endl;\n  std::cout << \"Weight 4: \" << weights[4] << std::endl;\n}\n\nvoid applyWeights(float const* weights, size_t count) noexcept {\n  applyWeightsNested(weights, count);\n}\n\nJNIEXPORT void JNICALL Java_ApplyWeights_applyWeights\n  (JNIEnv* env, jobject, jfloatArray weights_) {\n    std::cout << \"Initializing...\" << std::endl;\n    jfloat* weights = env->GetFloatArrayElements(weights_, NULL);\n    jsize size = env->GetArrayLength(weights_);\n    applyWeights(weights, size);\n    env->ReleaseFloatArrayElements(weights_, weights, 0);\n}\n```","n":0.087}}},{"i":157,"$":{"0":{"v":"Compiling and Linking","n":0.577},"1":{"v":"\nCompile with JNI headers using `g++`\n```\ng++ -c -fPIC -I${JAVA_HOME}/include -I${JAVA_HOME}/include/linux ApplyWeights.cpp -o ApplyWeights.o\n```\nIt will generate `ApplyWeights.o`. Then link it into bridge library\n\n```\ng++ -shared -fPIC -o libapplyWeights.so ApplyWeights.o -lc\n```\nIt will generate `libapplyWeights.so`. The last is run the program with full path of library directory\n```\njava -cp . -Djava.library.path=/home/ruichenzheng/jni_test ApplyWeights\n```\nCommand line output:\n```\nInitializing...\nWeight 0: 10\nWeight 4: 6\n```","n":0.137}}},{"i":158,"$":{"0":{"v":"Swift","n":1}}},{"i":159,"$":{"0":{"v":"OCpp Cpp Callback","n":0.577},"1":{"v":"\n> MyClass.swift\n\n```\nlet callbackTarget = UnsafeMutableRawPointer(Unmanaged.passUnretained(self).toOpaque())\noc_wrapper.myFunc(callback: {\n    let coordinator = Unmanaged<MyClass>.fromOpaque(observer!).takeUnretainedValue()\n    ...\n}, callbackTarget)\n```\n\n> oc_wrapper.h\n\n```\n- (void)myFunc:(void(*)(int, void*))callback callbackTarget:(void*)callbackTarget;\n```\n\n> oc_wrapper.mm\n\n```\n- (void)myFunc:(void(*)(int, void*))callback callbackTarget:(void*)callbackTarget {\n    _cpp->myFunc(callback, callbackTarget);\n}\n```\n\n> CppClass.h\n\n```\nvoid myFunc(void(*callback)(int param, void*), void* callbackTarget);\n```\n\n> CppClass.cpp\n\n```\nvoid CppClass::myFunc(void(*callback)(int param, void*), void* callbackTarget) {\n    callback(static_cast<int>(param), callbackTarget);\n}\n```\n","n":0.162}}},{"i":160,"$":{"0":{"v":"Python","n":1}}},{"i":161,"$":{"0":{"v":"Read Image","n":0.707},"1":{"v":"\n## Pillow\n> need array conversion\n\n```\nfrom PIL import Image\nimport numpy as np\n\nimage = Image.open(\"image.png\")\narray_from_image = np.array(image)\nimage_from_array = Image.fromarray(array_from_image)\nimage_from_array.save(\"new_image.png\")\n```\n\n## OpenCV\n> can be directly indexed\n\n```\nimport cv2\nimport numpy as np\n\nimage_bgr = cv2.imread(\"image.png\", cv2.IMREAD_COLOR)\nimage_grayscale = cv2.imread(\"image.png\", cv2.IMREAD_GRAYSCALE)\nimage_bgra = cv2.imread(\"image.png\", cv2.IMREAD_UNCHANGED)\n\n...\n\narray = np.zeros((10, 10, 4), dtype=np.uint8)\ncv2.imwrite(\"new_image.png\", array)\n```","n":0.156}}},{"i":162,"$":{"0":{"v":"Plot","n":1},"1":{"v":"\n## Equal axis\n\n```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nplt.axis('equal')\nplt.plot(np.arange(20), np.arange(10, step=0.5))\n```","n":0.289}}},{"i":163,"$":{"0":{"v":"Parse JSON","n":0.707},"1":{"v":"\n```\nimport json\njson_data = json.load(open(\"json_file.json\"))\n\n# processing as dictionary\n...\n\n# save as a file\njson.dump(json_data, open(\"json_file_edit.json\", mode='w'))\n\n# encode as string\njson.dumps(json_data)\n```\n","n":0.25}}},{"i":164,"$":{"0":{"v":"Parallelism","n":1},"1":{"v":"\n> `thread` alone won't guarantee parallelism due to [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)\n\n\n## Multiprocessing\n\n> unwrap for nested for loop\n\n```\nimport multiprocessing\n\n\nwith multiprocessing.Pool(multiprocessing.cpu_count()) as p:\n    # func has single argument\n    p.map(func, args1)\n    # func has multiple arguments\n    p.starmap(func, zip(args1, args2))\n```\n\n\n## Joblib\n\n```\npip install joblib\n```\n\n```\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\nParallel(n_jobs=multiprocessing.cpu_count())(\n    delayed(func)(arg1, arg2) for (arg1, arg2) in zip(args1, args2)\n```\n\n> Support nested case\n\n```\nfrom joblib import Parallel, delayed\nimport multiprocessing\n\nParallel(n_jobs=multiprocessing.cpu_count())(\n    delayed(func)(arg1, arg2) for arg1 in args1 for arg2 in args2)\n```","n":0.121}}},{"i":165,"$":{"0":{"v":"Install Dependency","n":0.707},"1":{"v":"## Command line\n```\npip install numpy, opencv-python\n```\n\n## Python Script\nCreate `install dependency.py`\n```\nimport subprocess\nimport sys\n\ndef install(package):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n\nfor pkg in ['numpy', 'opencv-python']:\n    install(pkg)\n```\nThen\n```\npython dependency.py\n```","n":0.204}}},{"i":166,"$":{"0":{"v":"HTTP Server","n":0.707},"1":{"v":"\n```\npython -m http.server\n```\n","n":0.577}}},{"i":167,"$":{"0":{"v":"HTTP Server with GET/POST","n":0.5},"1":{"v":"\n## Dependency\n- [Flask](https://flask.palletsprojects.com/en/2.0.x/)\n\n## Project hierarchy\n```\ntemplates/\n    index.html\napp.py\n```\n\n## Implementation\n> app.py\n\n```\nfrom flask import Flask, request, render_template\n\napp = Flask(__name__)\n\n@app.route('/')\ndef upload_photo():\n    return render_template('index.html', ...)\n\n@app.route('/file', methods=[\"GET\"])\ndef get_glb():\n    # Note: The full url is f\"{request.host_url}tmp/file\"\n    return send_file(\"tmp/file\")\n\n@app.route('/actionName', methods=[\"POST\"])\ndef my_action():\n    files = request.files.getlist('fileName')\n\n    for file in files:\n        # do something (i.e. file.save(f\"tmp/{file.filename}\"))\n\n    return render_template('index.html', ...)\n\napp.run(host=\"0.0.0.0\", port=5000, debug=True)\n```\n\n> index.html\n\n```\n    <form enctype=\"multipart/form-data\" method=\"POST\" action=\"/actionName\">\n        <input type=\"file\" name=\"fileName\">\n    </form>\n\n    <a href=\"/file\">Download file</a>\n```\n","n":0.128}}},{"i":168,"$":{"0":{"v":"Exif Metadata Edit","n":0.577},"1":{"v":"\n## Dependency\n- [pillow](https://pillow.readthedocs.io/en/stable/)\n- [piexif](https://piexif.readthedocs.io/en/latest/)\n\n## Implementation\n> [Tag reference](https://exiv2.org/tags.html) (Rational => Fraction)\n\n```\nimg = Image.open(image_path)\nif 'exif' in img.info:\n    exif_dict = piexif.load(img.info['exif'])\n    if '0th' not in exif_dict:\n        exif_dict['0th'] = {}\n    if 'Exif' not in exif_dict:\n        exif_dict['Exif'] = {}\nelse:\n    exif_dict = {'0th': {}, 'Exif': {}}\n\nif args.make:\n    exif_dict['0th'][piexif.ImageIFD.Make] = args.make\n\nif args.model:\n    exif_dict['0th'][piexif.ImageIFD.Model] = args.model\n\nif args.focal_length:\n    focal_length = Fraction(args.focal_length)\n    exif_dict['Exif'][piexif.ExifIFD.FocalLength] = (\n        focal_length.numerator, focal_length.denominator)\n\nexif_bytes = piexif.dump(exif_dict)\nimg.save(image_path, exif=exif_bytes)\n```","n":0.129}}},{"i":169,"$":{"0":{"v":"JS TS","n":0.707}}},{"i":170,"$":{"0":{"v":"Function Binding","n":0.707},"1":{"v":"\nUse `bind` to avoid lose `this` when pass object method as callback\n## Example 1\n```\nlet object = {\n  func(){}\n}\n\nobject.func(); // func undefined\nlet bindfunc = object.func.bind(object);\nobject.func(); // successfully called\n```\n\n## Example 2\n```\nClass C {\n  constructor() {\n    this.message = \"Class C message\";\n    // undefined\n    this.addEventListener(\"mousedown\", this.onMouseDown, false);\n    // Class C message\n    this.addEventListener(\"mousedown\", this.onMouseDown.bind(this), false);\n  }\n\n  onMouseDown() {\n    console.log(this.message);\n  }\n}\n```","n":0.135}}},{"i":171,"$":{"0":{"v":"Foreach","n":1},"1":{"v":"\nForeach: iterate elements while passing it to callback\n","n":0.354}}},{"i":172,"$":{"0":{"v":"Async Await","n":0.707},"1":{"v":"\n\n```\nfunction f(): Promise<T>{\n    return new Promise<T>(resolve -> {resolve(readfile...)}).then(()=>{return bar})\n}\nasync function af(): Promise<T>{\n    let bar = await f();\n    ...\n}\n```\n","n":0.236}}},{"i":173,"$":{"0":{"v":"Array","n":1},"1":{"v":"\n## Find\n\n```\nmyArray.find(element => element > 0)\n```\n","n":0.408}}},{"i":174,"$":{"0":{"v":"Cuda","n":1}}},{"i":175,"$":{"0":{"v":"Basics","n":1},"1":{"v":"> Reference: https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#programming-model\n\n## Concepts\n\n**Kernel**: C++ function executed N times in parallel by N different CUDA threads. \n\n**Grid of Thread Blocks** (Global memory): `umBlocks` x `threadPerBlock`, both `numBlocks` and `threadPerBlock` can be `int` or `dim3`.\n\n**Thread block** (Per-block shared memory): 1-3D block of threads, indexed by `threadIdx`. Num of threads per block is limited up to 1024. It is required to be executed independently.\n```\ndim3 threadPerBlock(N) -> N x 1 x 1\ndim3 threadPerBlock(N, N) -> N x N x 1\ndim3 threadPerBlock(N, N, N) -> N x N x N\n```\n\n**Threads within block** (Per-thread local memory): can have shared memory access, can be synced by `__syncthreads()` (like [Fence](https://chromium.googlesource.com/chromium/src/+/HEAD/docs/design/gpu_synchronization.md))\n\n**Host**: CPU and its memory\n\n**Device**: GPU and its memory\n\n## Function Execution Space Specifier\n> https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#function-declaration-specifiers\n\n| Specifier  | Executed | Callable      |\n|:----------:|:--------:|:-------------:|\n|`__global__`| Device   | Host & Device |\n|`__device__`| Device   | Device only   |\n|`__host__`  | Host     | Host only     |\n\n## TODO\n- [x] [function-declaration](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#function-declaration-specifiers)","n":0.084}}},{"i":176,"$":{"0":{"v":"Cpp","n":1}}},{"i":177,"$":{"0":{"v":"Variable Declaration","n":0.707},"1":{"v":"\n## Pointer\n**pointer** is a variable that holds a memory address as its value.\n```\nint *iPtr; // a pointer to an integer value, int* iPtr; and int * iPtr\n           //valid but not favorable\ndouble *dPtr; // a pointer to a double value\nint *iPtr1, *iPtr2; // declare two pointers to integer values\n```\n\n## Reference\n**reference** is a variable acts as an alias to another variable\n```\nint v = 5; // normal integer\nint &ref = v; // reference to variable v\nv = 6; // v is now 6\nref = 7; // v is now 7\n++ref; // v is 8 before processing current statement\nref++; // v is 8 after processing current statement\n```\n\n## Address-of Operator (&)\n`&`: get memory address assigned to the variable\n```\nint x = 5;\nstd::cout << x << '\\n'; // print value of x\nstd::cout << &x << '\\n'; // print the memory address of x\n```\n\n## Dereference Operator (*)\n`*`: get the value of a particular address\n```\nint x = 5;\nstd::cout << x << '\\n'; // print value of x\nstd::cout << &x << '\\n'; // print the memory address of x\nstd::cout << *&x << '\\n'; // print value at the memory address of x\n```\n\n## Rvalue Reference\nTemporary variables that doesn't have memory address\n```\nint a; //lvalue\nint&& b = 5; //rvalue\n\na = b; // ok\n//b = a; error\n```","n":0.07}}},{"i":178,"$":{"0":{"v":"NaN Check","n":0.707},"1":{"v":"\n## Use comparison\n> Reference: https://stackoverflow.com/questions/570669/checking-if-a-double-or-float-is-nan-in-c\n\nComparisons involving `NaN` values are always false\n\n`v != v`\n\n> Not hold with `-ffast-math` flag\n\n## C++ 11\nUse `std::isnan()`","n":0.218}}},{"i":179,"$":{"0":{"v":"Literal","n":1},"1":{"v":"\nHard coded, fixed value in code.\n\n```\n1\n123\n'c'\n\"literal\"\n```\n","n":0.408}}},{"i":180,"$":{"0":{"v":"Lambda","n":1},"1":{"v":"## Capture\n\n### No capture\n```\nauto functor = [](int i){\n    return i * i;\n}\n```\n\n### Read only\n```\n\nstd::vector<int> vec(1);\nauto functor = [vec](int i){\n    return i * vec[0];\n}\n```\n\n### Mutable\n```\nstd::vector<int> vec(1);\nauto functor = [&vec](int i){\n    vec[0] = i;\n}\n```\n\n### All capture\n```\nstd::vector<int> vec(1);\nauto functor = [&](int i){\n    vec[0] = i;\n    return i * vec[0];\n}\n```\n\n## Pass Lambda as function argument\n\n>**IMPORTANT** Function pointer `void(*func)()` doesn't capture states, use `std::function<void()> func` instead \n>\n> Reference: https://stackoverflow.com/questions/28573986/how-to-pass-a-lambda-in-a-function-with-a-capture\n\n```\nauto functor = [](int i){\n    return i * i;\n}\n```\n\n```\nvoid func (std::function<int(int)> callback) {\n    callback(1);\n}\n```\n\n```\nfunc(functor);\n```\n","n":0.114}}},{"i":181,"$":{"0":{"v":"Hexadecimal Notation","n":0.707},"1":{"v":"\n## Basis\n- Base 16\n- 0-9 -> 0-9\n- A-F -> 10->15\n\n## Examples\n```\n1 == 0x1 == 0x01 == 0b00000001\n4 == 0x04 == 0b00000100\n64 == 0x40 == 0b01000000\n```","n":0.2}}},{"i":182,"$":{"0":{"v":"Function Arguments","n":0.707},"1":{"v":"## Pass by Value\n\nValues of arguments are firstly copied, with copied values passed to the function, which doesn't change arguments' values\n```\nvoid add(int x1, int x2){\n  x1 += x2;\n  x2 = 0;\n}\nint a = 1;\nint b = 2;\nadd(a, b); // a is still 1, b is still 2\n```\n\n## Pass by Pointer\n\nPass pointer holds memory address of variable, will affect argument's value\n```\nvoid add(int *x1, int *x2){\n  *x1 += *x2; // dereferenced to change value being pointed to\n  *x1 = 0; // dereferenced to change value being pointed to\n}\nint a = 1;\nint b = 2;\nint *aPtr, *bPtr;\naPtr = &a;\nbPtr = &b;\nadd(aPtr, bPtr); // a is 3, b is 0\n```\n\n## Pass by Reference\n\nPass reference of variable, will affect the argument's value\n```\nvoid add(int &x1, int &x2){\n  x1 += x2;\n  x1 = 0;\n}\nint a = 1;\nint b = 2;\nadd(a, b); // a is 3, b is 0\n```\nGoogle c++ style do **NOT** use pass by reference to modify arguments (use const reference)\n```\nvoid access(const int x){\n  ...\n}\n\nint a = 1;\naccess(a); // a is 1\n```\n\n## Pass by Address\n\nPass address of argument, affect its value\n```\nvoid add(int *x1, int *x2){\n  *x1 += *x2; // dereferenced to change value being pointed to\n  *x1 = 0; // dereferenced to change value being pointed to\n}\nint a = 1;\nint b = 2;\nadd(&a, &b); // a is 3, b is 0\n```\n\n## Pass Address by Reference\n\nChange address of argument within the function\n```\nvoid SetToNull(int *&ptr){ // reference to pointer\n  ptr = nullptr;\n}\nint five = 5;\nint *ptr = &five; // pointer holds address of five\nstd::cout << *ptr; // dereference, print 5, the value at address of five\nSetToNull(ptr); // pointer changed to null pointer\n```\nIn Google c++ style, use ** to dereference twice\n\n","n":0.061}}},{"i":183,"$":{"0":{"v":"Container","n":1},"1":{"v":"\n## Array\n\nContinuous memory locations, number of elements must be constant\n```\ndouble dArr[10];\nint iArr[] = {1, 2, 3, 4, 5};\nfloat fArr[3] = {1, 2} // omiteed element will be 0\ndArr[0] = 5;\nb = iArr[1] + 2;\nfArr[2] = fArr[0] + fArr[1];\n```\nArray name stores the memory address of the first element (can be regarded as a pointer). Modify within the function will affects its values\n```\nint max(int[] a, int size){\n  ...\n}\nint arr[] = {1, 2, 3};\nint *aPtr;\naPtr = arr;\naPtr = &arr[0];\nstd::cout << max(arr, sizeof(arr)/sizeof(int));\n```\n\n## Vector\n\nContinuous memory locations, number of elements can change\n```\nvector<int> iVec10;\nvector<int> empty;\nvector<int> iVec5(1, 2, 3, 4, 5);\nint iArr3[] = {1, 2, 3};\nvector<int> iVec3(iArr3, iArr3+3)\n\nint average(vector<int> iVec){ // pass by copy, need local copy (Do not use in Google c++ style)\n  ...\n}\n\n// do not change argument, pass by reference\nint average(const vector<int> &iVec){\n  ...\n}\n\n// intend to change argument, pass by pointer\nint average(vector<int> *iVec){\n  iVec->size();\n  (*iVec)[0] = 1;\n}\n\n// pass temporary variable (rvalue reference)\nint average(vector<int> &&iVec){\n ...\n}\n```\n\n### Vector comparison\n> Reference: https://en.cppreference.com/w/cpp/container/vector/operator_cmp\n\nSimilar to `std::lexicographical_compare. compare [lexicographically](https://en.wikipedia.org/wiki/Lexicographic_order) (generalized alphabetical order )\n\n### Unroll nested vector\n>Reference: https://stackoverflow.com/questions/20994321/clean-ways-to-write-multiple-for-loops\n\n```\ngrid_dim_X = 10;\ngrid_dim_Y = 20;\ngrid_dim_Z = 30;\nstd::vector<int> grid_3d;\ngrid_3d.resize(grid_dim_X * grid_dim_Y * grid_dim_Z);\n\n```\nAcess with\n```\n\nauto grid_query = [&](int, i, int, j, int, k){\n  return grid_3d[(i * grid_dim_X + j) * grid_dim_Y + k];\n};\n```\nor\n```\n\nauto grid_query = [&](int, i, int, j, int, k){\n  return grid_3d[i + grid_dim_X * (j + (grid_dim_Y * k))];\n};\n\n```\ndepends on data storage\n\n> Note: for sparse data storage, `std::unordered_map` would be more efficient","n":0.066}}},{"i":184,"$":{"0":{"v":"Bitwise Operator","n":0.707},"1":{"v":"\n## Basis\n- AND `&`\n- OR `|`\n- XOR `^`\n- Left shift `<<`\n- Right shift `>>`\n- NOT `~`\n\n## Examples\n`1u`: unsigned value 1\n```\n1u << 0 = 1\n1u << 1 = 2\n1u << 2 = 4\n```","n":0.177}}},{"i":185,"$":{"0":{"v":"Cmake","n":1}}},{"i":186,"$":{"0":{"v":"Subdirectory","n":1},"1":{"v":"\nfolder\n |\n |---CMakeLists.txt\n |\n |---subfolder\n        |\n        |---CMakeLists.txt\n\nAdd\n```\ninclude_directories(subfolder)\nadd_subdirectory(subfolder)\n```\nto `folder/CMakeLists.txt`, don't forget to `target_link_libraries`","n":0.289}}},{"i":187,"$":{"0":{"v":"Override Defines","n":0.707},"1":{"v":"> Reference: https://stackoverflow.com/questions/32947974/file-path-with-cmake-add-definitions\n\n\n> CMakeList.txt\n\n```\ntarget_compile_definitions(target PRIVATE DATA_DIR=\"${DATA_DIR}/${DATA_DIR}\")\n```\n\n> main.cpp\n\n```\n// Use Macro as string literals (https://stackoverflow.com/questions/798221/c-macros-to-create-strings)\nstd::string path = DATA_DIR \"/data.txt\"\n```","n":0.243}}},{"i":188,"$":{"0":{"v":"Fetch Content","n":0.707},"1":{"v":"\n> Reference: https://cmake.org/cmake/help/latest/module/FetchContent.html\n\n## CMake project\n\n> cmake/gflags.cmake\n\n```\nif(TARGET gflags::gflags)\n    return()\nendif()\n\ninclude(FetchContent)\nFetchContent_Declare(\n    gflags\n    GIT_REPOSITORY https://github.com/gflags/gflags.git\n    GIT_TAG v2.2.2\n)\nFetchContent_MakeAvailable(gflags)\n```\n\n> CMakeLists.txt\n\n```\nlist(PREPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake)\ninclude(gflags)\ntarget_link_libraries(${PROJECT_NAME} PUBLIC gflags::gflags)\n```\n\n> **IMPORTANT** The module name can be arbitrary but the library name in `target_link_libraries` depends on the namespace of the library\n\n## Non-CMake project\n> cmake/imgui.cmake\n\n```\nif(TARGET imgui::imgui)\n    return()\nendif()\n\ninclude(FetchContent)\nFetchContent_Declare(\n    imgui\n    GIT_REPOSITORY https://github.com/ocornut/imgui.git\n    GIT_TAG v1.87\n)\nFetchContent_MakeAvailable(imgui)\n\nset(IMGUI_HDRS\n    ${imgui_SOURCE_DIR}/imconfig.h\n    ${imgui_SOURCE_DIR}/imgui.h\n)\n\nset(IMGUI_SRCS\n    ${imgui_SOURCE_DIR}/imgui.cpp\n    ${imgui_SOURCE_DIR}/imgui_demo.cpp\n    ${imgui_SOURCE_DIR}/imgui_draw.cpp\n    ${imgui_SOURCE_DIR}/imgui_tables.cpp\n    ${imgui_SOURCE_DIR}/imgui_widgets.cpp\n)\n\nadd_library(imgui STATIC ${IMGUI_HDRS} ${IMGUI_SRCS})\ntarget_include_directories(imgui PUBLIC ${imgui_SOURCE_DIR})\ntarget_compile_definitions(imgui PUBLIC\n    IMGUI_IMPL_OPENGL_LOADER_GLAD\n)\n```\n> CMakeLists.txt\n\n```\nlist(PREPEND CMAKE_MODULE_PATH ${CMAKE_CURRENT_SOURCE_DIR}/cmake)\ninclude(imgui)\ntarget_link_libraries(${PROJECT_NAME} PUBLIC imgui)\n```\n\n## Auxiliary\n### Pass define\n```\nFetchContent_Declare(\n    depname\n    ...\n)\nFetchContent_MakeAvailable(depname)\n\ntarget_compile_definitions(depname INTERFACE\n    DEPNAME_DEFINE\n    CMAKE_BUILD_TYPE=Release\n)\n```\n\n### Ignore git submodule\n```\nFetchContent_Declare(\n    ...\n    GIT_SUBMODULES \"\"\n)\n```\n\n### Apply patch\n```\nFetchContent_Declare(\n    ...\n    PATCH_COMMAND patch -p1 ${CMAKE_CURRENT_SOURCE_DIR}/fix.patch\n)\n```\n\n## TODO\n- [ ] Library namespace","n":0.104}}},{"i":189,"$":{"0":{"v":"C","n":1}}},{"i":190,"$":{"0":{"v":"Array","n":1},"1":{"v":"\n```\nint * arr = malloc(5 * sizeof(int));\narr[0] = 0;\narr[1] = 1;\narr[2] = 2;\narr[3] = 3;\narr[4] = 4;\n\nfree(arr);\n```","n":0.243}}},{"i":191,"$":{"0":{"v":"Blender","n":1}}},{"i":192,"$":{"0":{"v":"Python API","n":0.707},"1":{"v":"## Mesh ops\n```\nif bpy.ops.object.mode_set.poll():\n    bpy.ops.object.mode_set(mode=\"EDIT\")\n    bpy.ops.mesh.edge_face_add()\n    bpy.ops.mesh.faces_shade_smooth()\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n```\n\n## Join\n```\nif bpy.ops.object.mode_set.poll():\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n    \n    for obj in objs:    \n        obj.select_set(True)\n    \n    # at least one of them needs to be active\n    bpy.context.view_layer.objects.active = objs[0]\n\n    bpy.ops.object.join()\n    obj = bpy.context.selected_objects[0]\n    obj.select_set(False)\n```\n\n## Modifier\n\n### Subdivision\n```\nif bpy.ops.object.mode_set.poll():\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n    bpy.ops.object.subdivision_set()\n    bpy.ops.object.modifier_apply(modifier=\"Subdivision\")\n```\n\n### Triangulate\n```\nif bpy.ops.object.mode_set.poll():\n    bpy.ops.object.mode_set(mode=\"OBJECT\")\n    bpy.ops.object.modifier_add(type=\"TRIANGULATE\")\n    bpy.ops.object.modifier_apply(modifier=\"TRIANGULATE\")\n```\n\n## UV\n```\nuvlayer = mesh.uv_layers.new()\nmesh.uv_layers.active = uvlayer\nfor face in mesh.polygons:\n    for vert_idx, loop_idx in zip(face.vertices, face.loop_indices):\n        uv = np.array(mesh.vertices[vert_idx].co[:2]) / 2.0 + 0.5\n        mesh.uv_layers.active.data[loop_idx].uv = uv\n```\n\n## Material\n```\n# load image\nbpy.ops.image.open(filepath=image_path)\n# create bsdf material\nmat = bpy.data.materials.new(image_path)\nmat.use_nodes = True\nfor node in mat.node_tree.nodes:\n    if node.type == \"BSDF_PRINCIPLED\":\n        node.select = True\n        # create image texture\n        image_texture_node = mat.node_tree.nodes.new(\"ShaderNodeTexImage\")\n        image_texture_node.image = bpy.data.images[image_path]\n        # assign image texture to bsdf base color\n        mat.node_tree.links.new(node.inputs['Base Color'], image_texture_node.outputs['Color'])\nmesh.materials.append(mat)\n```\n\n## Bezier curve\n```\ndef create_bezier_curve_object(name, bpts, scale=1.0):\n    curve_data = bpy.data.curves.new(name, 'CURVE')\n    bevel_obj = bpy.data.objects.new(name, curve_data)\n    bpy.context.collection.objects.link(bevel_obj)\n\n    num_of_bpts = int(len(bpts) / 3)\n    polyline = curve_data.splines.new('BEZIER')\n\n    # bezier spline is initialized with one bezier_point\n    polyline.bezier_points.add(num_of_bpts - 1)\n    to_pt3 = lambda pt2: [scale * pt2['x'], -scale * pt2['y'], 0]\n    for i in range(num_of_bpts):\n        polyline.bezier_points[i].co = to_pt3(bpts[3 * i])\n        polyline.bezier_points[i].handle_left_type = 'ALIGNED'\n        polyline.bezier_points[i].handle_left = to_pt3(bpts[3 * i + 1])\n        polyline.bezier_points[i].handle_right_type = 'ALIGNED'\n        polyline.bezier_points[i].handle_right = to_pt3(bpts[3 * i + 2])\n```\n\n## Edge loop bevel\n```\nobj.select_set(True)\nbpy.ops.object.convert(target='CURVE')\nbpy.data.curves[name].bevel_mode = 'OBJECT'\nbpy.data.curves[name].bevel_object = bpy.data.objects[BEVEL_OBJECT_NAME]\nbpy.ops.object.convert(target='MESH')\nobj.select_set(False)\n```\n","n":0.072}}},{"i":193,"$":{"0":{"v":"Parallelism","n":1},"1":{"v":"> Reference: https://blender.stackexchange.com/questions/28304/using-multiprocessing-to-speed-up-multiple-sound-bakings\n\nPython `multiprocessing` alone won't work as Blender's python is merely wrapper for C api. Instead, one could run multiple blender C thread as subprocesses to achieve parallelism.\n\n> blender_main.py\n\n```\nimport bpy\nimport multiprocessing\nimport subprocess\n\ndef call_subprocess(folder, name):\n    ...\n    subprocess.run([\n        \"blender\", \"-b\", \"-P\", \"blender_subprocess.py\", \"--\", \"{folder}\", f\"{name}\"\n    ])\n\n\nif __name__ == \"__main__\":\n    ...\n    with multiprocessing.Pool(multiprocessing.cpu_count()) as p:\n        # call subprocesses in parallel\n        p.starmap(call_subprocess, zip(folders, names))\n        # append result to current blender project\n        for obj_name in names:\n            bpy.ops.wm.append(\n                filepath=f\"tmp/{obj_name}.blend\\\\Object\\\\{obj_name}\",\n                filename=f\"{obj_name}\",\n                directory=f\"tmp/{obj_name}.blend\\\\Object\\\\\")\n            obj = bpy.data.objects[obj_name]\n            obj.select_set(False)\n```\n\n> blender_subprocess.py\n\n```\nimport bpy\nimport os\n\nif __name__ == \"__main__\":\n\n    argv = sys.argv\n    argv = argv[argv.index(\"--\") + 1:]\n\n    folder = argv[0]\n    name = argv[1]\n\n    ...\n\n    bpy.ops.wm.save_as_mainfile(filepath=os.path.join(folder, f\"{name}.blend\"),\n                                check_existing=False)\n```\n","n":0.098}}},{"i":194,"$":{"0":{"v":"Bazel","n":1}}},{"i":195,"$":{"0":{"v":"Glog","n":1},"1":{"v":"\n## Logging\n`GLOG_alsologtostderr=1` shows all `LOG(severity_level)` logs, where `severity_level = {INFO, WARNING, ERROR, FATAL}`. Note `FATAL` will terminate the program\n## Verbose logging\n`GLOG_v=level` shows all `VLOG(level)` logs less or equal to specified level\n","n":0.18}}},{"i":196,"$":{"0":{"v":"External Packages","n":0.707},"1":{"v":"\n## Rules\n### `http_archive`\n`BUILD`, patch\n\n```\nload(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\")\n```\n\n### `git_repository`\ncommit, patch\n```\nload(\"@bazel_tools//tools/build_defs/repo:git.bzl\", \"git_repository\")\n```\n### `new_git_repository`\ncommit, `BUILD` or patch\n```\nload(\"@bazel_tools//tools/build_defs/repo:git.bzl\", \"new_git_repository\")\n```\n## Native Bazel Support\n\n```\nhttp_archive(\n    name = \"com_github_gflags_gflags\",\n    sha256 = \"34af2f15cf7367513b352bdcd2493ab14ce43692d2dcd9dfc499492966c64dcf\",\n    strip_prefix = \"gflags-2.2.2\",\n    urls = [\"https://github.com/gflags/gflags/archive/v2.2.2.tar.gz\"],\n)\n```\n`strip_prefix` is the name of the root folder of the archive\n","n":0.162}}},{"i":197,"$":{"0":{"v":"No Native Bazel Support","n":0.5}}},{"i":198,"$":{"0":{"v":"Write Custom Rules","n":0.577},"1":{"v":"sample customized rule for complicated package\n```\ndef gtsam_library(name, deps = []):\n    gtsam = \"external/%s\" % native.repository_name().lstrip(\"@\")\n    srcs = native.glob([\"gtsam/**/*.cpp\"], exclude = [\"gtsam/3rdparty/**/*.cpp\", \"gtsam/precompiled_header.cpp\", \"gtsam/**/tests/*\"]) + [\"gtsam/3rdparty/CCOLAMD/Source/ccolamd.c\"]\n    hrds = native.glob([\"gtsam/**/*.h\"], exclude = [\"gtsam/base/chartTesting.h\", \"gtsam/precompiled_header.h\", \"gtsam/3rdparty/**/*.h\"]) + native.glob([\"gtsam/3rdparty/ceres/*.h\"]) + [\"gtsam/3rdparty/metis/include/metis.h\"] + [\"gtsam/3rdparty/CCOLAMD/Include/ccolamd.h\"]\n    return native.cc_library(\n        name = name,\n        srcs = srcs + hrds,\n        hdrs = hrds,\n        copts = [\n            \"-I\" + gtsam,\n        ],\n        defines = [\n            \"GTSAM_ALLOCATOR_STL\",\n            \"GTSAM_THROW_CHEIRALITY_EXCEPTION\",\n            \"GTSAM_ALLOW_DEPRECATED_SINCE_V4\",\n        ],\n        includes = [\".\"],\n        visibility = [\"//visibility:public\"],\n        deps = deps + [\n            \"@com_gitlab_libeigen_eigen//:eigen\",\n            \"@boost//:archive\",\n            \"@boost//:bind\",\n            \"@boost//:variant\",\n            \"@boost//:lexical_cast\",\n            \"@boost//:date_time\",\n            \"@boost//:math\",\n            \"@boost//:serialization\",\n            \"@boost//:core\",\n            \"@boost//:multiprecision\",\n            \"@boost//:filesystem\",\n        ],\n    )\n```\nuse it by adding `load(\"//:gtsam.bzl\", \"gtsam_library\")`\n","n":0.103}}},{"i":199,"$":{"0":{"v":"Use Patches","n":0.707},"1":{"v":"\nThis is recommended if package has complicated/nested dependencies that cannot be easily extracted into `WORKSPACE`\n\n```\n# GTSAM - GTSAM is a library of C++ classes that implement smoothing and mapping (SAM) in robotics and vision, \n# using factor graphs and Bayes networks as the underlying computing paradigm rather than sparse matrices.\nhttp_archive(\n    name = \"com_github_borglab_gtsam\",\n    patch_args = [\n        \"-p1\",\n    ],\n    patches = [\n        \"@//third_party:com_github_borglab_gtsam_fixes.diff\",\n    ],\n    sha256 = \"eaa561749edf7a2d402981828253e28aed6c717dae35738301c5ab23e2595f25\",\n    strip_prefix = \"gtsam-4.0.3\",\n    urls = [\n        \"https://github.com/borglab/gtsam/archive/4.0.3.tar.gz\",\n    ],\n)\n```\n## Patch example\n\n> generated using git diff\n\n```\n# third_party/com_github_borglab_gtsam_fixes.diff\n\ndiff --git a/BUILD b/BUILD\nnew file mode 100644\nindex 000000000..c29bc0337\n--- /dev/null\n+++ b/BUILD\n@@ -0,0 +1,52 @@\n+load(\"//:gtsam.bzl\", \"gtsam_library\")\n+\n+cc_library(\n+    name = \"GKlib\",\n+    srcs = glob([\"gtsam/3rdparty/metis/GKlib/*.c\"]),\n+    hdrs = glob([\"gtsam/3rdparty/metis/GKlib/*.h\"]),\n+    includes = [\"gtsam/3rdparty/metis/GKlib\"],\n+    visibility = [\"//visibility:public\"],\n+)\n+\n+cc_library(\n+    name = \"libmetis\",\n+    srcs = glob([\"gtsam/3rdparty/metis/libmetis/*.c\"]),\n+    hdrs = glob([\"gtsam/3rdparty/metis/libmetis/*.h\"]),\n+    includes = [\"gtsam/3rdparty/metis/libmetis\"],\n+    visibility = [\"//visibility:public\"],\n+    deps = [\n+        \":GKlib\",\n+    ],\n+)\n+\n+cc_library(\n+    name = \"SuiteSparse_config\",\n+    srcs = [\"gtsam/3rdparty/SuiteSparse_config/SuiteSparse_config.c\"],\n+    hdrs = [\"gtsam/3rdparty/SuiteSparse_config/SuiteSparse_config.h\"],\n+    includes = [\"gtsam/3rdparty/SuiteSparse_config\"],\n+    visibility = [\"//visibility:public\"],\n+)\n+\n+gtsam_library(\n+    name = \"libgtsam\",\n+    deps = [\n+        \":SuiteSparse_config\",\n+        \":libmetis\",\n+    ],\n+)\n\n......\n```\n","n":0.081}}},{"i":200,"$":{"0":{"v":"Use Build File","n":0.577},"1":{"v":"It is recommended if package doesn't require external dependencies or they can be added similarly in `WORKSPACE`\n```\n# Animation compression is a fundamental aspect of modern video game engines. \n# Not only is it important to keep the memory footprint down but it is also critical to keep the animation clip sampling performance fast.\nhttp_archive(\n    name = \"com_github_nfrechette_acl\",\n    build_file = \"@//third_party:acl.BUILD\",\n    strip_prefix = \"acl-2.0.0\",\n    type = \"zip\",\n    sha256 = \"eae362d5a4c7d5bc6f82b5c41efc66d968066c5145c414b92d26f53ce689145f\",\n    urls = [\"https://github.com/nfrechette/acl/archive/refs/tags/v2.0.0.zip\"]\n)\n```\n## `Build ` example\n\n> third_party/acl.BUILD\n\n```\ncc_library(\n    name = \"acl\",\n    hdrs = glob([\"includes/**/*.h\"]),\n    srcs = glob([\"includes/**/*.h\"]),\n    includes = [\"includes\"],\n    visibility = [\"//visibility:public\"],\n)\n```\nreference it by `\"@com_github_nfrechette_acl//:acl\"`, or combine it with its dependencies\n\n\n> third_party/BUILD\n\n```\ncc_library(\n    name = \"acl\",\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \":rtm\",\n        \"@com_github_nfrechette_acl//:acl\",\n    ],\n)\n\ncc_library(\n    name = \"rtm\",\n    visibility = [\"//visibility:public\"],\n    deps = [\n        \"@com_github_catchorg_Catch2//:catch2\",\n        \"@com_github_google_benchmark//:benchmark\",\n        \"@com_github_nfrechette_rtm//:rtm\",\n    ],\n)\n```\nand reference by `\"//third_party:acl\"`\n","n":0.088}}},{"i":201,"$":{"0":{"v":"External Package Denpendencies","n":0.577},"1":{"v":"\n## Use External Package in Current Workspace\n```\ndeps = deps + [\n    \"@com_gitlab_libeigen_eigen//:eigen\",\n    \"@boost//:archive\",\n    \"@boost//:bind\",\n    \"@boost//:variant\",\n    \"@boost//:lexical_cast\",\n    \"@boost//:date_time\",\n    \"@boost//:math\",\n    \"@boost//:serialization\",\n    \"@boost//:core\",\n    \"@boost//:multiprecision\",\n    \"@boost//:filesystem\",\n],\n```\n\n## Use External Package in Another External Package Workspace\n```\ndiff --git a/audio/dsp/spectrogram/BUILD b/audio/dsp/spectrogram/BUILD\nindex 5c0c45e..b416324 100644\n--- a/audio/dsp/spectrogram/BUILD\n+++ b/audio/dsp/spectrogram/BUILD\n@@ -20,7 +20,7 @@ cc_library(\n         \"//audio/dsp:number_util\",\n         \"//audio/dsp:porting\",\n         \"//audio/dsp:window_functions\",\n-        \"//third_party/fft2d:fft2d\",\n-        \"@com_github_glog_glog//:glog\",\n+        \"@org_tensorflow//third_party/fft2d:fft2d_headers\",\n+        \"//third_party:glog\",\n     ],\n )\n```\n\n\n## Use External Package Workspace\n```\nload(\"@org_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\ntf_workspace(tf_repo_name = \"org_tensorflow\")\n```\n","n":0.132}}},{"i":202,"$":{"0":{"v":"Android","n":1}}},{"i":203,"$":{"0":{"v":"Bytebuffer","n":1},"1":{"v":"\nDo not forget `order(ByteOrder.nativeOrder()` when allocate `ByteBuffer`!\n","n":0.378}}},{"i":204,"$":{"0":{"v":"Eigen","n":1},"1":{"v":"\n## Types\n\n### Vector\nVector of dim n\n\n### Matrix\nMatrix of shape n x m\n\nNote `RowVectorXd` is Convenience typedefs for 1 x n matrix\n\n### Array\nGeneral purpose array, support coefficient-wise operation with no linear algebra meaning\n\n```\nEigen::VectorXd S; // VectorXd\ns.array(); // Array\n```\n\n## Partial reduction\n- `rowwise()`: treat as 1 x cols()\n- `colwise()`: treat as rows() x 1\n\n### Example 1\n```\nV: n x 3 (MatrixXd)\nTT: 4 x 4 transformation matrix\n\n(V.rowwise().homogeneous()*TT).rowwise().hnormalized())\n```\n\n### Example 2\n```\nP: n x 3 (MatrixXd)\nv: 1 x 3 (RowVector3d)\n\nP.rowwise() - v: n x 3\n```\n\n### Example 3\n```\nP: n x 3 (MatrixXd)\nv: n (VectorXd)\n\n(P.array().colwise() * v.array()).matrix()\n```\n\n## Concatenation\nCreate matrix/vector of target size, then assign block\n\n```\nA: n x 3\nB: n x 3\n\nEigen::MatrixXd C(2 * m, 3);\nC << A, B;\n```","n":0.096}}},{"i":205,"$":{"0":{"v":"Todos","n":1},"1":{"v":"## Epipolar geometry\n- [ ] Homogeneous coordinate system\n- [ ] Fundamental matrix, essential matrix\n- [ ] Pnp\n- [ ] Pnp tracking\n- [ ] Detector based tracking\n## Rendering\n- [x] gltf skinning\n- [ ] gl shared context\n## Linear algebra\n- [x] Vector differentiation\n- [x] Taylor expansion and linearization\n## Probability theory\n- [x] Random Variables\n- [x] Expectation Integral\n- [x] Gaussian distribution\n- [x] MLE, MAP\n- [ ] Kalman filter\n- [ ] Extended Kalman filter\n- [ ] Particle filter\n- [ ] Importance sampling\n- [ ] Sequential importance sampling\n- [ ] Rao-Blackwellized partical filter\n- [ ] Gaussian processing\n## Optimization\n- [ ] IRLS\n- [ ] Gauss-Newton\n## Proxy\n- [x] git\n- [x] svn\n- [x] pip\n## Python\n- [x] Image read from pillow to numpy array\n- [x] Image read rgb/rgba from opencv to numpy array\n- [x] matplotlib equal axis\n- [ ] nparray indexing\n## Blender\n- [x] join\n- [x] modifier\n- [x] uv\n- [x] material\n## Swift\n- [x] oc/cpp callback\n## Metal\n- [x] computing kernel","n":0.082}}},{"i":206,"$":{"0":{"v":"Ffmpeg","n":1},"1":{"v":"\n## Scale video such that its width equal to 720\n```\nfor i in *.mp4; do ffmpeg -i \"$i\" -vf scale=720:-1 \"${i%.*}_720p.mp4\";done\n```\n## Flip video upside down\n\n```\nfor i in *.mp4; do ffmpeg -i \"$i\" -vf \"transpose=2,transpose=2\" \"out/${i%.*}.mp4\";done\n```\n## Extract single frame as an image at given timestamp\n```\nffmpeg -i DeskPortal_State_03_PC_1440p_Idle_SitAd_30fps_40534656.m2v -ss 00:00:11 -vframes 1 output.png\n```\n\n## Extract all frame at given frame rate\n```\nffmpeg -i VID_20220126_151107.mp4 -r 4 images/frame%04d.png\n```\n\n## Pad video\n```\nffmpeg -i tom_1080.mp4 -vf \"pad=width=1920:height=1080:x=(1920-1080)/2:y=0:color=white\" tom_pad.mp4\n```\n\n## Loop video\n```\nffmpeg -stream_loop 2 -i tbrender.mp4 -c copy tbrender_loop.mp4\n```\n\n## Chain\n1. `scale` video to `1080x1080`\n2. `pad` video to `1920x1080`\n```\nffmpeg -i tom.mp4 -vf \"scale=1080:1080,pad=1920:height=1080:x=(1920-iw)/2:y=(1080-ih)/2:color=white\" tom_pad_chain.mp4\n```\n\n## Complex scenario\n\n### Example 1\n1. green screen to transparent (similarity 0.1, blend 0.2)\n2. overlay\n\n```\nffmpeg -i flower.mp4 -i women.mp4 -vcodec libx264 -filter_complex \"[0]chromakey=green:0.1:0.2[ia];[1][ia]overlay\" out.mp4\n```\n\n### Example 2\n1. `alphamerge` first two video\n2. create pure white video with given resolution, framerate and format\n3. overlap alphamerged video with pure white for suration equal to shortest one\n4. crop video to `2240x1260` with upper left corner at `(160, 0)`\n```\nfor i in *_1.mpg; do\n  ffmpeg -i \"$i\" -i \"${i%_*}_2.mpg\" -vcodec libx264 -filter_complex \"[0][1]alphamerge[ia];color=white:s=2560x1440:rate=30[bg];[bg][ia]overlay=shortest=1,format=yuv420p[v];[v]crop=2240:1260:160:0\" \"${i%.usm_1.mpg}.mp4\"\ndone\n```\n","n":0.077}}}]}
